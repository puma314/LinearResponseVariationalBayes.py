{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "from VariationalBayes.SparseObjectives import Objective\n",
    "import VariationalBayes.ExponentialFamilies as ef\n",
    "import VariationalBayes.Modeling as modeling\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "import numpy as onp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a variational implementation of the hierarchical_2pl model from the Stan examples.\n",
    "np.random.seed(42)\n",
    "\n",
    "num_i = 10\n",
    "num_j = 50\n",
    "\n",
    "# mu[0] <-> log_alpha\n",
    "# mu[1] <-> beta\n",
    "\n",
    "true_params = vb.ModelParamsDict('true')\n",
    "true_params.push_param(vb.VectorParam('alpha', size=num_i, lb=0))\n",
    "true_params.push_param(vb.VectorParam('beta', size=num_i))\n",
    "true_params.push_param(vb.VectorParam('theta', size=num_j))\n",
    "true_params.push_param(vb.VectorParam('mu', size=2))\n",
    "true_params.push_param(vb.PosDefMatrixParam('sigma', size=2))\n",
    "\n",
    "prior_params = vb.ModelParamsDict('prior')\n",
    "prior_params.push_param(vb.VectorParam('mu_mean', size=2, val=np.array([0., 0.])))\n",
    "mu_prior_cov = np.array([[1., 0.], [0., 25.]])\n",
    "prior_params.push_param(vb.PosDefMatrixParam('mu_info', size=2, val=np.linalg.inv(mu_prior_cov)))\n",
    "prior_params.push_param(vb.ScalarParam('theta_mean', val=0.0))\n",
    "prior_params.push_param(vb.ScalarParam('theta_var', val=1.0))\n",
    "prior_params.push_param(vb.VectorParam('tau_param', size=2, val=np.array([0.1, 0.1])))\n",
    "prior_params.push_param(vb.ScalarParam('lkj_param', val=4.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "# Set true parameters and generate data.\n",
    "\n",
    "# y is defined as a num_i * num_j matrix.  Here, alpha and beta are num_i vectors and\n",
    "# theta is the num_j vector.  Practically we expect num_j >> num_i.\n",
    "# Combine vectors in the appropriate way to match the shape of y.\n",
    "def get_logit_p_term(alpha, beta, theta):\n",
    "    return (np.expand_dims(theta, 0) - np.expand_dims(beta, 1)) * np.expand_dims(alpha, 1)\n",
    "\n",
    "true_params['alpha'].set(np.exp(np.random.random(num_i)))\n",
    "true_params['beta'].set(np.random.random(num_i) - 0.5)\n",
    "true_params['theta'].set(np.random.random(num_j) - 0.5)\n",
    "true_params['mu'].set(np.random.random(2))\n",
    "true_params['sigma'].set(np.eye(2))\n",
    "\n",
    "logit_p = get_logit_p_term(alpha=true_params['alpha'].get(),\n",
    "                           beta=true_params['beta'].get(),\n",
    "                           theta=true_params['theta'].get())\n",
    "y_prob = sp.special.expit(logit_p)\n",
    "y = sp.stats.bernoulli.rvs(y_prob)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb_params = vb.ModelParamsDict('params')\n",
    "vb_params.push_param(vb.UVNParamVector('log_alpha', length=num_i))\n",
    "vb_params.push_param(vb.UVNParamVector('beta', length=num_i))\n",
    "vb_params.push_param(vb.UVNParamVector('theta', length=num_j))\n",
    "vb_params.push_param(vb.MVNParam('mu', dim=2))\n",
    "vb_params.push_param(vb.WishartParam('sigma_inv', size=2))\n",
    "\n",
    "vb_init_par = vb_params.get_free()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, y, vb_params, prior_params, num_draws):\n",
    "        self.y = deepcopy(y)\n",
    "        self.vb_params = deepcopy(vb_params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.std_draws = modeling.get_standard_draws(num_draws)\n",
    "        \n",
    "        self.num_i = self.vb_params['log_alpha'].mean.size()\n",
    "        self.num_j = self.vb_params['theta'].mean.size()\n",
    "\n",
    "    def get_e_log_data_likelihood(self):\n",
    "        # P(y = 1) = expit(z)\n",
    "        log_alpha = self.vb_params['log_alpha']\n",
    "        beta = self.vb_params['beta']\n",
    "        theta = self.vb_params['theta']\n",
    "\n",
    "        e_z = get_logit_p_term(alpha=log_alpha.e_exp(), beta=beta.e(), theta=theta.e())\n",
    "        # The minus in front of the beta variance gets us the sum of the variances.\n",
    "        var_z = get_logit_p_term(alpha=log_alpha.e2_exp(), beta=-1 * beta.var(), theta=theta.var())\n",
    "        \n",
    "        y_logit_term = modeling.get_e_logistic_term(self.y, e_z, np.sqrt(var_z), self.std_draws)\n",
    "\n",
    "        return y_logit_term\n",
    "    \n",
    "    def get_e_log_hierarchy_likelihood(self):\n",
    "        log_alpha = self.vb_params['log_alpha']\n",
    "        beta = self.vb_params['beta']\n",
    "        mu = self.vb_params['mu']\n",
    "        sigma_inv = self.vb_params['sigma_inv']\n",
    "\n",
    "        # Refer to the combined (log_alpha, beta) vector as 'ab'.\n",
    "        e_ab = np.array([ log_alpha.e(), beta.e() ])\n",
    "        e_outer_ab = np.array([[ log_alpha.e_outer(), beta.e() * log_alpha.e() ],\n",
    "                               [ beta.e() * log_alpha.e(), beta.e_outer() ] ])\n",
    "\n",
    "        e_sigma_inv = sigma_inv.e()\n",
    "        \n",
    "        return -0.5 * (self.num_i * np.einsum('ij,ji', e_sigma_inv, mu.e_outer()) - \\\n",
    "                       2 * np.einsum('i,ij,jn->', mu.e(), e_sigma_inv, e_ab) +\n",
    "                       np.einsum('ij,jin->', e_sigma_inv, e_outer_ab)) + \\\n",
    "               0.5 * self.num_i * sigma_inv.e_log_det()\n",
    "        \n",
    "    def get_e_log_prior(self):\n",
    "        log_alpha = self.vb_params['log_alpha']\n",
    "        beta = self.vb_params['beta']\n",
    "        theta = self.vb_params['theta']\n",
    "        mu = self.vb_params['mu']\n",
    "        sigma_inv = self.vb_params['sigma_inv']\n",
    "        \n",
    "        prior_params = self.prior_params\n",
    "        \n",
    "        e_log_prior = 0.\n",
    "        \n",
    "        # Mu\n",
    "        e_log_prior += ef.mvn_prior(\n",
    "            prior_params['mu_mean'].get(), prior_params['mu_info'].get(),\n",
    "            mu.e(), mu.cov())\n",
    "\n",
    "        # Theta\n",
    "        e_log_prior += np.sum(ef.uvn_prior(\n",
    "            prior_params['theta_mean'].get(), prior_params['theta_var'].get(),\n",
    "            theta.e(), theta.var()))\n",
    "        \n",
    "        # Sigma\n",
    "        e_log_prior += np.sum(ef.exponential_prior(\n",
    "            prior_params['tau_param'].get(), np.diag(sigma_inv.e_inv())))\n",
    "        e_log_prior += sigma_inv.e_log_lkj_inv_prior(prior_params['lkj_param'].get())\n",
    "        \n",
    "        return e_log_prior\n",
    "    \n",
    "    def get_e_log_likelihood(self):\n",
    "        return \\\n",
    "            self.get_e_log_data_likelihood() + \\\n",
    "            self.get_e_log_hierarchy_likelihood() + \\\n",
    "            self.get_e_log_prior()\n",
    "\n",
    "    def get_entropy(self):\n",
    "        return \\\n",
    "            self.vb_params['log_alpha'].entropy() + \\\n",
    "            self.vb_params['beta'].entropy() + \\\n",
    "            vb_params['theta'].entropy() + \\\n",
    "            vb_params['mu'].entropy() + \\\n",
    "            vb_params['sigma_inv'].entropy()\n",
    "            \n",
    "    def get_kl(self):\n",
    "        return -1 * (self.get_e_log_likelihood() + self.get_entropy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692.09940003653662"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(y, vb_params, prior_params, 10)\n",
    "objective = Objective(model.vb_params, model.get_kl)\n",
    "objective.fun_free(vb_init_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFGS\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KLGrad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c7d49432dff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m vb_opt_bfgs = optimize.minimize(\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun_free\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_init_par\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     method='bfgs', jac=KLGrad, tol=1e-6)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print 'Running Newton Trust Region'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KLGrad' is not defined"
     ]
    }
   ],
   "source": [
    "print('Running BFGS')\n",
    "vb_opt_bfgs = optimize.minimize(\n",
    "    lambda par: objective.fun_free(par, verbose=True), vb_init_par,\n",
    "    method='bfgs', jac=objective.fun_free_grad, tol=1e-6)\n",
    "\n",
    "# print 'Running Newton Trust Region'\n",
    "# vb_opt = optimize.minimize(\n",
    "#     lambda par: kl_wrapper.Eval(par, verbose=True),\n",
    "#     vb_opt_bfgs.x, method='trust-ncg', jac=KLGrad, hess=KLHess)\n",
    "# mvn_par_opt = copy.deepcopy(mvn_par)\n",
    "# mvn_par_opt.set_free(vb_opt.x)\n",
    "# print 'Done.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
