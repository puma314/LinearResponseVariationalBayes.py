{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from VariationalBayes import ScalarParam, ModelParamsDict, VectorParam, PosDefMatrixParam\n",
    "# from VariationalBayes.NormalParams import MVNParam, UVNParam, UVNParamVector\n",
    "# from VariationalBayes.GammaParams import GammaParam\n",
    "# from VariationalBayes.ExponentialFamilies import \\\n",
    "#     univariate_normal_entropy, multivariate_normal_entropy, gamma_entropy, \\\n",
    "#     mvn_prior, uvn_prior, gamma_prior\n",
    "\n",
    "import VariationalBayes as vb\n",
    "import LogisticGLMM_lib as logit_glmm\n",
    "from VariationalBayes.SparseObjectives import Objective, SparseObjective\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import autograd.numpy as np\n",
    "import numpy as np\n",
    "\n",
    "from autograd import jacobian\n",
    "# import autograd.numpy as np\n",
    "# import autograd.numpy.random as npr\n",
    "# import autograd.scipy as sp\n",
    "# import scipy as osp\n",
    "\n",
    "import copy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Load data saved by stan_results_to_json.R and run_stan.R in LRVBLogitGLMM.\n",
    "import os\n",
    "import json\n",
    "\n",
    "simulate_data = False\n",
    "prior_par = vb.ModelParamsDict('Prior Parameters')\n",
    "\n",
    "if not simulate_data:\n",
    "    #analysis_name = 'simulated_data_small'\n",
    "    #analysis_name = 'simulated_data_large'\n",
    "    analysis_name = 'criteo_subsampled'\n",
    "\n",
    "    data_dir = os.path.join(os.environ['GIT_REPO_LOC'], 'LRVBLogitGLMM/LogitGLMMLRVB/inst/data/')\n",
    "    json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "    json_output_filename = os.path.join(data_dir, '%s_python_vb_results.json' % analysis_name)\n",
    "\n",
    "    json_file = open(json_filename, 'r')\n",
    "    json_dat = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    stan_dat = json_dat['stan_dat']\n",
    "    vp_base = json_dat['vp_base']\n",
    "\n",
    "    print(stan_dat.keys())\n",
    "    K = stan_dat['K'][0]\n",
    "    NObs = stan_dat['N'][0]\n",
    "    NG = stan_dat['NG'][0]\n",
    "    #N = NObs / NG\n",
    "    y_g_vec = np.array(stan_dat['y_group'])\n",
    "    y_vec = np.array(stan_dat['y'])\n",
    "    x_mat = np.array(stan_dat['x'])\n",
    "    \n",
    "    mu_info_min = vp_base['mu_info_min'][0]\n",
    "    tau_alpha_min = vp_base['tau_alpha_min'][0]\n",
    "    tau_beta_min = vp_base['tau_beta_min'][0]\n",
    "    beta_diag_min = vp_base['beta_diag_min'][0]\n",
    "    u_info_min = vp_base['u_info_min'][0]\n",
    "    \n",
    "    # Define a class to contain prior parameters.\n",
    "    prior_par.push_param(vb.VectorParam('beta_prior_mean', K, val=np.array(stan_dat['beta_prior_mean'])))\n",
    "    beta_prior_info = np.linalg.inv(np.array(stan_dat['beta_prior_var']))\n",
    "    prior_par.push_param(vb.PosDefMatrixParam('beta_prior_info', K, val=beta_prior_info))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_mean', val=stan_dat['mu_prior_mean'][0]))\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_info', val=1 / stan_dat['mu_prior_var'][0]))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_alpha', val=stan_dat['tau_prior_alpha'][0]))\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_beta', val=stan_dat['tau_prior_beta'][0]))\n",
    "\n",
    "    # An index set to make sure jacobians match the order expected by R.\n",
    "    prior_par_indices = copy.deepcopy(prior_par)\n",
    "    prior_par_indices.set_name('Prior Indices')\n",
    "    prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "else:\n",
    "    # Simulate data instead of loading it if you like\n",
    "    N = 200     # observations per group\n",
    "    K = 5      # dimension of regressors\n",
    "    NG = 200      # number of groups\n",
    "\n",
    "    # Generate data\n",
    "\n",
    "    true_beta = np.array(range(5))\n",
    "    true_beta = true_beta - np.mean(true_beta)\n",
    "    true_mu = 0.\n",
    "    true_tau = 40.0\n",
    "\n",
    "    x_mat, y_g_vec, y_vec, true_rho, true_u = \\\n",
    "        logit_glmm.simulate_data(N, NG, true_beta, true_mu, true_tau)\n",
    "\n",
    "    prior_par.push_param(vb.VectorParam('beta_prior_mean', K, val=np.zeros(K)))\n",
    "    prior_par.push_param(vb.PosDefMatrixParam('beta_prior_info', K, val=0.01 * np.eye(K)))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_mean', val=0))\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_info', val=0.5))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_alpha', val=3.0))\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_beta', val=10.0))\n",
    "    \n",
    "    mu_info_min = 0.001\n",
    "    tau_alpha_min = 0.001\n",
    "    tau_beta_min = 0.001\n",
    "    beta_diag_min = 0.001\n",
    "    u_info_min = 0.001\n",
    "    \n",
    "\n",
    "print(np.mean(y_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an object to contain a variational approximation to a K-dimensional multivariate normal.\n",
    "glmm_par = vb.ModelParamsDict('GLMM Parameters')\n",
    "\n",
    "# print(vp_base)\n",
    "\n",
    "glmm_par.push_param(\n",
    "    vb.UVNParam('mu', min_info=mu_info_min))\n",
    "glmm_par.push_param(\n",
    "    vb.GammaParam('tau', min_shape=tau_alpha_min, min_rate=tau_beta_min))\n",
    "glmm_par.push_param(vb.MVNParam('beta', K, min_info=beta_diag_min))\n",
    "glmm_par.push_param(vb.UVNParamVector('u', NG, min_info=u_info_min))\n",
    "\n",
    "# glmm_par.push_param(vb.UVNParam('mu', min_info=0.))\n",
    "# glmm_par.push_param(vb.GammaParam('tau',\n",
    "#                                    min_shape=0.,\n",
    "#                                    min_rate=0.))\n",
    "# glmm_par.push_param(vb.MVNParam('beta', K, min_info=0.))\n",
    "# glmm_par.push_param(vb.UVNParamVector('u', NG, min_info=0.))\n",
    "\n",
    "glmm_init = True\n",
    "if glmm_init and not simulate_data:\n",
    "    # Initialize with GLMM.  Don't forget to add the ADVI computation time to your final VB time!\n",
    "    glmm_time = 0.\n",
    "\n",
    "    glmm_fit = json_dat['glmm_fit']\n",
    "    glmm_par['mu'].mean.set(glmm_fit['mu_mean'][0])\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    tau_mean = 1.0 / glmm_fit['mu_sd'][0] ** 2\n",
    "    tau_var = 1.0\n",
    "    glmm_par['tau'].shape.set((tau_mean ** 2) / tau_var)\n",
    "    glmm_par['tau'].rate.set(tau_var / tau_mean)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.array(glmm_fit['beta_mean']))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.array(glmm_fit['u_map']))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "    free_par_vec = glmm_par.get_free()\n",
    "else:\n",
    "    glmm_time = 0.\n",
    "    glmm_par['mu'].mean.set(0.0)\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    glmm_par['tau'].shape.set(2.0)\n",
    "    glmm_par['tau'].rate.set(2.0)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.full(K, 0.0))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.full(NG, 0.0))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "free_par_vec = glmm_par.get_free()\n",
    "init_par_vec = copy.deepcopy(free_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trlib has a memory error if the dataset is too large.\n",
    "if False:\n",
    "    import trlib\n",
    "\n",
    "    glmm_par_local = copy.deepcopy(glmm_par)\n",
    "    def foo():\n",
    "        return sum(glmm_par_local.get_vector())\n",
    "\n",
    "\n",
    "    tr_obj = Objective(glmm_par_local, foo)\n",
    "\n",
    "\n",
    "    def f(par):\n",
    "        return -1. * tr_obj.fun_free(par)\n",
    "\n",
    "    def g(par):\n",
    "        return -1. * tr_obj.fun_free_grad(par)\n",
    "\n",
    "    def hvp(par, vec):\n",
    "        return -1. * tr_obj.fun_free_hvp(par, vec)\n",
    "\n",
    "    # f(init_par_vec)\n",
    "    # g(init_par_vec)\n",
    "\n",
    "    trlib.umin(f, g, hvp, init_par_vec)\n",
    "    \n",
    "    #print(init_par_vec)\n",
    "    print(objective.fun_free(init_par_vec, verbose=True)[0])\n",
    "    print(objective.fun_free_grad(init_par_vec))\n",
    "    print(objective.fun_free_hvp(init_par_vec, init_par_vec))\n",
    "\n",
    "    init_par_vec = np.random.random(model.glmm_par.free_size())\n",
    "\n",
    "    # Memory error with large datasets.\n",
    "    # tr_min = trlib.umin(\n",
    "    #     obj=objective.fun_free,\n",
    "    #     grad=objective.fun_free_grad,\n",
    "    #     hessvec=objective.fun_free_hvp,\n",
    "    #     x=init_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function time:\n",
      "0.03483127780054929\n",
      "Grad time:\n",
      "0.0759040716002346\n",
      "Hessian vector product time:\n",
      "0.15718193720094858\n"
     ]
    }
   ],
   "source": [
    "# Define moment parameters\n",
    "\n",
    "moment_wrapper = logit_glmm.MomentWrapper(glmm_par)\n",
    "get_moment_jacobian = jacobian(moment_wrapper.get_moments)\n",
    "\n",
    "# Moment indices.\n",
    "moment_indices = copy.deepcopy(moment_wrapper.moment_par)\n",
    "moment_indices.set_vector(1 + np.array(range(moment_indices.vector_size())))\n",
    "\n",
    "model = logit_glmm.LogisticGLMM(glmm_par, prior_par, x_mat, y_vec, y_g_vec, 10)\n",
    "model.get_e_log_prior()\n",
    "model.get_log_lik()\n",
    "model.get_entropy()\n",
    "\n",
    "objective = Objective(model.glmm_par, model.get_kl)\n",
    "objective.fun_free(free_par_vec)\n",
    "\n",
    "# # PriorHess evaluates the second order derivative d2 EPrior / dpar dprior_par\n",
    "# PriorModelGrad = grad(kl_wrapper.ExpectedLogPrior, argnum=0)\n",
    "# PriorHess = jacobian(PriorModelGrad, argnum=1)\n",
    "\n",
    "# kl_wrapper.ExpectedLogPrior(free_par_vec, prior_par.get_vector())\n",
    "\n",
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "print('Function time:')\n",
    "print(timeit.timeit(lambda: objective.fun_free(free_par_vec), number=time_num) / time_num)\n",
    "\n",
    "print('Grad time:')\n",
    "print(timeit.timeit(lambda: objective.fun_free_grad(free_par_vec), number=time_num) / time_num)\n",
    "\n",
    "print('Hessian vector product time:')\n",
    "print(timeit.timeit(lambda: objective.fun_free_hvp(free_par_vec, free_par_vec + 1), number=time_num) / time_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "def tr_optimize(trust_init, num_draws, maxiter=500):\n",
    "    model.set_draws(num_draws)\n",
    "    objective.logger.initialize()\n",
    "    objective.logger.print_every = 5\n",
    "    vb_opt = optimize.minimize(\n",
    "        lambda par: objective.fun_free(par, verbose=True),\n",
    "        x0=trust_init,\n",
    "        method='trust-ncg',\n",
    "        jac=objective.fun_free_grad,\n",
    "        hessp=objective.fun_free_hvp,\n",
    "        tol=1e-6, options={'maxiter': maxiter, 'disp': True, 'gtol': 1e-6 })\n",
    "    return vb_opt.x\n",
    "\n",
    "def get_moment_vec(vb_opt_x):\n",
    "    glmm_par_opt.set_free(vb_opt_x)\n",
    "    set_moments(glmm_par_opt, moment_par)\n",
    "    return moment_par.get_vector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#print(glmm_par)\n",
    "\n",
    "x_t_x = np.matmul(x_mat.transpose(), x_mat)\n",
    "x_t_y = np.matmul(x_mat.transpose(), y_vec)\n",
    "beta_init = np.linalg.solve(x_t_x, x_t_y)\n",
    "#print(beta_init)\n",
    "#plt.plot(sp.special.expit(np.matmul(x_mat, beta_init)), y_vec, 'k.')\n",
    "\n",
    "df = pd.DataFrame({ 'y_g': y_g_vec, 'y': y_vec}).groupby('y_g')\n",
    "#print(df.sum())\n",
    "u_init = np.array(df.sum()) / np.array(df.count()['y'])\n",
    "#plt.figure()\n",
    "print(np.min(y_g_vec))\n",
    "#plt.plot(u_init[y_g_vec], y_vec, 'k.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region\n",
      "Iter  0  value:  [ 29761.29785755]\n",
      "Iter  5  value:  [ 24766.4102803]\n",
      "Iter  10  value:  [ 24307.28296656]\n",
      "Iter  15  value:  [ 24307.01844132]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 24307.018111\n",
      "         Iterations: 18\n",
      "         Function evaluations: 19\n",
      "         Gradient evaluations: 19\n",
      "         Hessian evaluations: 0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Optimize.\n",
    "num_mc_draws = 3\n",
    "\n",
    "print('Running Newton Trust Region')\n",
    "vb_time = time.time()\n",
    "opt_x = tr_optimize(init_par_vec, num_mc_draws, maxiter=100)\n",
    "vb_time = time.time() - vb_time\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit beta:  [-2.01458326 -0.99108398  0.01353893  0.94533269  1.99489905]\n",
      "True beta:  [-2. -1.  0.  1.  2.]\n",
      "Fit mu:  [ 0.00016797]\n",
      "True mu:  0.0\n",
      "Fit tau:  [ 5.25012722]\n",
      "True tau:  40.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb450671f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glmm_par.set_free(opt_x)\n",
    "if simulate_data:\n",
    "    print('Fit beta: ', glmm_par['beta'].e())\n",
    "    print('True beta: ', true_beta)\n",
    "    \n",
    "    print('Fit mu: ', glmm_par['mu'].e())\n",
    "    print('True mu: ', true_mu)\n",
    "    \n",
    "    print('Fit tau: ', glmm_par['tau'].e())\n",
    "    print('True tau: ', true_tau)\n",
    "    \n",
    "    plt.plot(true_u, true_u, 'r.')\n",
    "    plt.plot(true_u, glmm_par['u'].e(), 'k.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glmm_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-14a50e1f7087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mglmm_par_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_free\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#glmm_par_opt.set_free(init_par_vec)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglmm_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglmm_par_opt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglmm_par_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glmm_fit' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(opt_x)\n",
    "#glmm_par_opt.set_free(init_par_vec)\n",
    "print(glmm_fit['beta_mean'])\n",
    "print(glmm_par_opt['beta'].e())\n",
    "print(glmm_par_opt)\n",
    "\n",
    "#plt.plot(glmm_par_opt['u'].e(), glmm_par_opt['u'].var(), 'k.')\n",
    "\n",
    "e_beta = glmm_par_opt['beta'].e()\n",
    "e_u = glmm_par_opt['u'].e()[model.y_g_vec]\n",
    "\n",
    "z_mean = e_u + np.matmul(model.x_mat, e_beta)\n",
    "#plt.plot(sp.special.expit(z_mean), model.y_vec, 'k.')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(z_mean[model.y_vec == 1], 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.title('y == 1')\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(z_mean[model.y_vec == 0], 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.title('y == 0')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(opt_x))\n",
    "print(glmm_par.free_size())\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(opt_x)\n",
    "moment_wrapper.glmm_par.set_free(opt_x)\n",
    "moment_wrapper.set_moments()\n",
    "\n",
    "print(vb_time / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Investigate the performance of different numbers of draws.   It doesn't appear to\n",
    "    # converge.\n",
    "    opt_x_20 = tr_optimize(init_par_vec, 20)\n",
    "    opt_x_60 = tr_optimize(opt_x_20, 60)\n",
    "    opt_x_100 = tr_optimize(opt_x_60, 100)\n",
    "    opt_x_200 = tr_optimize(opt_x_100, 200)\n",
    "    opt_x_400 = tr_optimize(opt_x_200, 400)\n",
    "    opt_x_800 = tr_optimize(opt_x_400, 800)\n",
    "    \n",
    "    mom_20 = get_moment_vec(opt_x_20)\n",
    "    mom_60 = get_moment_vec(opt_x_60)\n",
    "    mom_100 = get_moment_vec(opt_x_100)\n",
    "    mom_200 = get_moment_vec(opt_x_200)\n",
    "    mom_400 = get_moment_vec(opt_x_400)\n",
    "    mom_800 = get_moment_vec(opt_x_800)\n",
    "\n",
    "    print np.max(np.abs((mom_20 - mom_60) / mom_20))\n",
    "    print np.max(np.abs((mom_60 - mom_100) / mom_60))\n",
    "    print np.max(np.abs((mom_100 - mom_200) / mom_100))\n",
    "    print np.max(np.abs((mom_200 - mom_400) / mom_200))\n",
    "    print np.max(np.abs((mom_400 - mom_800) / mom_400))\n",
    "\n",
    "    print '-------\\n'\n",
    "    print np.max(np.abs((mom_20 - mom_60)))\n",
    "    print np.max(np.abs((mom_60 - mom_100)))\n",
    "    print np.max(np.abs((mom_100 - mom_200)))\n",
    "    print np.max(np.abs((mom_200 - mom_400)))\n",
    "    print np.max(np.abs((mom_400 - mom_800)))\n",
    "\n",
    "    #diff_inds = np.where(np.abs(mom_60 - mom_100) > 1e-2)\n",
    "    #print diff_inds\n",
    "    #print moment_indices\n",
    "\n",
    "    #print (get_moment_vec(opt_x_60) - get_moment_vec(opt_x_100)) / np.abs(get_moment_vec(opt_x_100))\n",
    "    get_moment_vec(opt_x_200)\n",
    "    u200 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_400)\n",
    "    u400 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_800)\n",
    "    u800 = copy.deepcopy(moment_par['e_u'].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine why the means are different for different number of simulations.\n",
    "def get_logit_terms(num_draws):\n",
    "    model.set_draws(num_draws)\n",
    "    std_draws = model.std_draws\n",
    "\n",
    "    e_beta = glmm_par_opt['beta'].mean.get()\n",
    "    info_beta = glmm_par_opt['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "\n",
    "    e_u = glmm_par_opt['u'].mean.get()[y_g_vec]\n",
    "    info_u = glmm_par_opt['u'].info.get()[y_g_vec]\n",
    "    var_u = 1 / info_u\n",
    "\n",
    "    z_mean = e_u + np.matmul(x_mat, e_beta)\n",
    "    z_sd = np.sqrt(var_u + np.einsum('nk,kj,nj->n', x_mat, cov_beta, x_mat))\n",
    "    z = np.einsum('i,j->ij', z_sd, std_draws) + np.expand_dims(z_mean, 1)\n",
    "\n",
    "    # The sum is over observations and draws, so dividing by the draws size\n",
    "    # gives the sum of sample expectations over the draws.\n",
    "    # p = exp(z) / (1 + exp(z))\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_terms = np.log1p(np.exp(z))\n",
    "    logit_term = -np.sum(logit_terms) / std_draws.size\n",
    "\n",
    "    return logit_term, logit_terms, z\n",
    "    \n",
    "logit_term_50, logit_terms_50, z_50 = get_logit_terms(50)    \n",
    "logit_term_800, logit_terms_800, z_800 = get_logit_terms(800)\n",
    "\n",
    "print( logit_term_50)\n",
    "print( logit_term_800)\n",
    "\n",
    "logit_terms_50_mean = np.mean(logit_terms_50, 1)\n",
    "logit_terms_800_mean = np.mean(logit_terms_800, 1)\n",
    "\n",
    "print( np.max(np.abs(logit_terms_50_mean - logit_terms_800_mean)))\n",
    "print( np.where(np.abs(logit_terms_50_mean - logit_terms_800_mean) > 1e-3))\n",
    "\n",
    "ind = 3\n",
    "plt.plot(z_800[ind, :], logit_terms_800[ind, :])\n",
    "plt.plot(z_50[ind, :], logit_terms_50[ind, :])\n",
    "print( logit_terms_50_mean[ind])\n",
    "print( logit_terms_800_mean[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hessians at the number of draws used for optimization.\n",
    "\n",
    "model.set_draws(num_mc_draws)\n",
    "\n",
    "hess_time = time.time()\n",
    "print('KL Hessian:\\n')\n",
    "kl_hess = objective.fun_free_hessian(opt_x)\n",
    "\n",
    "print('Log prior Hessian:\\n')\n",
    "log_prior_hess = PriorHess(opt_x, prior_par.get_vector())\n",
    "\n",
    "hess_time =  time.time() - hess_time\n",
    "elbo_hess = -kl_hess\n",
    "\n",
    "print('hess_time: %f' % hess_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moment_jac = MomentJacobian(opt_x)\n",
    "lrvb_cov = np.matmul(moment_jac, np.linalg.solve(kl_hess, moment_jac.T))\n",
    "\n",
    "prior_indices = copy.deepcopy(prior_par)\n",
    "prior_indices.set_vector(1 + np.array(range(prior_indices.vector_size())))\n",
    "\n",
    "vp_indices = copy.deepcopy(glmm_par_opt)\n",
    "vp_indices.set_vector(1 + np.array(range(vp_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not simulate_data:\n",
    "    # Write the result to a JSON file for use in R.\n",
    "    \n",
    "    run_name = 'production'\n",
    "    result_dict = { 'glmm_par_opt': glmm_par_opt.dictval(), 'run_name': run_name,\n",
    "                    'vb_time': vb_time, 'hess_time': hess_time, 'num_mc_draws': num_mc_draws, \n",
    "                    'moment_indices': moment_indices.dictval(),\n",
    "                    'prior_indices': prior_indices.dictval(),\n",
    "                    'vp_indices': vp_indices.dictval(),\n",
    "                    'lrvb_cov': lrvb_cov.tolist(), 'moment_jac': moment_jac.tolist(),\n",
    "                    'elbo_hess': elbo_hess.tolist(), 'log_prior_hess': log_prior_hess.tolist() }\n",
    "\n",
    "    result_json = json.dumps(result_dict)\n",
    "    json_file = open(json_output_filename, 'w')\n",
    "    json_file.write(result_json)\n",
    "    json_file.close()\n",
    "\n",
    "    print(json_output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
