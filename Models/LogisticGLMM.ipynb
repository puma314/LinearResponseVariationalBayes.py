{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from VariationalBayes import ScalarParam, ModelParamsDict, VectorParam, PosDefMatrixParam\n",
    "# from VariationalBayes.NormalParams import MVNParam, UVNParam, UVNParamVector\n",
    "# from VariationalBayes.GammaParams import GammaParam\n",
    "# from VariationalBayes.ExponentialFamilies import \\\n",
    "#     univariate_normal_entropy, multivariate_normal_entropy, gamma_entropy, \\\n",
    "#     mvn_prior, uvn_prior, gamma_prior\n",
    "\n",
    "import VariationalBayes as vb\n",
    "import LogisticGLMM_lib as logit_glmm\n",
    "from VariationalBayes.SparseObjectives import Objective, SparseObjective\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import autograd.numpy as np\n",
    "import numpy as np\n",
    "\n",
    "from autograd import jacobian\n",
    "# import autograd.numpy as np\n",
    "# import autograd.numpy.random as npr\n",
    "# import autograd.scipy as sp\n",
    "# import scipy as osp\n",
    "\n",
    "import copy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4973\n"
     ]
    }
   ],
   "source": [
    "# Load data saved by stan_results_to_json.R and run_stan.R in LRVBLogitGLMM.\n",
    "import os\n",
    "import json\n",
    "\n",
    "simulate_data = True\n",
    "prior_par = vb.ModelParamsDict('Prior Parameters')\n",
    "\n",
    "if not simulate_data:\n",
    "    #analysis_name = 'simulated_data_small'\n",
    "    #analysis_name = 'simulated_data_large'\n",
    "    analysis_name = 'criteo_subsampled'\n",
    "\n",
    "    data_dir = os.path.join(os.environ['GIT_REPO_LOC'], 'LRVBLogitGLMM/LogitGLMMLRVB/inst/data/')\n",
    "    json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "    json_output_filename = os.path.join(data_dir, '%s_python_vb_results.json' % analysis_name)\n",
    "\n",
    "    json_file = open(json_filename, 'r')\n",
    "    json_dat = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    stan_dat = json_dat['stan_dat']\n",
    "    vp_base = json_dat['vp_base']\n",
    "\n",
    "    print(stan_dat.keys())\n",
    "    K = stan_dat['K'][0]\n",
    "    NObs = stan_dat['N'][0]\n",
    "    NG = stan_dat['NG'][0]\n",
    "    #N = NObs / NG\n",
    "    y_g_vec = np.array(stan_dat['y_group'])\n",
    "    y_vec = np.array(stan_dat['y'])\n",
    "    x_mat = np.array(stan_dat['x'])\n",
    "    \n",
    "    mu_info_min = vp_base['mu_info_min'][0]\n",
    "    tau_alpha_min = vp_base['tau_alpha_min'][0]\n",
    "    tau_beta_min = vp_base['tau_beta_min'][0]\n",
    "    beta_diag_min = vp_base['beta_diag_min'][0]\n",
    "    u_info_min = vp_base['u_info_min'][0]\n",
    "    \n",
    "    # Define a class to contain prior parameters.\n",
    "    prior_par.push_param(vb.VectorParam('beta_prior_mean', K, val=np.array(stan_dat['beta_prior_mean'])))\n",
    "    beta_prior_info = np.linalg.inv(np.array(stan_dat['beta_prior_var']))\n",
    "    prior_par.push_param(vb.PosDefMatrixParam('beta_prior_info', K, val=beta_prior_info))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_mean', val=stan_dat['mu_prior_mean'][0]))\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_info', val=1 / stan_dat['mu_prior_var'][0]))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_alpha', val=stan_dat['tau_prior_alpha'][0]))\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_beta', val=stan_dat['tau_prior_beta'][0]))\n",
    "\n",
    "    # An index set to make sure jacobians match the order expected by R.\n",
    "    prior_par_indices = copy.deepcopy(prior_par)\n",
    "    prior_par_indices.set_name('Prior Indices')\n",
    "    prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "else:\n",
    "    # Simulate data instead of loading it if you like\n",
    "    N = 200     # observations per group\n",
    "    K = 5      # dimension of regressors\n",
    "    NG = 200      # number of groups\n",
    "\n",
    "    # Generate data\n",
    "\n",
    "    true_beta = np.array(range(5))\n",
    "    true_beta = true_beta - np.mean(true_beta)\n",
    "    true_mu = 0.\n",
    "    true_tau = 40.0\n",
    "\n",
    "    x_mat, y_g_vec, y_vec, true_rho, true_u = \\\n",
    "        logit_glmm.simulate_data(N, NG, true_beta, true_mu, true_tau)\n",
    "\n",
    "    prior_par.push_param(vb.VectorParam('beta_prior_mean', K, val=np.zeros(K)))\n",
    "    prior_par.push_param(vb.PosDefMatrixParam('beta_prior_info', K, val=0.01 * np.eye(K)))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_mean', val=0))\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_info', val=0.5))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_alpha', val=3.0))\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_beta', val=10.0))\n",
    "    \n",
    "    mu_info_min = 0.001\n",
    "    tau_alpha_min = 0.001\n",
    "    tau_beta_min = 0.001\n",
    "    beta_diag_min = 0.001\n",
    "    u_info_min = 0.001\n",
    "    \n",
    "\n",
    "print(np.mean(y_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an object to contain a variational approximation to a K-dimensional multivariate normal.\n",
    "glmm_par = vb.ModelParamsDict('GLMM Parameters')\n",
    "\n",
    "# print(vp_base)\n",
    "\n",
    "glmm_par.push_param(\n",
    "    vb.UVNParam('mu', min_info=mu_info_min))\n",
    "glmm_par.push_param(\n",
    "    vb.GammaParam('tau', min_shape=tau_alpha_min, min_rate=tau_beta_min))\n",
    "glmm_par.push_param(vb.MVNParam('beta', K, min_info=beta_diag_min))\n",
    "glmm_par.push_param(vb.UVNParamVector('u', NG, min_info=u_info_min))\n",
    "\n",
    "# glmm_par.push_param(vb.UVNParam('mu', min_info=0.))\n",
    "# glmm_par.push_param(vb.GammaParam('tau',\n",
    "#                                    min_shape=0.,\n",
    "#                                    min_rate=0.))\n",
    "# glmm_par.push_param(vb.MVNParam('beta', K, min_info=0.))\n",
    "# glmm_par.push_param(vb.UVNParamVector('u', NG, min_info=0.))\n",
    "\n",
    "glmm_init = True\n",
    "if glmm_init and not simulate_data:\n",
    "    # Initialize with GLMM.  Don't forget to add the ADVI computation time to your final VB time!\n",
    "    glmm_time = 0.\n",
    "\n",
    "    glmm_fit = json_dat['glmm_fit']\n",
    "    glmm_par['mu'].mean.set(glmm_fit['mu_mean'][0])\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    tau_mean = 1.0 / glmm_fit['mu_sd'][0] ** 2\n",
    "    tau_var = 1.0\n",
    "    glmm_par['tau'].shape.set((tau_mean ** 2) / tau_var)\n",
    "    glmm_par['tau'].rate.set(tau_var / tau_mean)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.array(glmm_fit['beta_mean']))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.array(glmm_fit['u_map']))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "    free_par_vec = glmm_par.get_free()\n",
    "else:\n",
    "    glmm_time = 0.\n",
    "    glmm_par['mu'].mean.set(0.0)\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    glmm_par['tau'].shape.set(2.0)\n",
    "    glmm_par['tau'].rate.set(2.0)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.full(K, 0.0))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.full(NG, 0.0))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "free_par_vec = glmm_par.get_free()\n",
    "init_par_vec = copy.deepcopy(free_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trlib has a memory error if the dataset is too large.\n",
    "if False:\n",
    "    import trlib\n",
    "\n",
    "    glmm_par_local = copy.deepcopy(glmm_par)\n",
    "    def foo():\n",
    "        return sum(glmm_par_local.get_vector())\n",
    "\n",
    "\n",
    "    tr_obj = Objective(glmm_par_local, foo)\n",
    "\n",
    "\n",
    "    def f(par):\n",
    "        return -1. * tr_obj.fun_free(par)\n",
    "\n",
    "    def g(par):\n",
    "        return -1. * tr_obj.fun_free_grad(par)\n",
    "\n",
    "    def hvp(par, vec):\n",
    "        return -1. * tr_obj.fun_free_hvp(par, vec)\n",
    "\n",
    "    # f(init_par_vec)\n",
    "    # g(init_par_vec)\n",
    "\n",
    "    trlib.umin(f, g, hvp, init_par_vec)\n",
    "    \n",
    "    #print(init_par_vec)\n",
    "    print(objective.fun_free(init_par_vec, verbose=True)[0])\n",
    "    print(objective.fun_free_grad(init_par_vec))\n",
    "    print(objective.fun_free_hvp(init_par_vec, init_par_vec))\n",
    "\n",
    "    init_par_vec = np.random.random(model.glmm_par.free_size())\n",
    "\n",
    "    # Memory error with large datasets.\n",
    "    # tr_min = trlib.umin(\n",
    "    #     obj=objective.fun_free,\n",
    "    #     grad=objective.fun_free_grad,\n",
    "    #     hessvec=objective.fun_free_hvp,\n",
    "    #     x=init_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 76960.14605639])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define moment parameters\n",
    "\n",
    "moment_wrapper = logit_glmm.MomentWrapper(glmm_par)\n",
    "get_moment_jacobian = jacobian(moment_wrapper.get_moments)\n",
    "\n",
    "# Moment indices.\n",
    "moment_indices = copy.deepcopy(moment_wrapper.moment_par)\n",
    "moment_indices.set_vector(1 + np.array(range(moment_indices.vector_size())))\n",
    "\n",
    "model = logit_glmm.LogisticGLMM(glmm_par, prior_par, x_mat, y_vec, y_g_vec, 10)\n",
    "model.get_e_log_prior()\n",
    "model.get_log_lik()\n",
    "model.get_entropy()\n",
    "\n",
    "objective = Objective(model.glmm_par, model.get_kl)\n",
    "objective.fun_free(free_par_vec)\n",
    "\n",
    "# # PriorHess evaluates the second order derivative d2 EPrior / dpar dprior_par\n",
    "# PriorModelGrad = grad(kl_wrapper.ExpectedLogPrior, argnum=0)\n",
    "# PriorHess = jacobian(PriorModelGrad, argnum=1)\n",
    "\n",
    "# kl_wrapper.ExpectedLogPrior(free_par_vec, prior_par.get_vector())\n",
    "\n",
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "print('Function time:')\n",
    "print(timeit.timeit(lambda: objective.fun_free(free_par_vec), number=time_num) / time_num)\n",
    "\n",
    "print('Grad time:')\n",
    "print(timeit.timeit(lambda: objective.fun_free_grad(free_par_vec), number=time_num) / time_num)\n",
    "\n",
    "print('Hessian vector product time:')\n",
    "print(timeit.timeit(lambda: objective.fun_free_hvp(free_par_vec, free_par_vec + 1), number=time_num) / time_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "def tr_optimize(trust_init, num_draws, maxiter=500):\n",
    "    model.set_draws(num_draws)\n",
    "    objective.logger.initialize()\n",
    "    objective.logger.print_every = 5\n",
    "    vb_opt = optimize.minimize(\n",
    "        lambda par: objective.fun_free(par, verbose=True),\n",
    "        x0=trust_init,\n",
    "        method='trust-ncg',\n",
    "        jac=objective.fun_free_grad,\n",
    "        hessp=objective.fun_free_hvp,\n",
    "        tol=1e-6, options={'maxiter': maxiter, 'disp': True, 'gtol': 1e-6 })\n",
    "    return vb_opt.x\n",
    "\n",
    "def get_moment_vec(vb_opt_x):\n",
    "    glmm_par_opt.set_free(vb_opt_x)\n",
    "    set_moments(glmm_par_opt, moment_par)\n",
    "    return moment_par.get_vector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#print(glmm_par)\n",
    "\n",
    "x_t_x = np.matmul(x_mat.transpose(), x_mat)\n",
    "x_t_y = np.matmul(x_mat.transpose(), y_vec)\n",
    "beta_init = np.linalg.solve(x_t_x, x_t_y)\n",
    "#print(beta_init)\n",
    "#plt.plot(sp.special.expit(np.matmul(x_mat, beta_init)), y_vec, 'k.')\n",
    "\n",
    "df = pd.DataFrame({ 'y_g': y_g_vec, 'y': y_vec}).groupby('y_g')\n",
    "#print(df.sum())\n",
    "u_init = np.array(df.sum()) / np.array(df.count()['y'])\n",
    "#plt.figure()\n",
    "print(np.min(y_g_vec))\n",
    "#plt.plot(u_init[y_g_vec], y_vec, 'k.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region\n",
      "Iter  0  value:  [ 74944.26335334]\n",
      "Iter  5  value:  [ 36768.81753565]\n",
      "Iter  10  value:  [-5502.47038227]\n",
      "Iter  15  value:  [-22668.56078258]\n",
      "Iter  20  value:  [-53529.42243194]\n",
      "Iter  25  value:  [-87539.33620241]\n",
      "Iter  30  value:  [-105864.99490848]\n",
      "Iter  35  value:  [-106450.16937417]\n",
      "Iter  40  value:  [-106468.78481938]\n",
      "Iter  45  value:  [-106468.84727557]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -106468.847276\n",
      "         Iterations: 47\n",
      "         Function evaluations: 48\n",
      "         Gradient evaluations: 46\n",
      "         Hessian evaluations: 0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Optimize.\n",
    "num_mc_draws = 3\n",
    "\n",
    "print('Running Newton Trust Region')\n",
    "vb_time = time.time()\n",
    "opt_x = tr_optimize(init_par_vec, num_mc_draws, maxiter=100)\n",
    "vb_time = time.time() - vb_time\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit beta:  [-2.00245181 -1.01682419  0.02317739  0.95615767  1.97296369]\n",
      "True beta:  [-2. -1.  0.  1.  2.]\n",
      "Fit mu:  [-0.0099802]\n",
      "True mu:  0.0\n",
      "Fit tau:  [ 1990.24821949]\n",
      "True tau:  40.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAD8CAYAAACl69mTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+QHGd95/H3d2Z/WA4xkRZs6ywvchI7iYlDiDfA1lVg\nE8nCgTvsOhOCK2EVy1i1wdRhOEKkMirrEJGMOWLBkZgV/pHV3VXCD5PDTnAcac3GruwaI2NI+GVL\nGJAlZBxkA+EoS9bu9/6YZ5beYWanZ7p7pmfm86qaUvczz3Q/T6+qv/386G5zd0RERLJQaHcBRESk\neynIiIhIZhRkREQkMwoyIiKSGQUZERHJjIKMiIhkRkFGREQyoyAjIiKZUZAREZHM9LW7AO32ghe8\nwNeuXdvuYoiIdJSHH374e+7+wnr5ej7IrF27lgMHDrS7GCIiHcXMvh0nn7rLREQkMwoyIiKSGQUZ\nERHJjIKMiIhkRkFGREQyoyAjIiKZUZAREUnB3Nwcu3btYm5urt1FyZWev09GRCSpubk51q1bx8mT\nJxkYGGB6eprR0dF2FysX1JIREUloZmaGkydPMj8/z8mTJ5mZmWl3kXJDQUZEJKGxsTEGBgYoFosM\nDAwwNjbW7iLlRqIgY2arzGyfmR0M/66skW9jyHPQzDZG0i82s381s0Nm9iEzs5D+EjObC9/dbWZn\nhPQBM7sjpH/JzMZC+ulm9vdm9nUz+4qZ3ZikXiLSep08pjE6Osr09DQ7duxQV1kld2/6A9wEbAnL\nW4D3VcmzCng8/LsyLK8M3z0EvAIw4B7gd0P654FXheVNwI6wfC1wR1g+E3iYUqA8HfjtkD4APFDe\nVr3PxRdf7CLSXrOzs75ixQovFou+YsUKn52dbXeRpA7ggMc4xybtLrsMmArLU8DlVfK8Gtjn7k+7\n+zPAPuBSM1sNnOHuD4YC7438/gLg/rC8D7giLF8I3Afg7k8B3wdG3P3H7v7ZkH4S+AKwJmHdRKRF\nNKbRvZIGmbPc/VhYfhI4q0qec4AnIutHQto5YbkyHeArlAIYwO8B54blLwGvM7M+MzsPuDjyHQBm\n9nPAfwamm6mQiLSexjS6V90pzGa2Hzi7ylfXR1fc3c3MUyrXJuBDZrYNuAs4GdJvB34FOAB8G5gF\n5iNl7QP+GviQuz9ea+NmthnYDDA8PJxSkUV6z9zcHDMzM4yNjSUahyiPaaSxLcmXukHG3dfX+s7M\nvmtmq939WOj+eqpKtqPAWGR9DTAT0tdUpB8N+/w6sCHs4wLgtSH9FPD2yP5ngcci29gDHHT33XXq\ntCfkZWRkJK3AKNJT0r43ZHR0VMGlCyXtLrsLKM8W2wh8ukqee4ENZrYyzD7bANwbutl+aGavCLPK\nxsu/N7Mzw78F4N3AR8L66Wb2M2H5EuCUu381rL8XeD5wXcI6iUgMGkeROJIGmRuBS8zsILA+rGNm\nI2Z2K4C7Pw3soDRj7PPAe0IawFuAW4FDwDcozTADuNLMHgO+DnwHuCOknwl8wcy+Bvwp8KawvzWU\nuu8uDN9/0czenLBuIrIMjaNIHFaa2NW7RkZGXK9fFmlOWmMy0nnM7GF3H6mXT88uE5GmaRxF6tFj\nZUREJDMKMiIikhkFGRERyYyCjEiDOvlBjiKtpoF/kQbo5VQijVFLRqQBugFRpDEKMiIN0A2IIo1R\nd5lIA/QgR5HGKMiINEg3IIrEp+4yERHJjIKMiIhkRkFGRKQK3Q+VDo3JiIhU0P1Q6VFLRkRyqZ0t\nCd0PlR61ZEQkd9rdkijfD1Xev+6Hap5aMiI9opPGGNrdkijfD7Vjxw51lSWkloxID2h3y6BReWhJ\n6H6odCjIiPSAai2DPJ9A9WSF7qEgI9ID8tAyaFQzLYm5uTkFppxJFGTMbBXwMWAt8C3gDe7+TJV8\nG4F3h9X3uvtUSL8Y+CtgBfAZ4G3u7mb2EuAjwPPCdv/A3X9oZgPAJDACLIT8MxX7ugv4eXf/1SR1\nE+km7WgZtPqE32ldgr0iaUtmCzDt7jea2Zaw/qfRDCEQ3UApMDjwsJndFYLRLcA1wOcoBZlLgXuA\nW4F3uvs/mdkm4E+AbSEv7n6RmZ0J3GNmv+nuC2Ff/wX4UcI6iXSlVo4xtOOE32ldgr0i6eyyy4Cp\nsDwFXF4lz6uBfe7+dAgs+4BLzWw1cIa7P+juDuyN/P4C4P6wvA+4IixfCNwH4O5PAd+nFLwws+cB\n7wDem7BOIj0v6Uy0dswO02sY8ilpS+Ysdz8Wlp8EzqqS5xzgicj6kZB2TliuTAf4CqUA9n+B3wPO\nDelfAl5nZn8d0i4O/z4E7AA+APw4WZVEelsarZB2jAFpskA+1Q0yZrYfOLvKV9dHV8JYiqdUrk3A\nh8xsG3AXcDKk3w78CnAA+DYwC8yb2a8Dv+DubzeztfU2bmabgc0Aw8PDKRVZpDuk0e3UrhN+Xqcd\n9/KEhLpBxt3X1/rOzL5rZqvd/Vjo/nqqSrajwFhkfQ0wE9LXVKQfDfv8OrAh7OMC4LUh/RTw9sj+\nZ4HHgFcBI2b2rVCnM81sxt2j+43WaQ+wB2BkZCStwCjSFdJqhbTrhD83N8fevXsBGB8fb9tJvRxY\nhoaGuO6663p3QoK7N/0B3g9sCctbgJuq5FkFfBNYGT7fBFaF7x4CXgEYpQH/14T0M8O/BUpjNZvC\n+unAz4TlS4D7q+xvLfDluHW4+OKLXUSWmp2d9Z07d/rs7Gy7i9KQ2dlZHxgYcEqTjHxwcLAtdZid\nnfUVK1Z4sVj0/v5+LxQKDnixWPSdO3e2vDzVypf07wsc8Bjn2KRjMjcCHzezqyl1X70BwMxGgAl3\nf7O7P21mO4DPh9+8x92fDstv4SdTmO8JH4ArzezasPwp4I6wfCZwr5ktUGr1vClh+UWkirx2O9Uz\nMzPDc889t7jerllm0S5Hd6dQKGBmuZiQ0OqZf4mCjLsfB9ZVST8AvDmyfjul8ZRq+X7qfhZ3/yDw\nwSrp3wJ+qU6ZvlVtmyKSnVaOOSy3r7GxMfr7+zl5sjSM266TemWX4+7duzl+/HguxmRaPdVbd/yL\nSCKtvDKut6/R0VFmZmbaPiaT55lurZ75pyAjIomkcWUctyUUZ1956erLSzkqtToAKsiIyKJmur2S\nXhk30hLqxGew5VErA6CCjIgAzXd7Jb0ybqQllOduKKlOQUZEgGTdXkmujBttncTdVy/fAJknCjIi\nAjR+sk/rJJ5F60RPZM4PBRkRARo72ad9Ek97jEBPZM4PBRkRWRT3ZJ/VSTyt1pEmCOSHgoxIhrp1\nXCB6Eu/r6+Pw4cPMzc0lqmOarSNNEMiROM+e6eaPnl0mWYk+v2rFihUd9xywemZnZ31iYsIHBgZS\nqePOnTu9WCzm6hlfUhsxn12W9KVlIlJDVi/uSvpCsTQ9/vjjnDp1KpU6VnvpWNp1zdOx6xXqLhPJ\nSBbjAnmZNVUux4kTJ1hYWKBQKCSuY2UXF5BqXfNy7HqNWjLSE9pxBVs+ae7YsSO1E1pWraNmy1EO\nMOvXr0+ljqOjo2zdunXxGWRx6xrn75uXY9dr1JKRrtfOK9i0p+a2etZUrYkLleXYvn176sc0bl3j\n/n0146w9FGSk63XTPROtnDW13Mm7FeWIu4+4f1/NOGsPBRnpet12BduqhxvWO3m3ohxx9tHI3zev\nT0buZgoy0vV0BducTgnO+vvmm5WmO/eukZERP3DgQLuLIR2qW2+2LOv2+kFv1DELZvawu4/Uy6eW\njEiTemFKbLd3L/XC37DdNIVZpElpT4nN242CeStPFjStOXtqyYg0Kc0xi1ZdUcftGuqVK/xOGXfq\nZIlaMma2ysz2mdnB8O/KGvk2hjwHzWxjJP1iM/tXMztkZh8yMwvpLzGzufDd3WZ2RkgfMLM7QvqX\nzGwssq0BM9tjZo+Z2dfN7IokdROpJ82bLVtxRV0OHNu2bWPdunW6cZFsbpiVpZK2ZLYA0+5+o5lt\nCet/Gs1gZquAG4ARwIGHzewud38GuAW4Bvgc8BngUuAe4Fbgne7+T2a2CfgTYFvIi7tfZGZnAveY\n2W+6+wJwPfCUu19gZgVgVcK6idSV1phFK66oG7lfqJeu8Lt93KndkgaZy4CxsDwFzFARZIBXA/vc\n/WkAM9sHXGpmM8AZ7v5gSN8LXE4pyFwA3B9+vw+4l1KQuRC4D8DdnzKz71MKXg8Bm4BfDt8tAN9L\nWDfpYHmaMRSnLK2Yhtvo/SStmBacp7+TZCTOo5prfYDvR5Ytuh5Jfyfw7sj6tpA2AuyPpP8W8Hdh\neRa4PCy/A/j3sLwZ+ASl4Hge8H3gCuDngCeAPwe+EPKctUy5NwMHgAPDw8MpPvxa8iBPj9jPU1nK\n5dm5c2fby1EuS56OjTSGtB71b2b7zezLVT6XVQQrp9QdloZNwFvM7GHgZ4GTIf124EgIELspBaN5\nSkFnDTDr7r8BzAH/o9bG3X2Pu4+4+8gLX/jClIoseZGn8YQ8lQWWPoCyUqtnk+Xt2Eg26naXufv6\nWt+Z2XfNbLW7HzOz1cBTVbId5SddalAKBjMhfU1F+tGwz68DG8I+LgBeG9JPAW+P7H8WeAw4DvwY\n+FT46hPA1fXqJt0prfGENLpyOmVsox2zyTrl2EgyScdk7gI2AjeGfz9dJc+9wM7IzLMNwFZ3f9rM\nfmhmr6A08D8O/E8AMzvTS2MuBeDdwEdC+umUnlLw/8zsEuCUu381fHc3pWB2H7AO+GrCukmHSmM8\noZmTbrWgVO0dKbt27crdGEStVkUeHoApHS5On1qtDzAETAMHgf3AqpA+AtwaybcJOBQ+V0XSR4Av\nA98APsxPHnPzNkotlMcoBbBy+lrgUeBrYX8vimzrRZQmC/xLKNNwnDro9ctSTaOvAo4zvtDKMYhG\nx15mZ2d9cHDQzcwHBwd9cnIyk7LmaUxIkiHmmEyiloy7H6fUaqhMPwC8ObJ+O6XxlGr5frVK+geB\nD1ZJ/xbwSzXK8m3glfFLL1Jbo105caYHR/OcOHGC7du3Z/Ielma7vjw8x9DdeeSRR1J/PUKv3OAp\nS+mxMiJVNHKT3tzcHIcPH6ZYLC55P32lcuAqFAosLCywf//+ujdFNqOZAfWZmRnm5+dxd+bn5wEY\nGBhYtj6tKFczeuFxOB0lTnOnmz/qLpMkot1M/f39PjExsWxX0OzsrG/YsMELhULsrrhmytRoV1e1\n36TdtZVGd2G9MmladOvQiu4ykV63d+9eTpw4AcBzzz0HsGyrZ3R0lO3bt/PAAw9kNquqmQH1Wr9J\nszsr6UB/nO62Rp5qIK2hICPSYnmdVZWXN13WEieAaFp0/ijIiCQwPj7O7bffznPPPUd/fz/j4+Ox\nfpflCb1bB9jjBJC8BvBepiAjXasVz8UaHR1lZmYm0/00Wo92dBm16ljHCSB64GW+6PXLev1yV+qW\nq/lmbwqt/A0svbEyzaCQl2Oddp3UGlqeXr8sPS3PA8CNnMCaqUe1pwxEg8Du3bu57rrrUgsKeTjW\ncQJrkm3l5f9OJ1KQka6U1wHgeiewygDUbD2iXUa7du1aDALPPvsst912W6pBIQ/HujLQ7d27l6mp\nqSWB9fjx45kFdqlNQUa6Ul4HgGdmZjhx4gQLCwucOHFiyQmsVgBKWo+xsTGKxeLizZaPPPIIxWIR\nIJWgEC3j0NDQ4k2WrTzmlYEOWPJ0hbe+9a0sLCzEapnkIWh2EwUZaYtWDRS3K7jUqt/Q0BALCwsA\nLCwsMDQ0tPhdrQCUtB6jo6Ns2rSJyclJ3J2FhQWuueYahoeHUzv+5W3E6WbK4m9frYuw3JIxM+bn\n51lYWIjVMsnrBUqnUpCRluv2Pu/l6nf8+PHFx8oUCgWOHz+++LvlAlBS4+PjS7qPxsfHUz/mcbqZ\nsvzbVwbjaOsqOgYVp2WiGWrpUZCRluv2Pu/lusTGxsYYHBysesJbLgAllZfXO7fybx8NFBdddJFa\nJm2iICMt1+193su1SJY72S8XgNKQ9dV5nEDWrr+9WibtoyAjLZenPu8sxgfqtUhqnfDyMICeVL2T\neZ7+9tIaCjLSFnm4ssxqfCBJi6SRAfSoTrp5MI2/fSfVt9cpyEjHa/aEk9X4QNKr9UbLVQ6WJ06c\noFgs8uEPf5jNmzcnrUZudfvEkW6jICMdLckJJ8vxgejVeqNBsNFyRScaLCwscO2113LRRRd17Ym3\n2yeOdBsFGeloSU44aYwP1AsgzQTBRss1NjaGmS2uLywsZHLizUsXVZwgnJeyioKMdKDoCaTeCafe\nySbJ+EBaL9GqVsZGyxUNMv39/anP2kraRZXmSb9eEFZ3Ws7EeX1mrQ+wCtgHHAz/rqyRb2PIcxDY\nGEm/GPhX4BDwIX7yVOiXAHPhu7uBM0L6AHBHSP8SMBbZ1pUh/V+AfwBeEKcOev1yZ2nkNcFZv4p3\n586dXiwWl32Ncr0ypFHGaDnMzCcmJpquU5x9NPrK6CR1bOYV0EnKKvER8/XLhYQxagsw7e7nA9Nh\nfQkzWwXcALwceBlwg5mtDF/fAlwDnB8+l4b0W4Et7n4R8LfAn4T0awBC+iXAB8ysYGZ9wAeB33b3\nXwuB5q0J6yY5VKtlsHXr1p960OT27ds5ceLEkrxpKj8TzMwoFosMDQ2xa9cu5ubmFvOUr7p37NgR\nu6XTTDkGBgYoFoucdtppsV+c1uw+Gh2/araO5RbJtm3bWLdu3ZLjmlVZJQNxIlGtD/AosDosrwYe\nrZLnSmAysj4Z0lYDX6+WD/gBP2nVnAt8NSz/BfCmyG+mKQWufuDfgBcBBnwE2BynDmrJdJY4V8Xl\nPIVCYfHqvr+/3ycnJ1Mvy+Dg4OL2BwcHvVgs+uDgoE9MTMS6+k6rtdXMFX+r9tFsHZO2nrI+Hr2O\nmC2ZpEHm+5Fli65H0t8JvDuyvi2kjQD7I+m/BfxdWJ4FLg/L7wD+PSxvBj5BaSzpPOD7wBXhu9cD\nPwSOAfcDxTh1UJDpPPVOINGTU6FQWPyk3WVW2U1lZg4srg8MDFQNNpXlb/SE2Akn0KR1LP8my+5O\nSSa1IAPsB75c5XNZZVABnqny+2aCzC8D/wg8TKmr7XhI7wNuBr4IfBr4DHB5aMlMA78Qgt2Ho/us\nUqbNwAHgwPDwcLZ/CWm56Mmpv79/sUXTbP98nDGfgYGBxVZNOdCUg030BJn0xBn9fSMtpiy0Yiys\nEwJqr2pVSyaT7rKK318APFRj/7PAhcBvUhobKqe/EvhMnDqoJdMeWZ88ytufnJxM7aRea+C+XI/Z\n2VmfmJjwgYGBJcEmGtySDkpHf18tiGVpcnLSN2zY4JOTk8seFw2894ZWBZn3Uxqgh9Kg/01V8qwC\nvgmsDJ9vAqvCdw8Brwitj3uA14T0M8O/BWAvsCmsnw78TFi+BLg/LP+H0E32wrC+A/hAnDooyLRe\nq7tBkgS0Zk6Y5WBTHqPJoiVTK4hlZXJyckkL7fLLL695XNTN1RviBpmk98ncCHzczK4Gvg28AcDM\nRoAJd3+zuz9tZjuAz4ffvMfdnw7LbwH+ClgRgsw9If1KM7s2LH+K0rRlgDOBe81sATgKvAnA3b9j\nZv8duN/Mngtl+aOEdZOMtPqO7cp7Thq5Z6OZpwKU9/fSl76UO++8kyuuuGLJfjZu3MiTTz7J2Wef\n3VRdpqenuemmm7j77rtx9yXlyuomxDvvvHPJ+ne+852ax0UPwZQl4kSibv6oJdN67eyzb2bfaQ1a\nT05Oel9f32IrpNnJCNHZc/39/f6ud70rla7B5VS2ZMpdZhov6V20qCUj0rC0rnSr3dkNsHfvXoCq\nb39sphXVzFMBKvezd+9ebrvtNk6dOrWYJ+7rgGttu/zOmptvvpmFhQXMbPH5ZWm3EMsP3Cy3zMrr\naqVIPQoy0hZpPO692on8jjvu4MSJEwDcfvvtP3WibdVLsyr3AzA/P78kT6FQoFgscvjwYebm5pp6\nsGf0/fXl7ZlZJnXbvHlzVz/dWbKhICMdaW5ujsOHD1MsFgEWT+QnT55czPPcc8/9VJCJ89yrNMYS\nKvcDMDU1tfg4/re//e089thj3H333ezZs4epqanYz9iqfLlZ9P31u3fv5vjx44lbiM0cAz2UUqqK\n06fWzR+NyXSeWveKlO/AJ4wbDAwM5Ormv8rpzv39/YtlLRQKTc8QS3NspNljoBllvQeNyUi3inaT\nAQwPDy9eOX/2s59ddkwm7najYxqVV+jNXrFHuwh37dq1pPusUCg03b2V5ltGm5351+oZg9I5FGQk\nF9KaVpzkhFttu9HJBX19fbz85S/nn//5n3F3BgcHm37k/dDQEIODg0veZpmHk3KzY1atGuuSzlN+\nCGXPGhkZ8QMHDrS7GD2tmfd/ZNX/X7ndXbt2sW3btp8atIdS62Pz5s0MDw/HGt+prGca4ydZ0JiM\nxGFmD7v7SL18aslI2zXa1ZLlyayyJVS+Qn/22WepvCAzM+644w5OnTr1U8GxWuCM1vPZZ5/lkUce\n4ZZbboldtizqncYL08rS7LaTLhJn4KabPxr4b79GBo3bMcBcfkxM9JlhgL/yla+s+WiVao+jmZ2d\n9YGBgcXfDw4OtnVgXYP1kgQtemmZSGL1XuwVlcZLvpop3y233MI111yzmFYoFFi1ahVmRqFQ+Klx\niGovzhodHWXTpk2Lr0o+depU7PJnUe92HEvpPQoykgvV3m5ZTTvfejg+Ps6KFSsoFov09fVxzz33\nsLCwQLFYZPfu3VXvx6kMnOPj45x22mkNlz9a7+gNnEnoDZLSChr418B/7tQbe0gyNpF0XKP8+8OH\nD/PRj36U+fl5isUiO3bsYOvWramVoVqeubm5xacaRMeBgLYdD+ldcQf+2z4m0u6PxmTyJctxgrQf\nzNnIthq5YbKRd7VMTExoXEXaAo3JSCfKcpwgzW03Mo5Unmm2bds21q1bV7eba7lyVnZxAanUaW5u\njl27diXughOppCnMOdTLXRhZ3tSX9rbjTtltdIp2vZtNqz0TLUmdOuX+HelMGpPJ2ZhMrRsTuyHw\nxK1Do3VtJH87jmMjN5tGnwgQ90SftE5//Md/zOTkZKlro1CgUCjg7rFvjJXepDGZDh2TqXV/Raf3\nu2dVh3Yem0bHWSYmJhYf5lkrTzvuAYreu1MoFFr6WmfpXGhMpjNVm1baDfczZFWHdh2b5cZZao1v\nTE1N8dGPfrTmuEw76jIzM7P4yJzy/TtlfX19mtYsiWlMJmdqve+kkx4+WK37JquxllY8mLE8dRh+\n8mTnykfE7N27t+rzycrdTXHGZeLUJe3uvug+y2/WhFLAueqqq9RVJsnFae5086cV3WVpvO+jU96n\nvlyXT1Z1yPLY1HpHTa1HxFTr7ixvJ05X2HJ1ybLLcefOnT45Odnx3bLSOsTsLkt8kgZWAfuAg+Hf\nlTXybQx5DgIbI+l/BjwB/Kgi/yDwMeAQ8DlgbeS7rSH9UeDVkfRLQ9ohYEuc8mcdZLphPKURtU6y\nnWrnzp1uZovBxMwW6zQxMbH4XZzxs6TBsBXHtlMuZqT9Whlkbiqf0IEtwPuq5FkFPB7+XRmWV4bv\nXgGsrhJk3gJ8JCy/EfhYWL4Q+FIIQucB3wCK4fMN4OeBgZDnwnrlzzrIdNtJt56sHuTYrhPfcm/b\nrFXXOIP8zZally5YJN9aGWQeBVaH5dXAo1XyXAlMRtYngSsr8lQGmXuB0bDcB3wPsNCK2VqZL3zu\njaQvyVfr0w0tmbxdfebhdcBpWi5oVKtr1k8tyNPfWnpX3CCTxsD/We5+LCw/CZxVJc85lLrEyo6E\ntOUs/sbdT5nZD4ChkP5gjW1V7uPlcSqQpVoD+Wlp5oVfy20rjXLm4XXAlZLUbbn6VPsurTI3WhaR\nPIoVZMxsP3B2la+uj664e7mPOtfMbDOwGUrvh89alieGNE/CaQWrNCWZPRa9sfG6665bfIXyVVdd\ntThLLG9lFuk2sYKMu6+v9Z2ZfdfMVrv7MTNbDTxVJdtRYCyyvgaYqbPbo8C5wBEz6wOeDxyPpEe3\ndTQs10pfwt33AHugdMd/nXLkWlontDSvvtOcZttsSzAaNAuFAvPz8ywsLDA/P8/k5CRTU1OpBNJa\nb5bMsvUq0lHi9Kkt9wHez9KB/5uq5FkFfJPSoP/KsLyqIk/lmMy1LB34/3hYfjFLB/4fpzTo3xeW\nz+MnA/8vrlf+vN3x34y0pkinMY6QhzEU96UTLgqFgvf19S2ZJVYoFHzDhg0Nly96rPNSV5F2oIUD\n/0PANKWpyfvLwQMYAW6N5NtEaWrxIeCqSPpNlMZPFsK/20P6acAnQv6HgJ+P/OZ6SjPJHgV+N5L+\nGuCx8N31ccrfDUEmLWkEq7zMpqsMAJOTkz4xMeEDAwOLj04pFAoNBYfKbUZfyZznmYOaLCBZaFmQ\n6fSPgkx9ab0LpdVqzfzasGHDYqBpJDh04rtc8vT3kO4SN8josTKyrEYnBORpPKLahIvR0VG2b9/O\nAw880PA4VuX41/j4OOPj47moay1ZznQTiUNBRpbVzEmq3dNs6008WC4QLvfbWr/L80lbM92k3RRk\nZFmddpKK2/KqFgjjvssnz0GlUitalt3wriPJjoKMLCtP3V9xJOkeqvWo/TzeP9SILANjXu+vkvzQ\n+2SkrtHRUbZu3bp48mjH++Dj7rPa+3ji6tZ3+WRJx0fqUUtGGtKO10M3crWcpOU1OjrK7t27ufPO\nO7niiis68l0+rdZp3anSegoy0pB2dCk12gXWaPdQtcfPPPDAA1x00UUd113Yajo+Uo+CjDSk2pVr\nrcCT1okny6vlWo+fiQazThvsbzUdH1mOgow0JM7roYeGhlJt2WR5tRwNkO5OoVDAzNT1I5ISBZkO\nkLcpopVXrpVBoFb3VlaP20+ispW0e/dujh8/nptjLdLpFGRyLu9TRKOBY+vWrYvpld1bea1HnFZS\n3oJ8Ut1WH8k3BZmcy3q8I4lagaPaiXvXrl25fbzJcq2kOMGxk07aeQ320r0UZHKusjsn7fGOJJab\n9VV54u7Uqa71ZrZ12kk7yc2qIs3QzZgt0uwNjOVWwY4dO5ienub48eO5ufmtkRsfK+vR7Ns7k94E\n2ug26tVqOpAvAAAKxUlEQVSx025GTHKzqkhT4jyquZs/rXjUf5qPW8/bo9vjvgYg6TtN0qh3s9tY\nruzNbLPd73dp9/6lO6D3yeQnyKT9Iq9OO0mkESDSOIbRbZiZT0xMNLyNajr1fTsiScQNMhqTaYG0\nxyPyfPNbtUHwNMYB0jiGY2Nj9PX1Ld4Tc/vttzM+Pp74WDby99CYiPQaBZkW6JVHb9QaBE8jQKRx\nDEdHR7nqqquYnJzE3Zmfn2/5Sb5TJ0CINEtBpkXy3PpIS62r9LSCbBrHcHx8nKmpqbad5HvlgkOk\nzEpda71rZGTEDxw40O5idIVOmc7bSfe1iOSVmT3s7iN18ynIKMhEJT0B6wQu0hviBplE3WVmtgr4\nGLAW+BbwBnd/pkq+jcC7w+p73X0qpP8ZMA6sdPfnRfIPAnuBi4HjwO+7+7fCd1uBq4F54L+6+71m\ndm7IfxbgwB53/2CSuvWiNFoivdAtKCLxJb0Zcwsw7e7nA9NhfYkQiG4AXg68DLjBzFaGr+8OaZWu\nBp5x918EbgbeF7Z1IfBG4MXApcBfmlkROAX8N3e/EHgFcG3IKw3otBsLRST/kgaZy4CpsDwFXF4l\nz6uBfe7+dGjl7KMUIHD3B939WJ3tfhJYZ2YW0v/G3U+4+zeBQ8DL3P2Yu38hbPPfga8B5ySsW8/R\n3eAikraks8vOigSJJyl1V1U6B3gisn6E+gFg8TfufsrMfgAMhfQHl9uWma0FXgp8rtbGzWwzsBlg\neHi4TlF6h2Y+iUja6gYZM9sPnF3lq+ujK+7uZtbWWQRm9jzgTuA6d/9hrXzuvgfYA6WB/xYVryNo\nTEVE0lQ3yLj7+lrfmdl3zWy1ux8zs9XAU1WyHQXGIutrgJk6uz0KnAscMbM+4PmUJgCU06PbOhrK\n0k8pwPwfd/9Une2LiEgLJB2TuQvYGJY3Ap+ukudeYIOZrQwD/htCWtztvh64Lzwr5y7gjWY2aGbn\nAecDD4XxmtuAr7n7nyeqkYiIpCZpkLkRuMTMDgLrwzpmNmJmtwK4+9PADuDz4fOekIaZ3WRmR4DT\nzeyImW0P270NGDKzQ8A7CLPW3P0rwMeBrwL/AFzr7vPAfwTeBPyOmX0xfF6TsG4iIpKQbsbUzZgi\nIg2LezOmXlomXSGNF5qJSPr0gEzpeJ3yzDSRXqSWTA/p1qt9PalAJL/UkukR3Xy1r3e0iOSXgkyP\n6OY3MupJBSL5pSDTI7r9al9PKhDJJwWZJnXae1N0tS8i7aAg04ROHd/Q1b6ItJpmlzVBs5lEROJR\nkGmC3rsiIhKPusuaoPENEZF4FGSapPENEZH61F0mIiKZUZAREZHMKMiIiEhmFGRERCQzCjIiIpIZ\nBRkREcmMgoyIiGRGQUZERDKTOMiY2Soz22dmB8O/K2vk2xjyHDSzjZH0PzOzJ8zsRxX5B83sY2Z2\nyMw+Z2ZrI99tDemPmtmrK35XNLNHzOzvktZNRESSSaMlswWYdvfzgemwvoSZrQJuAF4OvAy4IRKM\n7g5pla4GnnH3XwRuBt4XtnUh8EbgxcClwF+aWTHyu7cBX0uhXiIiklAaQeYyYCosTwGXV8nzamCf\nuz/t7s8A+ygFCNz9QXc/Vme7nwTWmZmF9L9x9xPu/k3gECFImdka4LXArSnUS0REEkojyJwVCRJP\nAmdVyXMO8ERk/UhIW87ib9z9FPADYKjOtnYD7wIWGii/iIhkJNYDMs1sP3B2la+uj664u5uZp1Gw\nRpnZfwKecveHzWysTt7NwGaA4eHhFpRORKQ3xQoy7r6+1ndm9l0zW+3ux8xsNfBUlWxHgbHI+hpg\nps5ujwLnAkfMrA94PnA8kh7d1lHgdcDrzOw1wGnAGWb2v939D6vUZw+wB2BkZKQtQVFEpBek0V12\nF1CeLbYR+HSVPPcCG8xsZRjw3xDS4m739cB97u4h/Y1h9tl5wPnAQ+6+1d3XuPtaShMD7qsWYERE\npHXSCDI3ApeY2UFgfVjHzEbM7FYAd38a2AF8PnzeE9Iws5vM7AhwupkdMbPtYbu3AUNmdgh4B2HW\nmrt/Bfg48FXgH4Br3X0+hXqIiEjKrNQ46F0jIyN+4MCBdhdDRKSjmNnD7j5SL5/u+BcRkcwoyIiI\nSGYUZEREJDMKMiIikhkFGRERyYyCjIiIZEZBRkREMqMgIyIimVGQERGRzCjISO7Mzc2xa9cu5ubm\n2l0UEUko1lOYRVplbm6OdevWcfLkSQYGBpienmZ0dLTdxRKRJqklI7kyMzPDyZMnmZ+f5+TJk8zM\nzLS7SCKSgIKM5MrY2BgDAwMUi0UGBgYYGxtrd5FEJAF1l0mujI6OMj09zczMDGNjY+oqE+lwCjKS\nO6OjowouIl1C3WUiIpIZBRkREcmMgoyIiGRGQUZERDKjICMiIplRkBERkcyYu7e7DG1lZv8GfDul\nzb0A+F5K2+omOi616dhUp+NSXZ6Oy4vc/YX1MvV8kEmTmR1w95F2lyNvdFxq07GpTseluk48Luou\nExGRzCjIiIhIZhRk0rWn3QXIKR2X2nRsqtNxqa7jjovGZEREJDNqyYiISGYUZBIws1Vmts/MDoZ/\nVy6T9wwzO2JmH25lGdshznExs183szkz+4qZ/YuZ/X47ytoKZnapmT1qZofMbEuV7wfN7GPh+8+Z\n2drWl7I9Yhybd5jZV8P/kWkze1E7ytlq9Y5LJN8VZuZmltsZZwoyyWwBpt39fGA6rNeyA7i/JaVq\nvzjH5cfAuLu/GLgU2G1mP9fCMraEmRWBvwB+F7gQuNLMLqzIdjXwjLv/InAz8L7WlrI9Yh6bR4AR\nd/814JPATa0tZevFPC6Y2c8CbwM+19oSNkZBJpnLgKmwPAVcXi2TmV0MnAX8Y4vK1W51j4u7P+bu\nB8Pyd4CngLo3dnWglwGH3P1xdz8J/A2l4xMVPV6fBNaZmbWwjO1S99i4+2fd/cdh9UFgTYvL2A5x\n/s9A6cL1fcCzrSxcoxRkkjnL3Y+F5ScpBZIlzKwAfAB4ZysL1mZ1j0uUmb0MGAC+kXXB2uAc4InI\n+pGQVjWPu58CfgAMtaR07RXn2ERdDdyTaYnyoe5xMbPfAM51979vZcGaoTdj1mFm+4Gzq3x1fXTF\n3d3Mqk3VewvwGXc/0k0Xpykcl/J2VgP/C9jo7gvpllK6hZn9ITACvKrdZWm3cOH658AftbkosSjI\n1OHu62t9Z2bfNbPV7n4snCyfqpJtFPgtM3sL8DxgwMx+5O7Ljd/kXgrHBTM7A/h74Hp3fzCjorbb\nUeDcyPqakFYtzxEz6wOeDxxvTfHaKs6xwczWU7p4eZW7n2hR2dqp3nH5WeBXgZlw4Xo2cJeZvc7d\nD7SslDGpuyyZu4CNYXkj8OnKDO7+B+4+7O5rKXWZ7e30ABND3eNiZgPA31I6Hp9sYdla7fPA+WZ2\nXqjzGykdn6jo8Xo9cJ/3xg1sdY+Nmb0UmARe5+5VL1a60LLHxd1/4O4vcPe14bzyIKXjk7sAAwoy\nSd0IXGJmB4H1YR0zGzGzW9tasvaKc1zeALwS+CMz+2L4/Hp7ipudMMbyVuBe4GvAx939K2b2HjN7\nXch2GzBkZoeAd7D8LMWuEfPYvJ9SD8Anwv+RygDddWIel46hO/5FRCQzasmIiEhmFGRERCQzCjIi\nIpIZBRkREcmMgoyIiGRGQUZERDKjICMiIplRkBERkcz8f8w4bSAAHVbuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f984c5675c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glmm_par.set_free(opt_x)\n",
    "if simulate_data:\n",
    "    print('Fit beta: ', glmm_par['beta'].e())\n",
    "    print('True beta: ', true_beta)\n",
    "    \n",
    "    print('Fit mu: ', glmm_par['mu'].e())\n",
    "    print('True mu: ', true_mu)\n",
    "    \n",
    "    print('Fit tau: ', glmm_par['tau'].e())\n",
    "    print('True tau: ', true_tau)\n",
    "    \n",
    "    plt.plot(true_u, true_u, 'r.')\n",
    "    plt.plot(true_u, glmm_par['u'].e(), 'k.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(opt_x)\n",
    "#glmm_par_opt.set_free(init_par_vec)\n",
    "print(glmm_fit['beta_mean'])\n",
    "print(glmm_par_opt['beta'].e())\n",
    "print(glmm_par_opt)\n",
    "\n",
    "#plt.plot(glmm_par_opt['u'].e(), glmm_par_opt['u'].var(), 'k.')\n",
    "\n",
    "e_beta = glmm_par_opt['beta'].e()\n",
    "e_u = glmm_par_opt['u'].e()[model.y_g_vec]\n",
    "\n",
    "z_mean = e_u + np.matmul(model.x_mat, e_beta)\n",
    "#plt.plot(sp.special.expit(z_mean), model.y_vec, 'k.')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(z_mean[model.y_vec == 1], 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.title('y == 1')\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(z_mean[model.y_vec == 0], 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.title('y == 0')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(opt_x))\n",
    "print(glmm_par.free_size())\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(opt_x)\n",
    "moment_wrapper.glmm_par.set_free(opt_x)\n",
    "moment_wrapper.set_moments()\n",
    "\n",
    "print(vb_time / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Investigate the performance of different numbers of draws.   It doesn't appear to\n",
    "    # converge.\n",
    "    opt_x_20 = tr_optimize(init_par_vec, 20)\n",
    "    opt_x_60 = tr_optimize(opt_x_20, 60)\n",
    "    opt_x_100 = tr_optimize(opt_x_60, 100)\n",
    "    opt_x_200 = tr_optimize(opt_x_100, 200)\n",
    "    opt_x_400 = tr_optimize(opt_x_200, 400)\n",
    "    opt_x_800 = tr_optimize(opt_x_400, 800)\n",
    "    \n",
    "    mom_20 = get_moment_vec(opt_x_20)\n",
    "    mom_60 = get_moment_vec(opt_x_60)\n",
    "    mom_100 = get_moment_vec(opt_x_100)\n",
    "    mom_200 = get_moment_vec(opt_x_200)\n",
    "    mom_400 = get_moment_vec(opt_x_400)\n",
    "    mom_800 = get_moment_vec(opt_x_800)\n",
    "\n",
    "    print np.max(np.abs((mom_20 - mom_60) / mom_20))\n",
    "    print np.max(np.abs((mom_60 - mom_100) / mom_60))\n",
    "    print np.max(np.abs((mom_100 - mom_200) / mom_100))\n",
    "    print np.max(np.abs((mom_200 - mom_400) / mom_200))\n",
    "    print np.max(np.abs((mom_400 - mom_800) / mom_400))\n",
    "\n",
    "    print '-------\\n'\n",
    "    print np.max(np.abs((mom_20 - mom_60)))\n",
    "    print np.max(np.abs((mom_60 - mom_100)))\n",
    "    print np.max(np.abs((mom_100 - mom_200)))\n",
    "    print np.max(np.abs((mom_200 - mom_400)))\n",
    "    print np.max(np.abs((mom_400 - mom_800)))\n",
    "\n",
    "    #diff_inds = np.where(np.abs(mom_60 - mom_100) > 1e-2)\n",
    "    #print diff_inds\n",
    "    #print moment_indices\n",
    "\n",
    "    #print (get_moment_vec(opt_x_60) - get_moment_vec(opt_x_100)) / np.abs(get_moment_vec(opt_x_100))\n",
    "    get_moment_vec(opt_x_200)\n",
    "    u200 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_400)\n",
    "    u400 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_800)\n",
    "    u800 = copy.deepcopy(moment_par['e_u'].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine why the means are different for different number of simulations.\n",
    "def get_logit_terms(num_draws):\n",
    "    model.set_draws(num_draws)\n",
    "    std_draws = model.std_draws\n",
    "\n",
    "    e_beta = glmm_par_opt['beta'].mean.get()\n",
    "    info_beta = glmm_par_opt['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "\n",
    "    e_u = glmm_par_opt['u'].mean.get()[y_g_vec]\n",
    "    info_u = glmm_par_opt['u'].info.get()[y_g_vec]\n",
    "    var_u = 1 / info_u\n",
    "\n",
    "    z_mean = e_u + np.matmul(x_mat, e_beta)\n",
    "    z_sd = np.sqrt(var_u + np.einsum('nk,kj,nj->n', x_mat, cov_beta, x_mat))\n",
    "    z = np.einsum('i,j->ij', z_sd, std_draws) + np.expand_dims(z_mean, 1)\n",
    "\n",
    "    # The sum is over observations and draws, so dividing by the draws size\n",
    "    # gives the sum of sample expectations over the draws.\n",
    "    # p = exp(z) / (1 + exp(z))\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_terms = np.log1p(np.exp(z))\n",
    "    logit_term = -np.sum(logit_terms) / std_draws.size\n",
    "\n",
    "    return logit_term, logit_terms, z\n",
    "    \n",
    "logit_term_50, logit_terms_50, z_50 = get_logit_terms(50)    \n",
    "logit_term_800, logit_terms_800, z_800 = get_logit_terms(800)\n",
    "\n",
    "print( logit_term_50)\n",
    "print( logit_term_800)\n",
    "\n",
    "logit_terms_50_mean = np.mean(logit_terms_50, 1)\n",
    "logit_terms_800_mean = np.mean(logit_terms_800, 1)\n",
    "\n",
    "print( np.max(np.abs(logit_terms_50_mean - logit_terms_800_mean)))\n",
    "print( np.where(np.abs(logit_terms_50_mean - logit_terms_800_mean) > 1e-3))\n",
    "\n",
    "ind = 3\n",
    "plt.plot(z_800[ind, :], logit_terms_800[ind, :])\n",
    "plt.plot(z_50[ind, :], logit_terms_50[ind, :])\n",
    "print( logit_terms_50_mean[ind])\n",
    "print( logit_terms_800_mean[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hessians at the number of draws used for optimization.\n",
    "\n",
    "model.set_draws(num_mc_draws)\n",
    "\n",
    "hess_time = time.time()\n",
    "print('KL Hessian:\\n')\n",
    "kl_hess = objective.fun_free_hessian(opt_x)\n",
    "\n",
    "print('Log prior Hessian:\\n')\n",
    "log_prior_hess = PriorHess(opt_x, prior_par.get_vector())\n",
    "\n",
    "hess_time =  time.time() - hess_time\n",
    "elbo_hess = -kl_hess\n",
    "\n",
    "print('hess_time: %f' % hess_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moment_jac = MomentJacobian(opt_x)\n",
    "lrvb_cov = np.matmul(moment_jac, np.linalg.solve(kl_hess, moment_jac.T))\n",
    "\n",
    "prior_indices = copy.deepcopy(prior_par)\n",
    "prior_indices.set_vector(1 + np.array(range(prior_indices.vector_size())))\n",
    "\n",
    "vp_indices = copy.deepcopy(glmm_par_opt)\n",
    "vp_indices.set_vector(1 + np.array(range(vp_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not simulate_data:\n",
    "    # Write the result to a JSON file for use in R.\n",
    "    \n",
    "    run_name = 'production'\n",
    "    result_dict = { 'glmm_par_opt': glmm_par_opt.dictval(), 'run_name': run_name,\n",
    "                    'vb_time': vb_time, 'hess_time': hess_time, 'num_mc_draws': num_mc_draws, \n",
    "                    'moment_indices': moment_indices.dictval(),\n",
    "                    'prior_indices': prior_indices.dictval(),\n",
    "                    'vp_indices': vp_indices.dictval(),\n",
    "                    'lrvb_cov': lrvb_cov.tolist(), 'moment_jac': moment_jac.tolist(),\n",
    "                    'elbo_hess': elbo_hess.tolist(), 'log_prior_hess': log_prior_hess.tolist() }\n",
    "\n",
    "    result_json = json.dumps(result_dict)\n",
    "    json_file = open(json_output_filename, 'w')\n",
    "    json_file.write(result_json)\n",
    "    json_file.close()\n",
    "\n",
    "    print(json_output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
