{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from VariationalBayes import ScalarParam, ModelParamsDict, VectorParam, PosDefMatrixParam\n",
    "from VariationalBayes.NormalParams import MVNParam, UVNParam, UVNParamVector\n",
    "from VariationalBayes.GammaParams import GammaParam\n",
    "from VariationalBayes.ExponentialFamilies import \\\n",
    "    UnivariateNormalEntropy, MultivariateNormalEntropy, GammaEntropy, \\\n",
    "    MVNPrior, UVNPrior, GammaPrior\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from autograd import grad, hessian, jacobian, hessian_vector_product\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy as asp\n",
    "import scipy as sp\n",
    "\n",
    "import copy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'y_group', u'mu_prior_var', u'mu_prior_t', u'mu_prior_var_c', u'K', u'beta_prior_var', u'tau_prior_beta', u'N', u'mu_prior_mean_c', u'mu_prior_epsilon', u'mu_prior_mean', u'y', u'x', u'NG', u'beta_prior_mean', u'tau_prior_alpha']\n",
      "10000\n",
      "0.4668\n"
     ]
    }
   ],
   "source": [
    "# Load data saved by stan_results_to_json.R and run_stan.R in LRVBLogitGLMM.\n",
    "import os\n",
    "import json\n",
    "\n",
    "simulate_data = False\n",
    "prior_par = ModelParamsDict('Prior Parameters')\n",
    "\n",
    "if not simulate_data:\n",
    "    #analysis_name = 'simulated_data_small'\n",
    "    analysis_name = 'simulated_data_large'\n",
    "\n",
    "    data_dir = os.path.join(os.environ['GIT_REPO_LOC'], 'LRVBLogitGLMM/LogitGLMMLRVB/inst/data/')\n",
    "    json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "    json_output_filename = os.path.join(data_dir, '%s_python_vb_results.json' % analysis_name)\n",
    "\n",
    "    json_file = open(json_filename, 'r')\n",
    "    json_dat = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    stan_dat = json_dat['stan_dat']\n",
    "    vp_base = json_dat['vp_base']\n",
    "\n",
    "    print stan_dat.keys()\n",
    "    K = stan_dat['K'][0]\n",
    "    NObs = stan_dat['N'][0]\n",
    "    NG = stan_dat['NG'][0]\n",
    "    N = NObs / NG\n",
    "    y_g_vec = np.array(stan_dat['y_group'])\n",
    "    y_vec = np.array(stan_dat['y'])\n",
    "    x_mat = np.array(stan_dat['x'])\n",
    "    \n",
    "    # Define a class to contain prior parameters.\n",
    "    prior_par.push_param(VectorParam('beta_prior_mean', K, val=np.array(stan_dat['beta_prior_mean'])))\n",
    "    beta_prior_info = np.linalg.inv(np.array(stan_dat['beta_prior_var']))\n",
    "    prior_par.push_param(PosDefMatrixParam('beta_prior_info', K, val=beta_prior_info))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('mu_prior_mean', val=stan_dat['mu_prior_mean'][0]))\n",
    "    prior_par.push_param(ScalarParam('mu_prior_info', val=1 / stan_dat['mu_prior_var'][0]))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('tau_prior_alpha', val=stan_dat['tau_prior_alpha'][0]))\n",
    "    prior_par.push_param(ScalarParam('tau_prior_beta', val=stan_dat['tau_prior_beta'][0]))\n",
    "\n",
    "    # An index set to make sure jacobians match the order expected by R.\n",
    "    prior_par_indices = copy.deepcopy(prior_par)\n",
    "    prior_par_indices.set_name('Prior Indices')\n",
    "    prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "else:\n",
    "    # Simulate data instead of loading it if you like\n",
    "    N = 200     # observations per group\n",
    "    K = 5      # dimension of regressors\n",
    "    NG = 200      # number of groups\n",
    "\n",
    "    # Generate data\n",
    "    def Logistic(u):\n",
    "        return np.exp(u) / (1 + np.exp(u))\n",
    "\n",
    "    NObs = NG * N\n",
    "    true_beta = np.array(range(5))\n",
    "    true_beta = true_beta - np.mean(true_beta)\n",
    "    true_mu = 0.\n",
    "    true_tau = 40.0\n",
    "    true_u = np.random.normal(true_mu, 1 / np.sqrt(true_tau), NG)\n",
    "\n",
    "    x_mat = np.random.random(K * NObs).reshape(NObs, K) - 0.5\n",
    "    y_g_vec = [ g for g in range(NG) for n in range(N) ]\n",
    "    true_rho = Logistic(np.matmul(x_mat, true_beta) + true_u[y_g_vec])\n",
    "    y_vec = np.random.random(NObs) < true_rho\n",
    "    \n",
    "    prior_par.push_param(VectorParam('beta_prior_mean', K, val=np.zeros(K)))\n",
    "    prior_par.push_param(PosDefMatrixParam('beta_prior_info', K, val=0.01 * np.eye(K)))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('mu_prior_mean', val=0))\n",
    "    prior_par.push_param(ScalarParam('mu_prior_info', val=0.5))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('tau_prior_alpha', val=3.0))\n",
    "    prior_par.push_param(ScalarParam('tau_prior_beta', val=10.0))\n",
    "\n",
    "print N * NG\n",
    "print np.mean(y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build an object to contain a variational approximation to a K-dimensional multivariate normal.\n",
    "glmm_par = ModelParamsDict('GLMM Parameters')\n",
    "\n",
    "glmm_par.push_param(UVNParam('mu', min_info=vp_base['mu_info_min'][0]))\n",
    "glmm_par.push_param(GammaParam('tau',\n",
    "                               min_shape=vp_base['tau_alpha_min'][0],\n",
    "                               min_rate=vp_base['tau_beta_min'][0]))\n",
    "glmm_par.push_param(MVNParam('beta', K, min_info=vp_base['beta_diag_min'][0]))\n",
    "glmm_par.push_param(UVNParamVector('u', NG, min_info=vp_base['u_info_min'][0]))\n",
    "\n",
    "# Initialize with ADVI.  Don't forget to add the ADVI computation time to your final VB time!\n",
    "advi_init = False\n",
    "if advi_init:\n",
    "    advi_fit = json_dat['advi_results']\n",
    "    glmm_par['mu'].mean.set(advi_fit['mu_mean'][0])\n",
    "    glmm_par['mu'].info.set(1 / advi_fit['mu_var'][0])\n",
    "\n",
    "    tau_mean = advi_fit['tau_mean'][0]\n",
    "    tau_var = advi_fit['tau_var'][0]\n",
    "    glmm_par['tau'].shape.set((tau_mean ** 2) / tau_var)\n",
    "    glmm_par['tau'].rate.set(tau_var / tau_mean)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.array(advi_fit['beta_mean']))\n",
    "    glmm_par['beta'].info.set(np.array(advi_fit['beta_info']))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.array(advi_fit['u_mean']))\n",
    "    glmm_par['u'].info.set(1 / np.array(advi_fit['u_var']))\n",
    "\n",
    "    free_par_vec = glmm_par.get_free()\n",
    "else:\n",
    "    glmm_par['mu'].mean.set(0.0)\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    glmm_par['tau'].shape.set(2.0)\n",
    "    glmm_par['tau'].rate.set(2.0)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.full(K, 0.0))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.full(NG, 0.0))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "free_par_vec = glmm_par.get_free()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define moment parameters\n",
    "\n",
    "moment_par = ModelParamsDict('Moment Parameters')\n",
    "moment_par.push_param(VectorParam('e_beta', K))\n",
    "moment_par.push_param(PosDefMatrixParam('e_beta_outer', K))\n",
    "moment_par.push_param(ScalarParam('e_mu'))\n",
    "moment_par.push_param(ScalarParam('e_mu2'))\n",
    "moment_par.push_param(ScalarParam('e_tau'))\n",
    "moment_par.push_param(ScalarParam('e_log_tau'))\n",
    "moment_par.push_param(VectorParam('e_u', NG))\n",
    "moment_par.push_param(VectorParam('e_u2', NG))\n",
    "\n",
    "def set_moments(glmm_par, moment_par):\n",
    "    moment_par['e_beta'].set(glmm_par['beta'].e())\n",
    "    moment_par['e_beta_outer'].set(glmm_par['beta'].e_outer())\n",
    "    moment_par['e_mu'].set(glmm_par['mu'].e())\n",
    "    moment_par['e_mu2'].set(glmm_par['mu'].e_outer())\n",
    "    moment_par['e_tau'].set(glmm_par['tau'].e())\n",
    "    moment_par['e_log_tau'].set(glmm_par['tau'].e_log())\n",
    "    moment_par['e_u'].set(glmm_par['u'].e())\n",
    "    moment_par['e_u2'].set((glmm_par['u'].e_outer()))\n",
    "    \n",
    "set_moments(glmm_par, moment_par)\n",
    "\n",
    "# Moment indices.\n",
    "moment_indices = copy.deepcopy(moment_par)\n",
    "moment_indices.set_vector(1 + np.array(range(moment_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14516.0995561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.6657256909229563"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ELogPrior(prior_par, glmm_par_elbo):\n",
    "    e_beta = glmm_par_elbo['beta'].mean.get()\n",
    "    info_beta = glmm_par_elbo['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "    beta_prior_info = prior_par['beta_prior_info'].get()\n",
    "    beta_prior_mean = prior_par['beta_prior_mean'].get()\n",
    "    e_log_p_beta = MVNPrior(beta_prior_mean, beta_prior_info, e_beta, cov_beta)\n",
    "    \n",
    "    e_mu = glmm_par_elbo['mu'].mean.get()\n",
    "    info_mu = glmm_par_elbo['mu'].info.get()\n",
    "    var_mu = 1 / info_mu\n",
    "    e_log_p_mu = UVNPrior(prior_par['mu_prior_mean'].get(), prior_par['mu_prior_info'].get(), e_mu, var_mu) \n",
    "\n",
    "    e_tau = glmm_par_elbo['tau'].e()\n",
    "    e_log_tau = glmm_par_elbo['tau'].e_log()\n",
    "    tau_prior_shape = prior_par['tau_prior_alpha'].get()\n",
    "    tau_prior_rate = prior_par['tau_prior_beta'].get()\n",
    "    e_log_p_tau = GammaPrior(tau_prior_shape, tau_prior_rate, e_tau, e_log_tau)\n",
    "    \n",
    "    return  e_log_p_beta + e_log_p_mu + e_log_p_tau\n",
    "           \n",
    "\n",
    "def DataLogLikelihood(x_mat, y_vec, e_beta, cov_beta, e_u, var_u, std_draws):\n",
    "    z_mean = e_u + np.matmul(x_mat, e_beta)\n",
    "    z_sd = np.sqrt(var_u + np.einsum('nk,kj,nj->n', x_mat, cov_beta, x_mat))\n",
    "    z = np.einsum('i,j->ij', z_sd, std_draws) + np.expand_dims(z_mean, 1)\n",
    "\n",
    "    # The sum is over observations and draws, so dividing by the draws size\n",
    "    # gives the sum of sample expectations over the draws.\n",
    "    # p = exp(z) / (1 + exp(z))\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_term = -np.sum(np.log1p(np.exp(z))) / std_draws.size\n",
    "    y_term = np.sum(y_vec * z_mean)\n",
    "    return y_term + logit_term\n",
    "\n",
    "\n",
    "def RandomEffectLogLikelihood(e_u, var_u, e_mu, var_mu, e_tau, e_log_tau):\n",
    "    return -0.5 * e_tau * np.sum(((e_mu - e_u) ** 2) + var_mu + var_u) + 0.5 * e_log_tau * len(e_u)\n",
    "\n",
    "    \n",
    "def Elbo(y_vec, x_mat, y_g_vec, glmm_par_elbo, std_draws, prior_par):\n",
    "    e_beta = glmm_par_elbo['beta'].mean.get()\n",
    "    info_beta = glmm_par_elbo['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "    \n",
    "    e_u = glmm_par_elbo['u'].mean.get()\n",
    "    info_u = glmm_par_elbo['u'].info.get()\n",
    "    var_u = 1 / info_u\n",
    "    \n",
    "    e_mu = glmm_par_elbo['mu'].mean.get()\n",
    "    info_mu = glmm_par_elbo['mu'].info.get()\n",
    "    var_mu = 1 / info_mu\n",
    "    \n",
    "    e_tau = glmm_par_elbo['tau'].e()\n",
    "    e_log_tau = glmm_par_elbo['tau'].e_log()\n",
    "        \n",
    "    ll = \\\n",
    "        DataLogLikelihood(x_mat, y_vec, e_beta, cov_beta,\n",
    "                          e_u[y_g_vec], var_u[y_g_vec], std_draws) + \\\n",
    "        RandomEffectLogLikelihood(e_u, var_u, e_mu, var_mu, e_tau, e_log_tau)\n",
    "    if np.isnan(ll):\n",
    "        return -np.inf\n",
    "\n",
    "    e_log_prior = ELogPrior(prior_par, glmm_par_elbo)\n",
    "    if np.isnan(e_log_prior):\n",
    "        return -np.inf\n",
    "    \n",
    "    tau_shape = glmm_par_elbo['tau'].shape.get()\n",
    "    tau_rate = glmm_par_elbo['tau'].rate.get()\n",
    "    entropy = \\\n",
    "        UnivariateNormalEntropy(info_mu) + \\\n",
    "        MultivariateNormalEntropy(info_beta) + \\\n",
    "        UnivariateNormalEntropy(info_u) + \\\n",
    "        GammaEntropy(tau_shape, tau_rate)\n",
    "\n",
    "    return ll[0] + e_log_prior[0] + entropy\n",
    "\n",
    "\n",
    "class KLWrapper(object):\n",
    "    def __init__(self, glmm_par, prior_par, x_mat, y_vec, y_g_vec, num_draws):\n",
    "        self.__glmm_par_ad = copy.deepcopy(glmm_par)\n",
    "        self.__prior_par_ad = copy.deepcopy(prior_par)\n",
    "        self.x_mat = x_mat\n",
    "        self.y_vec = y_vec\n",
    "        self.y_g_vec = y_g_vec\n",
    "        self.set_draws(num_draws)\n",
    "        \n",
    "    def set_draws(self, num_draws):\n",
    "        draw_spacing = 1 / float(num_draws + 1)\n",
    "        target_quantiles = np.linspace(draw_spacing, 1 - draw_spacing, num_draws)\n",
    "        self.std_draws = sp.stats.norm.ppf(target_quantiles)\n",
    "\n",
    "    def Eval(self, free_par_vec, verbose=False):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        kl = -Elbo(self.y_vec, self.x_mat, self.y_g_vec,\n",
    "                   self.__glmm_par_ad, self.std_draws, self.__prior_par_ad)\n",
    "        if verbose: print kl\n",
    "            \n",
    "        # TODO: this is returning an array when it should be a scalar.\n",
    "        return kl\n",
    "    \n",
    "    def ExpectedLogPrior(self, free_par_vec, prior_par_vec):\n",
    "        # Encode the glmm parameters first and the prior second.\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        self.__prior_par_ad.set_vector(prior_par_vec)\n",
    "        e_log_prior = ELogPrior(self.__prior_par_ad, self.__glmm_par_ad)\n",
    "        return e_log_prior[0]\n",
    "        \n",
    "        \n",
    "class MomentWrapper(object):\n",
    "    def __init__(self, glmm_par, moment_par):\n",
    "        self.__glmm_par_ad = copy.deepcopy(glmm_par)\n",
    "        self.__moment_par = copy.deepcopy(moment_par)\n",
    "\n",
    "    # Return a posterior moment of interest as a function of unconstrained parameters.\n",
    "    def GetMoments(self, free_par_vec):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        set_moments(self.__glmm_par_ad, self.__moment_par)\n",
    "        return self.__moment_par.get_vector()\n",
    "    \n",
    "    def GetMomentParameters(self, free_par_vec):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        set_moments(self.__glmm_par_ad, self.__moment_par)\n",
    "        return self.__moment_par\n",
    "\n",
    "\n",
    "kl_wrapper = KLWrapper(glmm_par, prior_par, x_mat, y_vec, y_g_vec, 10)\n",
    "KLGrad = grad(kl_wrapper.Eval)\n",
    "KLHess = hessian(kl_wrapper.Eval)\n",
    "KLHessVecProd = hessian_vector_product(kl_wrapper.Eval)  \n",
    "print kl_wrapper.Eval(free_par_vec)\n",
    "\n",
    "moment_wrapper = MomentWrapper(glmm_par, moment_par)\n",
    "MomentJacobian = jacobian(moment_wrapper.GetMoments)\n",
    "\n",
    "# PriorHess evaluates the second order derivative d2 EPrior / dpar dprior_par\n",
    "PriorModelGrad = grad(kl_wrapper.ExpectedLogPrior, argnum=0)\n",
    "PriorHess = jacobian(PriorModelGrad, argnum=1)\n",
    "\n",
    "kl_wrapper.ExpectedLogPrior(free_par_vec, prior_par.get_vector())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function time:\n",
      "0.0236967086792\n",
      "Grad time:\n",
      "0.0572127103806\n",
      "Hessian vector product time:\n",
      "0.112129306793\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "print 'Function time:'\n",
    "print timeit.timeit(lambda: kl_wrapper.Eval(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Grad time:'\n",
    "print timeit.timeit(lambda: KLGrad(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Hessian vector product time:'\n",
    "print timeit.timeit(lambda: KLHessVecProd(free_par_vec, free_par_vec + 1), number=time_num) / time_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "def tr_optimize(trust_init, num_draws):\n",
    "    kl_wrapper.set_draws(num_draws)\n",
    "    vb_opt = optimize.minimize(\n",
    "        lambda par: kl_wrapper.Eval(par, verbose=True),\n",
    "        trust_init, method='trust-ncg', jac=KLGrad, hessp=KLHessVecProd,\n",
    "        tol=1e-6, options={'maxiter': 100, 'disp': True, 'gtol': 1e-6 })\n",
    "    return vb_opt.x\n",
    "\n",
    "def get_moment_vec(vb_opt_x):\n",
    "    glmm_par_opt.set_free(vb_opt_x)\n",
    "    set_moments(glmm_par_opt, moment_par)\n",
    "    return moment_par.get_vector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region\n",
      "16320.689584\n",
      "9038.30934811\n",
      "5009.20455721\n",
      "3813.34808885\n",
      "3003.48432409\n",
      "2392.18151882\n",
      "2308.02385314\n",
      "1937.79661974\n",
      "1914.25804238\n",
      "1766.0002339\n",
      "1763.67259786\n",
      "1694.19685044\n",
      "1693.44038432\n",
      "1688.44471877\n",
      "1687.5955049\n",
      "1686.48239351\n",
      "1686.40448757\n",
      "1686.32247727\n",
      "1686.17676412\n",
      "1686.17419633\n",
      "1686.16570804\n",
      "1686.16524403\n",
      "1686.16519242\n",
      "1686.16519235\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1686.165192\n",
      "         Iterations: 23\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n",
      "         Hessian evaluations: 0\n",
      "Done.\n",
      "0.978202748299\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "init_par_vec = copy.deepcopy(free_par_vec)\n",
    "\n",
    "# Optimize.\n",
    "num_mc_draws = 50\n",
    "\n",
    "print 'Running Newton Trust Region'\n",
    "vb_time = time.time()\n",
    "opt_x = tr_optimize(init_par_vec, num_mc_draws)\n",
    "vb_time = time.time() - vb_time\n",
    "\n",
    "print 'Done.'\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(opt_x)\n",
    "set_moments(glmm_par_opt, moment_par)\n",
    "\n",
    "print vb_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Investigate the performance of different numbers of draws.   It doesn't appear to\n",
    "    # converge.\n",
    "    opt_x_20 = tr_optimize(init_par_vec, 20)\n",
    "    opt_x_60 = tr_optimize(opt_x_20, 60)\n",
    "    opt_x_100 = tr_optimize(opt_x_60, 100)\n",
    "    opt_x_200 = tr_optimize(opt_x_100, 200)\n",
    "    opt_x_400 = tr_optimize(opt_x_200, 400)\n",
    "    opt_x_800 = tr_optimize(opt_x_400, 800)\n",
    "    \n",
    "    mom_20 = get_moment_vec(opt_x_20)\n",
    "    mom_60 = get_moment_vec(opt_x_60)\n",
    "    mom_100 = get_moment_vec(opt_x_100)\n",
    "    mom_200 = get_moment_vec(opt_x_200)\n",
    "    mom_400 = get_moment_vec(opt_x_400)\n",
    "    mom_800 = get_moment_vec(opt_x_800)\n",
    "\n",
    "    print np.max(np.abs((mom_20 - mom_60) / mom_20))\n",
    "    print np.max(np.abs((mom_60 - mom_100) / mom_60))\n",
    "    print np.max(np.abs((mom_100 - mom_200) / mom_100))\n",
    "    print np.max(np.abs((mom_200 - mom_400) / mom_200))\n",
    "    print np.max(np.abs((mom_400 - mom_800) / mom_400))\n",
    "\n",
    "    print '-------\\n'\n",
    "    print np.max(np.abs((mom_20 - mom_60)))\n",
    "    print np.max(np.abs((mom_60 - mom_100)))\n",
    "    print np.max(np.abs((mom_100 - mom_200)))\n",
    "    print np.max(np.abs((mom_200 - mom_400)))\n",
    "    print np.max(np.abs((mom_400 - mom_800)))\n",
    "\n",
    "    #diff_inds = np.where(np.abs(mom_60 - mom_100) > 1e-2)\n",
    "    #print diff_inds\n",
    "    #print moment_indices\n",
    "\n",
    "    #print (get_moment_vec(opt_x_60) - get_moment_vec(opt_x_100)) / np.abs(get_moment_vec(opt_x_100))\n",
    "    get_moment_vec(opt_x_200)\n",
    "    u200 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_400)\n",
    "    u400 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_800)\n",
    "    u800 = copy.deepcopy(moment_par['e_u'].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24072.6486679\n",
      "-24091.4677665\n",
      "0.00937548974616\n",
      "(array([   1,    3,    6, ..., 9994, 9998, 9999]),)\n",
      "0.495732316683\n",
      "0.503175450209\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f93b2d3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine why the means are different for different number of simulations.\n",
    "def get_logit_terms(num_draws):\n",
    "    kl_wrapper.set_draws(num_draws)\n",
    "    std_draws = kl_wrapper.std_draws\n",
    "\n",
    "    e_beta = glmm_par_opt['beta'].mean.get()\n",
    "    info_beta = glmm_par_opt['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "\n",
    "    e_u = glmm_par_opt['u'].mean.get()[y_g_vec]\n",
    "    info_u = glmm_par_opt['u'].info.get()[y_g_vec]\n",
    "    var_u = 1 / info_u\n",
    "\n",
    "    z_mean = e_u + np.matmul(x_mat, e_beta)\n",
    "    z_sd = np.sqrt(var_u + np.einsum('nk,kj,nj->n', x_mat, cov_beta, x_mat))\n",
    "    z = np.einsum('i,j->ij', z_sd, std_draws) + np.expand_dims(z_mean, 1)\n",
    "\n",
    "    # The sum is over observations and draws, so dividing by the draws size\n",
    "    # gives the sum of sample expectations over the draws.\n",
    "    # p = exp(z) / (1 + exp(z))\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_terms = np.log1p(np.exp(z))\n",
    "    logit_term = -np.sum(logit_terms) / std_draws.size\n",
    "\n",
    "    return logit_term, logit_terms, z\n",
    "    \n",
    "logit_term_50, logit_terms_50, z_50 = get_logit_terms(50)    \n",
    "logit_term_800, logit_terms_800, z_800 = get_logit_terms(800)\n",
    "\n",
    "print logit_term_50\n",
    "print logit_term_800\n",
    "\n",
    "logit_terms_50_mean = np.mean(logit_terms_50, 1)\n",
    "logit_terms_800_mean = np.mean(logit_terms_800, 1)\n",
    "\n",
    "print np.max(np.abs(logit_terms_50_mean - logit_terms_800_mean))\n",
    "print np.where(np.abs(logit_terms_50_mean - logit_terms_800_mean) > 1e-3)\n",
    "\n",
    "ind = 3\n",
    "plt.plot(z_800[ind, :], logit_terms_800[ind, :])\n",
    "plt.plot(z_50[ind, :], logit_terms_50[ind, :])\n",
    "print logit_terms_50_mean[ind]\n",
    "print logit_terms_800_mean[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Hessian:\n",
      "\n",
      "Log prior Hessian:\n",
      "\n",
      "hess_time: 96.953184\n"
     ]
    }
   ],
   "source": [
    "# Get the Hessians at the number of draws used for optimization.\n",
    "\n",
    "kl_wrapper.set_draws(num_mc_draws)\n",
    "\n",
    "hess_time = time.time()\n",
    "print 'KL Hessian:\\n'\n",
    "kl_hess = KLHess(opt_x)\n",
    "\n",
    "print 'Log prior Hessian:\\n'\n",
    "log_prior_hess = PriorHess(opt_x, prior_par.get_vector())\n",
    "\n",
    "hess_time =  time.time() - hess_time\n",
    "elbo_hess = -kl_hess\n",
    "\n",
    "print 'hess_time: %f' % hess_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moment_jac = MomentJacobian(opt_x)\n",
    "lrvb_cov = np.matmul(moment_jac, np.linalg.solve(kl_hess, moment_jac.T))\n",
    "\n",
    "prior_indices = copy.deepcopy(prior_par)\n",
    "prior_indices.set_vector(1 + np.array(range(prior_indices.vector_size())))\n",
    "\n",
    "vp_indices = copy.deepcopy(glmm_par_opt)\n",
    "vp_indices.set_vector(1 + np.array(range(vp_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/LRVBLogitGLMM/LogitGLMMLRVB/inst/data/simulated_data_large_python_vb_results.json\n"
     ]
    }
   ],
   "source": [
    "if not simulate_data:\n",
    "    run_name = 'production'\n",
    "    result_dict = { 'glmm_par_opt': glmm_par_opt.dictval(), 'run_name': run_name,\n",
    "                    'vb_time': vb_time, 'hess_time': hess_time, 'num_mc_draws': num_mc_draws, \n",
    "                    'moment_indices': moment_indices.dictval(),\n",
    "                    'prior_indices': prior_indices.dictval(),\n",
    "                    'vp_indices': vp_indices.dictval(),\n",
    "                    'lrvb_cov': lrvb_cov.tolist(), 'moment_jac': moment_jac.tolist(),\n",
    "                    'elbo_hess': elbo_hess.tolist(), 'log_prior_hess': log_prior_hess.tolist() }\n",
    "\n",
    "    result_json = json.dumps(result_dict)\n",
    "    json_file = open(json_output_filename, 'w')\n",
    "    json_file.write(result_json)\n",
    "    json_file.close()\n",
    "\n",
    "    print(json_output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
