{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from VariationalBayes import ScalarParam, ModelParamsDict, VectorParam, PosDefMatrixParam\n",
    "# from VariationalBayes.NormalParams import MVNParam, UVNParam, UVNParamVector\n",
    "# from VariationalBayes.GammaParams import GammaParam\n",
    "# from VariationalBayes.ExponentialFamilies import \\\n",
    "#     univariate_normal_entropy, multivariate_normal_entropy, gamma_entropy, \\\n",
    "#     mvn_prior, uvn_prior, gamma_prior\n",
    "\n",
    "import VariationalBayes as vb\n",
    "import LogisticGLMM_lib as logit_glmm\n",
    "from VariationalBayes.SparseObjectives import Objective, SparseObjective\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from autograd import jacobian\n",
    "# import autograd.numpy as np\n",
    "# import autograd.numpy.random as npr\n",
    "# import autograd.scipy as sp\n",
    "# import scipy as osp\n",
    "\n",
    "import copy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'y_group', u'mu_prior_var', u'mu_prior_t', u'mu_prior_var_c', u'K', u'beta_prior_var', u'tau_prior_beta', u'N', u'mu_prior_mean_c', u'mu_prior_epsilon', u'mu_prior_mean', u'y', u'x', u'NG', u'beta_prior_mean', u'tau_prior_alpha']\n",
      "0.171046565237\n"
     ]
    }
   ],
   "source": [
    "# Load data saved by stan_results_to_json.R and run_stan.R in LRVBLogitGLMM.\n",
    "import os\n",
    "import json\n",
    "\n",
    "simulate_data = False\n",
    "prior_par = vb.ModelParamsDict('Prior Parameters')\n",
    "\n",
    "if not simulate_data:\n",
    "    #analysis_name = 'simulated_data_small'\n",
    "    #analysis_name = 'simulated_data_large'\n",
    "    analysis_name = 'criteo_subsampled'\n",
    "\n",
    "    data_dir = os.path.join(os.environ['GIT_REPO_LOC'], 'LRVBLogitGLMM/LogitGLMMLRVB/inst/data/')\n",
    "    json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "    json_output_filename = os.path.join(data_dir, '%s_python_vb_results.json' % analysis_name)\n",
    "\n",
    "    json_file = open(json_filename, 'r')\n",
    "    json_dat = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    stan_dat = json_dat['stan_dat']\n",
    "    vp_base = json_dat['vp_base']\n",
    "\n",
    "    print stan_dat.keys()\n",
    "    K = stan_dat['K'][0]\n",
    "    NObs = stan_dat['N'][0]\n",
    "    NG = stan_dat['NG'][0]\n",
    "    #N = NObs / NG\n",
    "    y_g_vec = np.array(stan_dat['y_group'])\n",
    "    y_vec = np.array(stan_dat['y'])\n",
    "    x_mat = np.array(stan_dat['x'])\n",
    "    \n",
    "    # Define a class to contain prior parameters.\n",
    "    prior_par.push_param(vb.VectorParam('beta_prior_mean', K, val=np.array(stan_dat['beta_prior_mean'])))\n",
    "    beta_prior_info = np.linalg.inv(np.array(stan_dat['beta_prior_var']))\n",
    "    prior_par.push_param(vb.PosDefMatrixParam('beta_prior_info', K, val=beta_prior_info))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_mean', val=stan_dat['mu_prior_mean'][0]))\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_info', val=1 / stan_dat['mu_prior_var'][0]))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_alpha', val=stan_dat['tau_prior_alpha'][0]))\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_beta', val=stan_dat['tau_prior_beta'][0]))\n",
    "\n",
    "    # An index set to make sure jacobians match the order expected by R.\n",
    "    prior_par_indices = copy.deepcopy(prior_par)\n",
    "    prior_par_indices.set_name('Prior Indices')\n",
    "    prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "else:\n",
    "    # Simulate data instead of loading it if you like\n",
    "    N = 200     # observations per group\n",
    "    K = 5      # dimension of regressors\n",
    "    NG = 200      # number of groups\n",
    "\n",
    "    # Generate data\n",
    "\n",
    "    true_beta = np.array(range(5))\n",
    "    true_beta = true_beta - np.mean(true_beta)\n",
    "    true_mu = 0.\n",
    "    true_tau = 40.0\n",
    "\n",
    "    x_mat, y_g_vec, y_vec, true_rho, true_u = \\\n",
    "        logit_glmm.simulate_data(N, NG, true_beta, true_mu, true_tau)\n",
    "\n",
    "    prior_par.push_param(vb.VectorParam('beta_prior_mean', K, val=np.zeros(K)))\n",
    "    prior_par.push_param(vb.PosDefMatrixParam('beta_prior_info', K, val=0.01 * np.eye(K)))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_mean', val=0))\n",
    "    prior_par.push_param(vb.ScalarParam('mu_prior_info', val=0.5))\n",
    "\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_alpha', val=3.0))\n",
    "    prior_par.push_param(vb.ScalarParam('tau_prior_beta', val=10.0))\n",
    "\n",
    "print np.mean(y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-27-7e2e075b08ff>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-7e2e075b08ff>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Build an object to contain a variational approximation to a K-dimensional multivariate normal.\n",
    "glmm_par = vb.ModelParamsDict('GLMM Parameters')\n",
    "\n",
    "glmm_par.push_param(vb.UVNParam('mu', min_info=vp_base['mu_info_min'][0]))\n",
    "glmm_par.push_param(vb.GammaParam('tau',\n",
    "                               min_shape=vp_base['tau_alpha_min'][0],\n",
    "                               min_rate=vp_base['tau_beta_min'][0]))\n",
    "glmm_par.push_param(vb.MVNParam('beta', K, min_info=vp_base['beta_diag_min'][0]))\n",
    "glmm_par.push_param(vb.UVNParamVector('u', NG, min_info=vp_base['u_info_min'][0]))\n",
    "\n",
    "advi_init = False\n",
    "if advi_init:\n",
    "    pass\n",
    "# Initialize with ADVI.  Don't forget to add the ADVI computation time to your final VB time!\n",
    "#     advi_fit = json_dat['advi_results']\n",
    "#     glmm_par['mu'].mean.set(advi_fit['mu_mean'][0])\n",
    "#     glmm_par['mu'].info.set(1 / advi_fit['mu_var'][0])\n",
    "\n",
    "#     tau_mean = advi_fit['tau_mean'][0]\n",
    "#     tau_var = advi_fit['tau_var'][0]\n",
    "#     glmm_par['tau'].shape.set((tau_mean ** 2) / tau_var)\n",
    "#     glmm_par['tau'].rate.set(tau_var / tau_mean)\n",
    "\n",
    "#     glmm_par['beta'].mean.set(np.array(advi_fit['beta_mean']))\n",
    "#     glmm_par['beta'].info.set(np.array(advi_fit['beta_info']))\n",
    "\n",
    "#     glmm_par['u'].mean.set(np.array(advi_fit['u_mean']))\n",
    "#     glmm_par['u'].info.set(1 / np.array(advi_fit['u_var']))\n",
    "\n",
    "#     free_par_vec = glmm_par.get_free()\n",
    "else:\n",
    "    glmm_par['mu'].mean.set(0.0)\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    glmm_par['tau'].shape.set(2.0)\n",
    "    glmm_par['tau'].rate.set(2.0)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.full(K, 0.0))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.full(NG, 0.0))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "free_par_vec = glmm_par.get_free()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define moment parameters\n",
    "\n",
    "moment_par = vb.ModelParamsDict('Moment Parameters')\n",
    "moment_par.push_param(vb.VectorParam('e_beta', K))\n",
    "moment_par.push_param(vb.PosDefMatrixParam('e_beta_outer', K))\n",
    "moment_par.push_param(vb.ScalarParam('e_mu'))\n",
    "moment_par.push_param(vb.ScalarParam('e_mu2'))\n",
    "moment_par.push_param(vb.ScalarParam('e_tau'))\n",
    "moment_par.push_param(vb.ScalarParam('e_log_tau'))\n",
    "moment_par.push_param(vb.VectorParam('e_u', NG))\n",
    "moment_par.push_param(vb.VectorParam('e_u2', NG))\n",
    "\n",
    "def set_moments(glmm_par, moment_par):\n",
    "    moment_par['e_beta'].set(glmm_par['beta'].e())\n",
    "    moment_par['e_beta_outer'].set(glmm_par['beta'].e_outer())\n",
    "    moment_par['e_mu'].set(glmm_par['mu'].e())\n",
    "    moment_par['e_mu2'].set(glmm_par['mu'].e_outer())\n",
    "    moment_par['e_tau'].set(glmm_par['tau'].e())\n",
    "    moment_par['e_log_tau'].set(glmm_par['tau'].e_log())\n",
    "    moment_par['e_u'].set(glmm_par['u'].e())\n",
    "    moment_par['e_u2'].set((glmm_par['u'].e_outer()))\n",
    "    \n",
    "set_moments(glmm_par, moment_par)\n",
    "\n",
    "# Moment indices.\n",
    "moment_indices = copy.deepcopy(moment_par)\n",
    "moment_indices.set_vector(1 + np.array(range(moment_indices.vector_size())))\n",
    "\n",
    "class MomentWrapper(object):\n",
    "    def __init__(self, glmm_par, moment_par):\n",
    "        self.__glmm_par_ad = copy.deepcopy(glmm_par)\n",
    "        self.__moment_par = copy.deepcopy(moment_par)\n",
    "\n",
    "    # Return a posterior moment of interest as a function of unconstrained parameters.\n",
    "    def get_moments(self, free_par_vec):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        set_moments(self.__glmm_par_ad, self.__moment_par)\n",
    "        return self.__moment_par.get_vector()\n",
    "    \n",
    "    def get_moment_parameters(self, free_par_vec):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        set_moments(self.__glmm_par_ad, self.__moment_par)\n",
    "        return self.__moment_par\n",
    "\n",
    "moment_wrapper = MomentWrapper(glmm_par, moment_par)\n",
    "get_moment_jacobian = jacobian(moment_wrapper.get_moments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we need to calculate for the random effect.  For a scalar quantity, $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, whose distribution is determined by the variational approximation and which is different for every observation, we need to calculate\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\log\\left(1 - \\frac{\\exp(z)}{1 + \\exp(z)}\\right)\\right] = \\\\\n",
    "\\mathbb{E}\\left[\\log\\left(\\frac{1}{1 + \\exp(z)}\\right)\\right] = \\\\\n",
    "-\\mathbb{E}\\left[\\log\\left(1 + \\exp(z)\\right)\\right]\n",
    "$$\n",
    "\n",
    "Given $n$ draws from a standard normal, $u$, we can approximate this expectation with\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_n \\log\\left(1 + \\exp(\\sigma s_n + \\mu) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25300.32307202])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = logit_glmm.LogisticGLMM(glmm_par, prior_par, x_mat, y_vec, y_g_vec, 10)\n",
    "model.get_e_log_prior()\n",
    "model.get_log_lik()\n",
    "model.get_entropy()\n",
    "\n",
    "\n",
    "objective = Objective(model.glmm_par, model.get_kl)\n",
    "objective.fun_free(free_par_vec)\n",
    "\n",
    "# # PriorHess evaluates the second order derivative d2 EPrior / dpar dprior_par\n",
    "# PriorModelGrad = grad(kl_wrapper.ExpectedLogPrior, argnum=0)\n",
    "# PriorHess = jacobian(PriorModelGrad, argnum=1)\n",
    "\n",
    "# kl_wrapper.ExpectedLogPrior(free_par_vec, prior_par.get_vector())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function time:\n",
      "0.0274235963821\n",
      "Grad time:\n",
      "0.0664848089218\n",
      "Hessian vector product time:\n",
      "0.121609592438\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "print 'Function time:'\n",
    "print timeit.timeit(lambda: objective.fun_free(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Grad time:'\n",
    "print timeit.timeit(lambda: objective.fun_free_grad(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Hessian vector product time:'\n",
    "print timeit.timeit(lambda: objective.fun_free_hvp(free_par_vec, free_par_vec + 1), number=time_num) / time_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "def tr_optimize(trust_init, num_draws):\n",
    "    model.set_draws(num_draws)\n",
    "    vb_opt = optimize.minimize(\n",
    "        lambda par: objective.fun_free(par, verbose=True),\n",
    "        x0=trust_init,\n",
    "        method='trust-ncg',\n",
    "        jac=objective.fun_free_grad,\n",
    "        hessp=objective.fun_free_hvp,\n",
    "        tol=1e-6, options={'maxiter': 500, 'disp': True, 'gtol': 1e-6 })\n",
    "    return vb_opt.x\n",
    "\n",
    "def get_moment_vec(vb_opt_x):\n",
    "    glmm_par_opt.set_free(vb_opt_x)\n",
    "    set_moments(glmm_par_opt, moment_par)\n",
    "    return moment_par.get_vector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region\n",
      "('Iter ', 74, ' value: ', array([ 27104.9130999]))\n",
      "('Iter ', 75, ' value: ', array([ 19445.7923634]))\n",
      "('Iter ', 76, ' value: ', array([ 11937.56162486]))\n",
      "('Iter ', 77, ' value: ', array([ 8778.34570187]))\n",
      "('Iter ', 78, ' value: ', array([ 7326.78748725]))\n",
      "('Iter ', 79, ' value: ', array([ 6347.83589087]))\n",
      "('Iter ', 80, ' value: ', array([ 5292.24551932]))\n",
      "('Iter ', 81, ' value: ', array([ 4646.78724457]))\n",
      "('Iter ', 82, ' value: ', array([ 4184.24965692]))\n",
      "('Iter ', 83, ' value: ', array([ 2871.87627749]))\n",
      "('Iter ', 84, ' value: ', array([ 1681.46338868]))\n",
      "('Iter ', 85, ' value: ', array([ 824.00377353]))\n",
      "('Iter ', 86, ' value: ', array([ 463.90996476]))\n",
      "('Iter ', 87, ' value: ', array([-2800.23164255]))\n",
      "('Iter ', 88, ' value: ', array([-3989.89812081]))\n",
      "('Iter ', 89, ' value: ', array([-6724.36379076]))\n",
      "('Iter ', 90, ' value: ', array([-7697.64254572]))\n",
      "('Iter ', 91, ' value: ', array([-8815.30535638]))\n",
      "('Iter ', 92, ' value: ', array([-9708.72828352]))\n",
      "('Iter ', 93, ' value: ', array([-9950.87318647]))\n",
      "('Iter ', 94, ' value: ', array([-11431.26364851]))\n",
      "('Iter ', 95, ' value: ', array([-11638.37084771]))\n",
      "('Iter ', 96, ' value: ', array([-10788.43297102]))\n",
      "('Iter ', 97, ' value: ', array([-12313.70506391]))\n",
      "('Iter ', 98, ' value: ', array([-13814.3865894]))\n",
      "('Iter ', 99, ' value: ', array([-15941.45623158]))\n",
      "('Iter ', 100, ' value: ', array([-16708.40900934]))\n",
      "('Iter ', 101, ' value: ', array([-17250.15214703]))\n",
      "('Iter ', 102, ' value: ', array([-22008.25366182]))\n",
      "('Iter ', 103, ' value: ', array([-22107.55937948]))\n",
      "('Iter ', 104, ' value: ', array([ 26047.63509043]))\n",
      "('Iter ', 105, ' value: ', array([ 60313.69440782]))\n",
      "('Iter ', 106, ' value: ', array([-22572.52318961]))\n",
      "('Iter ', 107, ' value: ', array([-22955.00569289]))\n",
      "('Iter ', 108, ' value: ', array([-23074.78916273]))\n",
      "('Iter ', 109, ' value: ', array([-23223.90545527]))\n",
      "('Iter ', 110, ' value: ', array([-23358.70316508]))\n",
      "('Iter ', 111, ' value: ', array([-23579.14912998]))\n",
      "('Iter ', 112, ' value: ', array([-24031.04075961]))\n",
      "('Iter ', 113, ' value: ', array([-24031.09198003]))\n",
      "('Iter ', 114, ' value: ', array([-24686.48314287]))\n",
      "('Iter ', 115, ' value: ', array([-24948.1997111]))\n",
      "('Iter ', 116, ' value: ', array([-25075.44038296]))\n",
      "('Iter ', 117, ' value: ', array([-26175.97621801]))\n",
      "('Iter ', 118, ' value: ', array([-26424.2935889]))\n",
      "('Iter ', 119, ' value: ', array([-27496.8833007]))\n",
      "('Iter ', 120, ' value: ', array([-27561.82483835]))\n",
      "('Iter ', 121, ' value: ', array([-27561.82908719]))\n",
      "('Iter ', 122, ' value: ', array([-27570.41017075]))\n",
      "('Iter ', 123, ' value: ', array([-27722.41521271]))\n",
      "('Iter ', 124, ' value: ', array([-27723.64631643]))\n",
      "('Iter ', 125, ' value: ', array([-27723.64646384]))\n",
      "('Iter ', 126, ' value: ', array([-27736.76611097]))\n",
      "('Iter ', 127, ' value: ', array([-27736.77875655]))\n",
      "('Iter ', 128, ' value: ', array([-27737.0055951]))\n",
      "('Iter ', 129, ' value: ', array([-27737.04081015]))\n",
      "('Iter ', 130, ' value: ', array([-27737.19646482]))\n",
      "('Iter ', 131, ' value: ', array([-27737.19883019]))\n",
      "('Iter ', 132, ' value: ', array([-27737.20938078]))\n",
      "('Iter ', 133, ' value: ', array([-27737.20992712]))\n",
      "('Iter ', 134, ' value: ', array([-27737.20999293]))\n",
      "('Iter ', 135, ' value: ', array([-27737.20999301]))\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -27737.209993\n",
      "         Iterations: 61\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 59\n",
      "         Hessian evaluations: 0\n",
      "Done.\n",
      "2.07109261354\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "init_par_vec = copy.deepcopy(free_par_vec)\n",
    "objective.logger.initialize()\n",
    "\n",
    "# Optimize.\n",
    "num_mc_draws = 50\n",
    "\n",
    "print 'Running Newton Trust Region'\n",
    "vb_time = time.time()\n",
    "opt_x = tr_optimize(init_par_vec, num_mc_draws)\n",
    "vb_time = time.time() - vb_time\n",
    "\n",
    "print 'Done.'\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(opt_x)\n",
    "set_moments(glmm_par_opt, moment_par)\n",
    "\n",
    "print vb_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Investigate the performance of different numbers of draws.   It doesn't appear to\n",
    "    # converge.\n",
    "    opt_x_20 = tr_optimize(init_par_vec, 20)\n",
    "    opt_x_60 = tr_optimize(opt_x_20, 60)\n",
    "    opt_x_100 = tr_optimize(opt_x_60, 100)\n",
    "    opt_x_200 = tr_optimize(opt_x_100, 200)\n",
    "    opt_x_400 = tr_optimize(opt_x_200, 400)\n",
    "    opt_x_800 = tr_optimize(opt_x_400, 800)\n",
    "    \n",
    "    mom_20 = get_moment_vec(opt_x_20)\n",
    "    mom_60 = get_moment_vec(opt_x_60)\n",
    "    mom_100 = get_moment_vec(opt_x_100)\n",
    "    mom_200 = get_moment_vec(opt_x_200)\n",
    "    mom_400 = get_moment_vec(opt_x_400)\n",
    "    mom_800 = get_moment_vec(opt_x_800)\n",
    "\n",
    "    print np.max(np.abs((mom_20 - mom_60) / mom_20))\n",
    "    print np.max(np.abs((mom_60 - mom_100) / mom_60))\n",
    "    print np.max(np.abs((mom_100 - mom_200) / mom_100))\n",
    "    print np.max(np.abs((mom_200 - mom_400) / mom_200))\n",
    "    print np.max(np.abs((mom_400 - mom_800) / mom_400))\n",
    "\n",
    "    print '-------\\n'\n",
    "    print np.max(np.abs((mom_20 - mom_60)))\n",
    "    print np.max(np.abs((mom_60 - mom_100)))\n",
    "    print np.max(np.abs((mom_100 - mom_200)))\n",
    "    print np.max(np.abs((mom_200 - mom_400)))\n",
    "    print np.max(np.abs((mom_400 - mom_800)))\n",
    "\n",
    "    #diff_inds = np.where(np.abs(mom_60 - mom_100) > 1e-2)\n",
    "    #print diff_inds\n",
    "    #print moment_indices\n",
    "\n",
    "    #print (get_moment_vec(opt_x_60) - get_moment_vec(opt_x_100)) / np.abs(get_moment_vec(opt_x_100))\n",
    "    get_moment_vec(opt_x_200)\n",
    "    u200 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_400)\n",
    "    u400 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_800)\n",
    "    u800 = copy.deepcopy(moment_par['e_u'].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20954.820949\n",
      "-20958.0387844\n",
      "0.00209838052166\n",
      "(array([  16,   20,   32,   62,   65,   68,   70,   90,  110,  115,  143,\n",
      "        146,  165,  178,  184,  185,  195,  207,  212,  216,  219,  235,\n",
      "        254,  273,  282,  295,  323,  342,  379,  406,  411,  417,  436,\n",
      "        442,  479,  484,  490,  501,  505,  508,  523,  554,  575,  590,\n",
      "        597,  598,  605,  608,  619,  622,  637,  639,  640,  643,  646,\n",
      "        649,  650,  665,  679,  685,  692,  693,  713,  723,  736,  755,\n",
      "        759,  808,  815,  837,  841,  848,  849,  853,  872,  874,  896,\n",
      "        912,  919,  921,  927,  936,  937,  938,  951,  963,  967,  968,\n",
      "        978,  979,  981,  989, 1007, 1008, 1024, 1040, 1041, 1042, 1055,\n",
      "       1058, 1070, 1092, 1094, 1099, 1107, 1110, 1121, 1122, 1151, 1164,\n",
      "       1173, 1179, 1192, 1200, 1218, 1225, 1231, 1240, 1245, 1248, 1258,\n",
      "       1259, 1273, 1274, 1283, 1300, 1313, 1316, 1319, 1340, 1345, 1361,\n",
      "       1388, 1402, 1410, 1415, 1431, 1444, 1469, 1471, 1479, 1485, 1504,\n",
      "       1505, 1577, 1584, 1617, 1620, 1622, 1624, 1631, 1641, 1672, 1685,\n",
      "       1693, 1695, 1699, 1708, 1716, 1721, 1726, 1748, 1765, 1769, 1785,\n",
      "       1792, 1797, 1802, 1814, 1815, 1850, 1854, 1856, 1862, 1869, 1873,\n",
      "       1875, 1930, 1932, 1962, 1966, 1972, 1997, 2023, 2027, 2035, 2044,\n",
      "       2049, 2059, 2060, 2070, 2071, 2073, 2076, 2106, 2110, 2124, 2139,\n",
      "       2141, 2145, 2155, 2158, 2197, 2214, 2227, 2228, 2239, 2255, 2256,\n",
      "       2270, 2287, 2289, 2290, 2293, 2297, 2334, 2364, 2381, 2401, 2424,\n",
      "       2428, 2441, 2470, 2506, 2525, 2527, 2581, 2599, 2606, 2614, 2618,\n",
      "       2633, 2634, 2641, 2643, 2647, 2663, 2673, 2686, 2687, 2688, 2723,\n",
      "       2729, 2732, 2733, 2741, 2775, 2794, 2827, 2836, 2845, 2850, 2905,\n",
      "       2937, 2945, 2951, 2963, 2972, 2983, 2991, 3022, 3025, 3066, 3067,\n",
      "       3073, 3075, 3098, 3100, 3111, 3118, 3131, 3147, 3175, 3195, 3200,\n",
      "       3204, 3209, 3226, 3233, 3238, 3266, 3279, 3290, 3305, 3325, 3329,\n",
      "       3354, 3357, 3372, 3377, 3381, 3387, 3396, 3398, 3406, 3409, 3418,\n",
      "       3435, 3436, 3444, 3458, 3499, 3538, 3561, 3566, 3569, 3584, 3604,\n",
      "       3615, 3640, 3668, 3685, 3691, 3698, 3720, 3722, 3729, 3744, 3751,\n",
      "       3760, 3776, 3792, 3802, 3806, 3812, 3823, 3824, 3854, 3870, 3874,\n",
      "       3885, 3900, 3910, 3913, 3928, 3942, 3949, 3958, 3964, 3969, 3971,\n",
      "       3984, 4005, 4101, 4116, 4129, 4144, 4152, 4203, 4209, 4215, 4218,\n",
      "       4223, 4241, 4261, 4267, 4268, 4278, 4282, 4289, 4290, 4295, 4301,\n",
      "       4304, 4319, 4328, 4344, 4347, 4356, 4385, 4415, 4417, 4421, 4425,\n",
      "       4427, 4431, 4448, 4452, 4488, 4515, 4519, 4521, 4534, 4545, 4553,\n",
      "       4560, 4561, 4581, 4582, 4618, 4625, 4632, 4643, 4648, 4655, 4685,\n",
      "       4689, 4693, 4697, 4699, 4705, 4716, 4739, 4742, 4743, 4754, 4767,\n",
      "       4770, 4787, 4791, 4799, 4812, 4824, 4839, 4845, 4846, 4855, 4868,\n",
      "       4879, 4896, 4911, 4926, 4930, 4949, 5028, 5038, 5046, 5052, 5067,\n",
      "       5069, 5082, 5089, 5122, 5125, 5126, 5128, 5129, 5172, 5180, 5182,\n",
      "       5184, 5187, 5189, 5192, 5209, 5216, 5226, 5229, 5239, 5269, 5297,\n",
      "       5303, 5308, 5326, 5333, 5337, 5342, 5371, 5383, 5397, 5406, 5425,\n",
      "       5446, 5462, 5477, 5487, 5493, 5500, 5506, 5523, 5525, 5526, 5532,\n",
      "       5535, 5546, 5554, 5561, 5611, 5635, 5639, 5642, 5652, 5656, 5670,\n",
      "       5682, 5695, 5706, 5707, 5726, 5735, 5736, 5761, 5773, 5788, 5795,\n",
      "       5800, 5821, 5832, 5838, 5844, 5847, 5861, 5867, 5875, 5892, 5897,\n",
      "       5923, 5931, 5942, 5955, 5959, 5974, 5983, 5996, 6003, 6035, 6044,\n",
      "       6063, 6075, 6080, 6081, 6088, 6089, 6102, 6106, 6120, 6137, 6142,\n",
      "       6145, 6148, 6151, 6173, 6198, 6206, 6224, 6233, 6236, 6284, 6285,\n",
      "       6341, 6357, 6360, 6361, 6376, 6383, 6388, 6389, 6391, 6408, 6409,\n",
      "       6418, 6434, 6445, 6461, 6462, 6466, 6472, 6473, 6477, 6480, 6489,\n",
      "       6490, 6500, 6508, 6556, 6559, 6569, 6574, 6576, 6577, 6587, 6609,\n",
      "       6616, 6627, 6660, 6666, 6697, 6705, 6716, 6718, 6739, 6743, 6744,\n",
      "       6745, 6755, 6772, 6773, 6777, 6782, 6802, 6805, 6807, 6813, 6814,\n",
      "       6829, 6836, 6840, 6847, 6854, 6862, 6864, 6896, 6899, 6905, 6910,\n",
      "       6943, 6947, 6948, 6963, 6964, 6966, 6984, 7012, 7017, 7018, 7031,\n",
      "       7051, 7055, 7057, 7059, 7068, 7072, 7078, 7103, 7105, 7123, 7164,\n",
      "       7181, 7186, 7191, 7195, 7208, 7222, 7243, 7263, 7272, 7327, 7334,\n",
      "       7344, 7354, 7370, 7397, 7401, 7415, 7432, 7438, 7449, 7459, 7475,\n",
      "       7477, 7479, 7496, 7497, 7511, 7521, 7524, 7534, 7540, 7556, 7570,\n",
      "       7574, 7584, 7585, 7590, 7593, 7622, 7631, 7637, 7639, 7645, 7668,\n",
      "       7676, 7681, 7700, 7711, 7712, 7719, 7756, 7766, 7769, 7788, 7812,\n",
      "       7816, 7829, 7831, 7842, 7844, 7858, 7879, 7899, 7915, 7929, 7934,\n",
      "       7975, 7998, 8010, 8024, 8029, 8031, 8055, 8074, 8087, 8100, 8112,\n",
      "       8113, 8116, 8150, 8160, 8163, 8172, 8184, 8195, 8217, 8219, 8232,\n",
      "       8233, 8275, 8291, 8299, 8301, 8335, 8341, 8344, 8351, 8352, 8373,\n",
      "       8379, 8385, 8400, 8409, 8412, 8431, 8459, 8467, 8476, 8491, 8493,\n",
      "       8497, 8515, 8516, 8518, 8523, 8530, 8531, 8540, 8541, 8546, 8563,\n",
      "       8587, 8601, 8622, 8642, 8650, 8654, 8660, 8662, 8672, 8678, 8698,\n",
      "       8702, 8714, 8738, 8772, 8774, 8776, 8784, 8785, 8795, 8804, 8824,\n",
      "       8828, 8852, 8867, 8881, 8900, 8904, 8943, 8974, 8977, 8978, 8996,\n",
      "       9009, 9030, 9036, 9037, 9038, 9044, 9050, 9052, 9057, 9060, 9074,\n",
      "       9088, 9097, 9100, 9109, 9154, 9159, 9166, 9168, 9193, 9194, 9198,\n",
      "       9205, 9220, 9231, 9232, 9238, 9242, 9247, 9250, 9257, 9300, 9311,\n",
      "       9383, 9389, 9390, 9394, 9406, 9409, 9411, 9420, 9425, 9450, 9459,\n",
      "       9467, 9468, 9470, 9507, 9510, 9511, 9524, 9530, 9533, 9555, 9578,\n",
      "       9581, 9583, 9584, 9598, 9602, 9618, 9624, 9635, 9641, 9669, 9672,\n",
      "       9676, 9678, 9692, 9694, 9698, 9705, 9711, 9724, 9726, 9727, 9730,\n",
      "       9736, 9741, 9755, 9764, 9767, 9768, 9775, 9776, 9794, 9830, 9849,\n",
      "       9852, 9858, 9872, 9884, 9895, 9897, 9908, 9949, 9961, 9962, 9966,\n",
      "       9969, 9975, 9991]),)\n",
      "0.177588723668\n",
      "0.178248644529\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAIiIglMJSAiksD+P9lMW59b2GtaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0310c00ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine why the means are different for different number of simulations.\n",
    "def get_logit_terms(num_draws):\n",
    "    model.set_draws(num_draws)\n",
    "    std_draws = model.std_draws\n",
    "\n",
    "    e_beta = glmm_par_opt['beta'].mean.get()\n",
    "    info_beta = glmm_par_opt['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "\n",
    "    e_u = glmm_par_opt['u'].mean.get()[y_g_vec]\n",
    "    info_u = glmm_par_opt['u'].info.get()[y_g_vec]\n",
    "    var_u = 1 / info_u\n",
    "\n",
    "    z_mean = e_u + np.matmul(x_mat, e_beta)\n",
    "    z_sd = np.sqrt(var_u + np.einsum('nk,kj,nj->n', x_mat, cov_beta, x_mat))\n",
    "    z = np.einsum('i,j->ij', z_sd, std_draws) + np.expand_dims(z_mean, 1)\n",
    "\n",
    "    # The sum is over observations and draws, so dividing by the draws size\n",
    "    # gives the sum of sample expectations over the draws.\n",
    "    # p = exp(z) / (1 + exp(z))\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_terms = np.log1p(np.exp(z))\n",
    "    logit_term = -np.sum(logit_terms) / std_draws.size\n",
    "\n",
    "    return logit_term, logit_terms, z\n",
    "    \n",
    "logit_term_50, logit_terms_50, z_50 = get_logit_terms(50)    \n",
    "logit_term_800, logit_terms_800, z_800 = get_logit_terms(800)\n",
    "\n",
    "print logit_term_50\n",
    "print logit_term_800\n",
    "\n",
    "logit_terms_50_mean = np.mean(logit_terms_50, 1)\n",
    "logit_terms_800_mean = np.mean(logit_terms_800, 1)\n",
    "\n",
    "print np.max(np.abs(logit_terms_50_mean - logit_terms_800_mean))\n",
    "print np.where(np.abs(logit_terms_50_mean - logit_terms_800_mean) > 1e-3)\n",
    "\n",
    "ind = 3\n",
    "plt.plot(z_800[ind, :], logit_terms_800[ind, :])\n",
    "plt.plot(z_50[ind, :], logit_terms_50[ind, :])\n",
    "print logit_terms_50_mean[ind]\n",
    "print logit_terms_800_mean[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hessians at the number of draws used for optimization.\n",
    "\n",
    "model.set_draws(num_mc_draws)\n",
    "\n",
    "hess_time = time.time()\n",
    "print 'KL Hessian:\\n'\n",
    "kl_hess = objective.fun_free_hessian(opt_x)\n",
    "\n",
    "print 'Log prior Hessian:\\n'\n",
    "log_prior_hess = PriorHess(opt_x, prior_par.get_vector())\n",
    "\n",
    "hess_time =  time.time() - hess_time\n",
    "elbo_hess = -kl_hess\n",
    "\n",
    "print 'hess_time: %f' % hess_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moment_jac = MomentJacobian(opt_x)\n",
    "lrvb_cov = np.matmul(moment_jac, np.linalg.solve(kl_hess, moment_jac.T))\n",
    "\n",
    "prior_indices = copy.deepcopy(prior_par)\n",
    "prior_indices.set_vector(1 + np.array(range(prior_indices.vector_size())))\n",
    "\n",
    "vp_indices = copy.deepcopy(glmm_par_opt)\n",
    "vp_indices.set_vector(1 + np.array(range(vp_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not simulate_data:\n",
    "    # Write the result to a JSON file for use in R.\n",
    "    \n",
    "    run_name = 'production'\n",
    "    result_dict = { 'glmm_par_opt': glmm_par_opt.dictval(), 'run_name': run_name,\n",
    "                    'vb_time': vb_time, 'hess_time': hess_time, 'num_mc_draws': num_mc_draws, \n",
    "                    'moment_indices': moment_indices.dictval(),\n",
    "                    'prior_indices': prior_indices.dictval(),\n",
    "                    'vp_indices': vp_indices.dictval(),\n",
    "                    'lrvb_cov': lrvb_cov.tolist(), 'moment_jac': moment_jac.tolist(),\n",
    "                    'elbo_hess': elbo_hess.tolist(), 'log_prior_hess': log_prior_hess.tolist() }\n",
    "\n",
    "    result_json = json.dumps(result_dict)\n",
    "    json_file = open(json_output_filename, 'w')\n",
    "    json_file.write(result_json)\n",
    "    json_file.close()\n",
    "\n",
    "    print(json_output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
