{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "import LogisticGLMM_lib as logit_glmm\n",
    "from VariationalBayes.SparseObjectives import Objective, SparseObjective\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import autograd\n",
    "\n",
    "import copy\n",
    "from scipy import optimize\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['y', 'mu_prior_t', 'N', 'NG', 'K', 'mu_prior_var_c', 'beta_prior_var', 'beta_prior_mean', 'mu_prior_mean_c', 'x', 'mu_prior_var', 'tau_prior_beta', 'y_group', 'mu_prior_mean', 'mu_prior_epsilon', 'tau_prior_alpha'])\n",
      "0.324\n"
     ]
    }
   ],
   "source": [
    "# Load data saved by stan_results_to_json.R and run_stan.R in LRVBLogitGLMM.\n",
    "\n",
    "simulate_data = False\n",
    "\n",
    "if not simulate_data:\n",
    "    analysis_name = 'simulated_data_small'\n",
    "    #analysis_name = 'simulated_data_large'\n",
    "    #analysis_name = 'criteo_subsampled'\n",
    "\n",
    "    data_dir = os.path.join(os.environ['GIT_REPO_LOC'],\n",
    "                            'LinearResponseVariationalBayes.py/Models/LogisticGLMM/data')\n",
    "    json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "\n",
    "    json_file = open(json_filename, 'r')\n",
    "    json_dat = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    stan_dat = json_dat['stan_dat']\n",
    "    #vp_base = json_dat['vp_base']\n",
    "\n",
    "    print(stan_dat.keys())\n",
    "    K = stan_dat['K'][0]\n",
    "    NObs = stan_dat['N'][0]\n",
    "    NG = stan_dat['NG'][0]\n",
    "    #N = NObs / NG\n",
    "    y_g_vec = np.array(stan_dat['y_group'])\n",
    "    y_vec = np.array(stan_dat['y'])\n",
    "    x_mat = np.array(stan_dat['x'])\n",
    "    \n",
    "    # TODO: don't rely on vp_base, set it here.\n",
    "#     glmm_par = logit_glmm.get_glmm_parameters(\n",
    "#         K=K, NG=NG, \n",
    "#         mu_info_min=vp_base['mu_info_min'][0],\n",
    "#         tau_alpha_min=vp_base['tau_alpha_min'][0],\n",
    "#         tau_beta_min=vp_base['tau_beta_min'][0],\n",
    "#         beta_diag_min=vp_base['beta_diag_min'][0],\n",
    "#         u_info_min=vp_base['u_info_min'][0])\n",
    "\n",
    "    glmm_par = logit_glmm.get_glmm_parameters(K=K, NG=NG)\n",
    "\n",
    "    # Define a class to contain prior parameters.\n",
    "    prior_par = logit_glmm.get_default_prior_params(K)\n",
    "    prior_par['beta_prior_mean'].set(np.array(stan_dat['beta_prior_mean']))\n",
    "\n",
    "    beta_prior_info = np.linalg.inv(np.array(stan_dat['beta_prior_var']))\n",
    "    prior_par['beta_prior_info'].set(beta_prior_info)\n",
    "\n",
    "    prior_par['mu_prior_mean'].set(stan_dat['mu_prior_mean'][0])\n",
    "    prior_par['mu_prior_info'].set(1 / stan_dat['mu_prior_var'][0])\n",
    "    \n",
    "    prior_par['tau_prior_alpha'].set(stan_dat['tau_prior_alpha'][0])\n",
    "    prior_par['tau_prior_beta'].set(stan_dat['tau_prior_beta'][0])\n",
    "    \n",
    "    # An index set to make sure jacobians match the order expected by R.\n",
    "    prior_par_indices = copy.deepcopy(prior_par)\n",
    "    prior_par_indices.set_name('Prior Indices')\n",
    "    prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "else:\n",
    "    # Simulate data instead of loading it if you like\n",
    "    N = 200     # observations per group\n",
    "    K = 5      # dimension of regressors\n",
    "    NG = 200      # number of groups\n",
    "\n",
    "    # Generate data\n",
    "\n",
    "    true_beta = np.array(range(5))\n",
    "    true_beta = true_beta - np.mean(true_beta)\n",
    "    true_mu = 0.\n",
    "    true_tau = 40.0\n",
    "\n",
    "    x_mat, y_g_vec, y_vec, true_rho, true_u = \\\n",
    "        logit_glmm.simulate_data(N, NG, true_beta, true_mu, true_tau)\n",
    "\n",
    "    prior_par = logit_glmm.get_default_prior_params(K)\n",
    "\n",
    "    glmm_par = logit_glmm.get_glmm_parameters(\n",
    "        K=K, NG=NG, \n",
    "        mu_info_min=0.001, tau_alpha_min=0.001,\n",
    "        tau_beta_min=0.001, beta_diag_min=0.001,\n",
    "        u_info_min=0.001)\n",
    "\n",
    "\n",
    "print(np.mean(y_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "glmm_init = False\n",
    "if glmm_init and not simulate_data:\n",
    "    # Initialize with GLMM.\n",
    "    # If you use this, don't forget to add the computation time to your final VB time!\n",
    "    glmm_time = 0.\n",
    "\n",
    "    glmm_fit = json_dat['glmm_fit']\n",
    "    glmm_par['mu'].mean.set(glmm_fit['mu_mean'][0])\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    tau_mean = 1.0 / glmm_fit['mu_sd'][0] ** 2\n",
    "    tau_var = 1.0\n",
    "    glmm_par['tau'].shape.set((tau_mean ** 2) / tau_var)\n",
    "    glmm_par['tau'].rate.set(tau_var / tau_mean)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.array(glmm_fit['beta_mean']))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.array(glmm_fit['u_map']))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "    free_par_vec = glmm_par.get_free()\n",
    "else:\n",
    "    glmm_time = 0.\n",
    "    glmm_par['mu'].mean.set(0.0)\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    glmm_par['tau'].shape.set(2.0)\n",
    "    glmm_par['tau'].rate.set(2.0)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.full(K, 0.0))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.full(NG, 0.0))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "free_par_vec = glmm_par.get_free()\n",
    "init_par_vec = copy.deepcopy(free_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define moment parameters\n",
    "# Fix this:\n",
    "moment_wrapper = logit_glmm.MomentWrapper(glmm_par)\n",
    "get_moment_jacobian = autograd.jacobian(moment_wrapper.get_moment_vector)\n",
    "moment_jac = get_moment_jacobian(init_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_draws =  3\n",
      "\tFunction time: 0.0011361395008862019\n",
      "\tGrad time: 0.006242163898423314\n",
      "\tHessian vector product time: 0.015179845504462719\n",
      "\tPrior hess time:  0.10202980041503906\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = logit_glmm.LogisticGLMM(glmm_par, prior_par, x_mat, y_vec, y_g_vec, num_gh_points=10)\n",
    "model.get_e_log_prior()\n",
    "model.get_log_lik()\n",
    "model.get_entropy()\n",
    "\n",
    "objective = Objective(model.glmm_par, model.get_kl)\n",
    "objective.fun_free(free_par_vec)\n",
    "\n",
    "# # PriorHess evaluates the second order derivative d2 EPrior / dpar dprior_par\n",
    "def get_e_log_prior(prior_vec, free_par):\n",
    "    model.glmm_par.set_free(free_par)\n",
    "    model.prior_par.set_vector(prior_vec)\n",
    "    return model.get_e_log_prior()\n",
    "\n",
    "get_prior_model_grad = autograd.grad(get_e_log_prior, argnum=0)\n",
    "get_prior_hess = autograd.jacobian(get_prior_model_grad, argnum=1)\n",
    "\n",
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "num_draws = 3\n",
    "model.set_gh_points(num_draws)\n",
    "print('num_draws = ', num_draws)\n",
    "print('\\tFunction time:',\n",
    "      timeit.timeit(lambda: objective.fun_free(free_par_vec), number=time_num) / time_num)\n",
    "\n",
    "print('\\tGrad time:', \n",
    "      timeit.timeit(lambda: objective.fun_free_grad(free_par_vec), number=time_num) / time_num)\n",
    "\n",
    "print('\\tHessian vector product time:',\n",
    "      timeit.timeit(lambda: objective.fun_free_hvp(free_par_vec, free_par_vec + 1), number=time_num) / time_num)\n",
    "\n",
    "prior_vec = model.prior_par.get_vector()\n",
    "prior_hess_time = time.time()\n",
    "get_prior_hess(prior_vec, init_par_vec)\n",
    "prior_hess_time = time.time() - prior_hess_time\n",
    "print('\\tPrior hess time: ', prior_hess_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "def tr_optimize(trust_init, num_gh_points, gtol=1e-6, maxiter=500):\n",
    "    model.set_gh_points(num_gh_points)\n",
    "    objective.logger.initialize()\n",
    "    objective.logger.print_every = 5\n",
    "    vb_opt = optimize.minimize(\n",
    "        lambda par: objective.fun_free(par, verbose=True),\n",
    "        x0=trust_init,\n",
    "        method='trust-ncg',\n",
    "        jac=objective.fun_free_grad,\n",
    "        hessp=objective.fun_free_hvp,\n",
    "        tol=1e-6, options={'maxiter': maxiter, 'disp': True, 'gtol': gtol })\n",
    "    return vb_opt.x\n",
    "\n",
    "def get_moment_vec(vb_opt_x):\n",
    "    glmm_par_opt.set_free(vb_opt_x)\n",
    "    set_moments(glmm_par_opt, moment_par)\n",
    "    return moment_par.get_vector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#print(glmm_par)\n",
    "\n",
    "x_t_x = np.matmul(x_mat.transpose(), x_mat)\n",
    "x_t_y = np.matmul(x_mat.transpose(), y_vec)\n",
    "beta_init = np.linalg.solve(x_t_x, x_t_y)\n",
    "#print(beta_init)\n",
    "#plt.plot(sp.special.expit(np.matmul(x_mat, beta_init)), y_vec, 'k.')\n",
    "\n",
    "df = pd.DataFrame({ 'y_g': y_g_vec, 'y': y_vec}).groupby('y_g')\n",
    "#print(df.sum())\n",
    "u_init = np.array(df.sum()) / np.array(df.count()['y'])\n",
    "#plt.figure()\n",
    "print(np.min(y_g_vec))\n",
    "#plt.plot(u_init[y_g_vec], y_vec, 'k.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region with few draws.\n",
      "Iter  0  value:  1034.21559918\n",
      "\tx_diff:  inf\n",
      "Iter  5  value:  221.739464329\n",
      "\tx_diff:  1.4322191988\n",
      "Iter  10  value:  74.8349719048\n",
      "\tx_diff:  0.965661124529\n",
      "Iter  15  value:  66.4789758874\n",
      "\tx_diff:  0.247761061643\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 66.475043\n",
      "         Iterations: 19\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 20\n",
      "         Hessian evaluations: 0\n",
      "VB time:  3.2917680740356445\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Optimize.\n",
    "\n",
    "print('Running Newton Trust Region with few draws.')\n",
    "num_gh_points = 4\n",
    "vb_time = time.time()\n",
    "opt_x = tr_optimize(init_par_vec, num_gh_points, gtol=1e-6, maxiter=200)\n",
    "vb_time_opt_1 = time.time() - vb_time\n",
    "#print('vb_time_opt_1: ', vb_time_opt_1)\n",
    "vb_time = time.time() - vb_time\n",
    "\n",
    "# print('Running Newton Trust Region with more draws')\n",
    "# num_gh_points = 20\n",
    "# # vb_time = time.time()\n",
    "# opt_x = tr_optimize(opt_x, num_gh_points, gtol=1e-6, maxiter=100)\n",
    "# vb_time = time.time() - vb_time\n",
    "\n",
    "print('VB time: ', vb_time)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "glmm_par.set_free(opt_x)\n",
    "if simulate_data:\n",
    "    print('Fit beta: ', glmm_par['beta'].e())\n",
    "    print('True beta: ', true_beta)\n",
    "    \n",
    "    print('Fit mu: ', glmm_par['mu'].e())\n",
    "    print('True mu: ', true_mu)\n",
    "    \n",
    "    print('Fit tau: ', glmm_par['tau'].e())\n",
    "    print('True tau: ', true_tau)\n",
    "    \n",
    "    plt.plot(true_u, true_u, 'r.')\n",
    "    plt.plot(true_u, glmm_par['u'].e(), 'k.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.291741132736206\n",
      "2.6941299438476562e-05\n"
     ]
    }
   ],
   "source": [
    "print(vb_time_opt_1)\n",
    "print(vb_time - vb_time_opt_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB time:  3.2917680740356445\n",
      "[ 0.75057333  1.97883704  3.47870429  4.4064895   5.5172761 ]\n",
      "GLMM Parameters:\n",
      "\tmu:\n",
      "mu_mean: [-3.76040342]\n",
      "mu_info: [ 90.88414313]\n",
      "\ttau:\n",
      "tau_shape: [ 52.99999869]\n",
      "tau_rate: [ 58.32241746]\n",
      "\tbeta:\n",
      "beta_mean:\n",
      "[ 0.75057333  1.97883704  3.47870429  4.4064895   5.5172761 ]\n",
      "beta_info:\n",
      "[[ 18.91233326   1.45969847   1.67317139   1.09378758   1.37572422]\n",
      " [  1.45969847  18.97411949  -0.59499007   1.52431663  -1.09454916]\n",
      " [  1.67317139  -0.59499007  13.36520854  -0.51367932  -2.48218544]\n",
      " [  1.09378758   1.52431663  -0.51367932  15.00974391  -3.36009759]\n",
      " [  1.37572422  -1.09454916  -2.48218544  -3.36009759  12.75644606]]\n",
      "\tu:\n",
      "u_mean:\n",
      "[-3.18980066 -3.74859999 -3.97913848 -4.70410931 -3.69093709 -4.09411352\n",
      " -3.87183773 -3.94172706 -2.82666987 -3.6359192  -2.88333634 -3.41870507\n",
      " -3.91284487 -4.00891723 -3.76399229 -3.17962137 -3.44450378 -4.41156218\n",
      " -3.99633005 -3.1490323  -2.68846161 -3.83548643 -3.8408927  -4.82842501\n",
      " -3.35567165 -4.53142786 -3.83090614 -3.76787967 -3.42111924 -3.06340287\n",
      " -3.73167467 -3.36630898 -3.67263037 -4.11260261 -3.34343926 -5.59397769\n",
      " -3.94356693 -3.98061892 -4.21236721 -3.53283313 -3.89648206 -3.66683703\n",
      " -3.02188862 -4.71694563 -4.43426117 -4.3189521  -3.17712508 -3.89826213\n",
      " -3.31981319 -3.69163837 -3.53551663 -4.22378663 -3.41676435 -3.90806124\n",
      " -3.63758267 -2.80811589 -3.3226759  -4.26318088 -5.01158464 -3.31900162\n",
      " -3.59169999 -3.74566277 -3.76806199 -3.48473007 -3.70869314 -2.37969157\n",
      " -4.09349129 -4.02934308 -2.52645205 -3.8557559  -3.99597181 -4.254015\n",
      " -3.62082535 -5.32335615 -4.1501273  -3.05629241 -4.05322566 -3.23934751\n",
      " -3.95939307 -4.38257173 -3.33998824 -4.54926631 -4.11769389 -3.33506698\n",
      " -3.88903764 -3.33172681 -3.75137073 -2.60497026 -4.10033574 -4.62312127\n",
      " -3.76571372 -4.31130794 -3.143235   -3.30339387 -4.03158932 -3.8462852\n",
      " -3.79568795 -4.01539715 -3.69267976 -3.22127986]\n",
      "u_info:\n",
      "[ 1.23243046  1.07244071  1.77036705  1.16619925  1.47803136  1.15417275\n",
      "  1.00734764  1.05404812  1.23238419  1.21558183  1.28338599  1.1954618\n",
      "  1.2945286   1.44205468  0.92864276  1.52525037  1.16645367  1.12378318\n",
      "  1.14489779  1.15018432  1.50892323  1.0027715   1.75482208  1.12949558\n",
      "  1.38068286  1.21515937  1.48686561  1.19671347  1.12861974  1.39990774\n",
      "  1.42513708  1.27730633  1.42388762  1.13402406  1.20690318  1.52299209\n",
      "  1.07549155  1.08415719  1.11350508  1.20788107  1.05198915  1.17846657\n",
      "  1.37860385  1.29542599  1.44614252  1.26359887  1.83449955  1.55393044\n",
      "  1.32040096  1.04652458  1.32339214  1.4443766   1.7181527   1.32947092\n",
      "  1.37673358  1.55333522  1.29299608  1.60677366  1.18130018  1.23170808\n",
      "  1.46005702  1.11256554  0.91691339  1.46057215  1.3430229   1.26927195\n",
      "  1.24738809  1.37652366  1.6927929   1.08969597  1.58053823  1.56042114\n",
      "  1.32802892  1.39090917  1.36595726  1.40636429  1.49525216  1.6685651\n",
      "  1.07254263  1.24762382  1.17419664  1.14607478  1.33040724  1.23083192\n",
      "  1.35463693  1.18951959  1.28579075  1.40711399  1.12433199  1.10390231\n",
      "  1.38942822  1.20577659  1.36572296  1.15069039  1.28769817  1.42077038\n",
      "  1.38764678  1.48674427  1.47217472  1.4573427 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fc7973099e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhpi6EtSQwx9SWqIoS9JDTH0Jakh/w+vvrMjCrpuewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc779d74588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7973b2fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('VB time: ', vb_time)\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(opt_x)\n",
    "#glmm_par_opt.set_free(init_par_vec)\n",
    "print(glmm_par_opt['beta'].e())\n",
    "print(glmm_par_opt)\n",
    "\n",
    "#plt.plot(glmm_par_opt['u'].e(), glmm_par_opt['u'].var(), 'k.')\n",
    "\n",
    "e_beta = glmm_par_opt['beta'].e()\n",
    "e_u = glmm_par_opt['u'].e()[model.y_g_vec]\n",
    "\n",
    "z_mean = e_u + np.matmul(model.x_mat, e_beta)\n",
    "#plt.plot(sp.special.expit(z_mean), model.y_vec, 'k.')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(z_mean[model.y_vec == 1], 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.title('y == 1')\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(z_mean[model.y_vec == 0], 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.title('y == 0')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Hessian...\n",
      "\n",
      "Log prior Hessian...\n",
      "\n",
      "hess_time: 2.018448\n"
     ]
    }
   ],
   "source": [
    "# Get the Hessians at the number of draws used for optimization.\n",
    "\n",
    "hess_time = time.time()\n",
    "print('KL Hessian...\\n')\n",
    "kl_hess = objective.fun_free_hessian(opt_x)\n",
    "\n",
    "print('Log prior Hessian...\\n')\n",
    "log_prior_hess = get_prior_hess(prior_par.get_vector(), opt_x)\n",
    "\n",
    "hess_time =  time.time() - hess_time\n",
    "elbo_hess = -kl_hess\n",
    "\n",
    "print('hess_time: %f' % hess_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment_jac = get_moment_jacobian(opt_x)\n",
    "lrvb_cov = np.matmul(moment_jac, np.linalg.solve(kl_hess, moment_jac.T))\n",
    "\n",
    "prior_indices = copy.deepcopy(prior_par)\n",
    "prior_indices.set_vector(1 + np.array(range(prior_indices.vector_size())))\n",
    "\n",
    "vp_indices = copy.deepcopy(glmm_par_opt)\n",
    "vp_indices.set_vector(1 + np.array(range(vp_indices.vector_size())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/LinearResponseVariationalBayes.py/Models/LogisticGLMM/data/simulated_data_small_python_vb_results.json\n",
      "/home/rgiordan/Documents/git_repos/LinearResponseVariationalBayes.py/Models/LogisticGLMM/data/simulated_data_small_python_vb_results.pkl\n"
     ]
    }
   ],
   "source": [
    "if not simulate_data:\n",
    "    # Write the result to a JSON file for use in R.\n",
    "    \n",
    "    run_name = 'production'\n",
    "    result_dict = { 'glmm_par_opt': glmm_par_opt.dictval(), 'run_name': run_name,\n",
    "                    'vb_time': vb_time, 'hess_time': hess_time, 'num_gh_points': num_gh_points, \n",
    "                    'lrvb_cov': lrvb_cov.tolist(), 'moment_jac': moment_jac.tolist(),\n",
    "                    'elbo_hess': elbo_hess.tolist(), 'log_prior_hess': log_prior_hess.tolist() }\n",
    "\n",
    "    result_json = json.dumps(result_dict)\n",
    "    json_output_filename = os.path.join(data_dir, '%s_python_vb_results.json' % analysis_name)\n",
    "    json_file = open(json_output_filename, 'w')\n",
    "    json_file.write(result_json)\n",
    "    json_file.close()\n",
    "    print(json_output_filename)\n",
    "\n",
    "    \n",
    "    \n",
    "    pickle_output_filename = os.path.join(data_dir, '%s_python_vb_results.pkl' % analysis_name)\n",
    "    pickle_output = open(pickle_output_filename, 'wb')\n",
    "\n",
    "    # TODO: save everything needed to reconstruct glmm_par, since the parameters cannot be pickled\n",
    "    # due to having function pointers.\n",
    "    # Unlike with JSON, numpy arrays can be pickled.\n",
    "    pickle_result_dict = {  'glmm_par_dictval': glmm_par_opt.dictval(),\n",
    "                            'glmm_par_vector': glmm_par_opt.get_vector(),\n",
    "                            'glmm_par_free': glmm_par_opt.get_free(),\n",
    "                            'run_name': run_name,\n",
    "                            'vb_time': vb_time,\n",
    "                            'hess_time': hess_time,\n",
    "                            'num_gh_points': num_gh_points, \n",
    "                            'lrvb_cov': np.squeeze(lrvb_cov),\n",
    "                            'moment_jac': np.squeeze(moment_jac),\n",
    "                            'elbo_hess': np.squeeze(elbo_hess),\n",
    "                            'log_prior_hess': np.squeeze(log_prior_hess) }\n",
    "\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(pickle_result_dict, pickle_output)\n",
    "    pickle_output.close()\n",
    "    \n",
    "    print(pickle_output_filename)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
