{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "import LogisticGLMM_lib as logit_glmm\n",
    "import VariationalBayes.SparseObjectives as vb_sparse\n",
    "import VariationalBayes.ExponentialFamilies as ef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import autograd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "\n",
    "import copy\n",
    "from scipy import optimize\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "#analysis_name = 'simulated_data_small'\n",
    "analysis_name = 'criteo_subsampled'\n",
    "\n",
    "data_dir = os.path.join(os.environ['GIT_REPO_LOC'],\n",
    "                        'LinearResponseVariationalBayes.py/Models/LogisticGLMM/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_output_filename = os.path.join(data_dir, '%s_python_vb_results.pkl' % analysis_name)\n",
    "pkl_file = open(pickle_output_filename, 'rb')\n",
    "vb_data = pickle.load(pkl_file)\n",
    "\n",
    "json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "\n",
    "json_file = open(json_filename, 'r')\n",
    "json_dat = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "stan_dat = json_dat['stan_dat']\n",
    "\n",
    "K = stan_dat['K'][0]\n",
    "NObs = stan_dat['N'][0]\n",
    "NG = stan_dat['NG'][0]\n",
    "y_g_vec = np.array(stan_dat['y_group'])\n",
    "y_vec = np.array(stan_dat['y'])\n",
    "x_mat = np.array(stan_dat['x'])\n",
    "\n",
    "glmm_par = logit_glmm.get_glmm_parameters(K=K, NG=NG)\n",
    "\n",
    "# Define a class to contain prior parameters.\n",
    "prior_par = logit_glmm.get_default_prior_params(K)\n",
    "prior_par['beta_prior_mean'].set(np.array(stan_dat['beta_prior_mean']))\n",
    "\n",
    "prior_par['beta_prior_info'].set(np.array(stan_dat['beta_prior_info']))\n",
    "\n",
    "prior_par['mu_prior_mean'].set(stan_dat['mu_prior_mean'][0])\n",
    "prior_par['mu_prior_info'].set(stan_dat['mu_prior_info'][0])\n",
    "\n",
    "prior_par['tau_prior_alpha'].set(stan_dat['tau_prior_alpha'][0])\n",
    "prior_par['tau_prior_beta'].set(stan_dat['tau_prior_beta'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticGLMMLogPosterior(object):\n",
    "    def __init__(\n",
    "        self, glmm_par_draw, prior_par, x_mat, y_vec, y_g_vec):\n",
    "\n",
    "        self.glmm_par_draw = copy.deepcopy(glmm_par_draw)\n",
    "        self.prior_par = copy.deepcopy(prior_par)\n",
    "        self.x_mat = x_mat\n",
    "        self.y_vec = y_vec\n",
    "        self.y_g_vec = y_g_vec\n",
    "        self.K = x_mat.shape[1]\n",
    "\n",
    "        assert np.min(y_g_vec) == 0\n",
    "        assert np.max(y_g_vec) == self.glmm_par_draw['u'].size() - 1\n",
    "\n",
    "    def get_log_prior(self):\n",
    "        beta = self.glmm_par_draw['beta'].get()\n",
    "        mu = self.glmm_par_draw['mu'].get()\n",
    "        tau = self.glmm_par_draw['tau'].get()\n",
    "        log_tau = np.log(tau)\n",
    "        \n",
    "        cov_beta = np.zeros((self.K, self.K))\n",
    "        beta_prior_info = self.prior_par['beta_prior_info'].get()\n",
    "        beta_prior_mean = self.prior_par['beta_prior_mean'].get()\n",
    "        log_p_beta = ef.mvn_prior(\n",
    "            beta_prior_mean, beta_prior_info, beta, cov_beta)\n",
    "\n",
    "        log_p_mu = ef.uvn_prior(\n",
    "            self.prior_par['mu_prior_mean'].get(),\n",
    "            self.prior_par['mu_prior_info'].get(), mu, 0.0)\n",
    "\n",
    "        tau_prior_shape = self.prior_par['tau_prior_alpha'].get()\n",
    "        tau_prior_rate = self.prior_par['tau_prior_beta'].get()\n",
    "        log_p_tau = ef.gamma_prior(\n",
    "            tau_prior_shape, tau_prior_rate, tau, log_tau)\n",
    "\n",
    "        return log_p_beta + log_p_mu + log_p_tau\n",
    "\n",
    "    def get_log_lik(self):\n",
    "        beta = self.glmm_par_draw['beta'].get()\n",
    "        u = self.glmm_par_draw['u'].get()\n",
    "        mu = self.glmm_par_draw['mu'].get()\n",
    "        tau = self.glmm_par_draw['tau'].get()\n",
    "        log_tau = np.log(tau)\n",
    "\n",
    "        log_lik = 0.\n",
    "\n",
    "        # Log likelihood from data.\n",
    "        z = u[self.y_g_vec] + np.matmul(self.x_mat, beta)\n",
    "        log_lik += np.sum(self.y_vec * z - np.log1p(np.exp(z)))\n",
    "\n",
    "        # Log likelihood from random effect terms.\n",
    "        log_lik += -0.5 * tau * np.sum((mu - u) ** 2) + 0.5 * log_tau * len(u)\n",
    "\n",
    "        return log_lik\n",
    "\n",
    "    def get_log_posterior(self):\n",
    "        return np.squeeze(\n",
    "            self.get_log_lik() + \\\n",
    "            self.get_log_prior())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-42905.344740757813"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = logit_glmm.LogisticGLMM(\n",
    "    glmm_par, prior_par, x_mat, y_vec, y_g_vec, num_gh_points=vb_data['num_gh_points'])\n",
    "\n",
    "glmm_par_draw = vb.ModelParamsDict('GLMM Parameter Draw')\n",
    "glmm_par_draw.push_param(vb.ScalarParam('mu', val=0.0))\n",
    "glmm_par_draw.push_param(vb.ScalarParam('tau', val=1.0))\n",
    "glmm_par_draw.push_param(vb.VectorParam('beta', K, val=np.full(K, 0.)))\n",
    "glmm_par_draw.push_param(vb.VectorParam('u', NG))\n",
    "\n",
    "log_model = LogisticGLMMLogPosterior(glmm_par_draw, prior_par, x_mat, y_vec, y_g_vec)\n",
    "log_model.get_log_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10014, 10014)\n"
     ]
    }
   ],
   "source": [
    "class LogisticGLMMBootstrap(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.weights = np.full(model.x_mat.shape[0], 1.0)\n",
    "        self.num_gh_points = 5\n",
    "        self.model.set_gh_points(self.num_gh_points)\n",
    "        \n",
    "        self.get_weight_obs_jacobian = autograd.grad(\n",
    "            self.get_data_log_lik_term)\n",
    "        \n",
    "        self.obs = 0\n",
    "        \n",
    "    def get_data_log_lik_term(self, free_par, obs):\n",
    "        self.model.glmm_par.set_free(free_par)\n",
    "        return logit_glmm.get_data_log_lik_terms(\n",
    "                glmm_par = self.model.glmm_par,\n",
    "                x_mat = np.atleast_2d(self.model.x_mat[obs, :]),\n",
    "                y_vec = np.atleast_1d(self.model.y_vec[obs]),\n",
    "                y_g_vec = np.atleast_1d(self.model.y_g_vec[obs]),\n",
    "                gh_x = self.model.gh_x,\n",
    "                gh_w = self.model.gh_w)[0]\n",
    "    \n",
    "    def get_weight_jacobian_list(self, free_par):\n",
    "        weight_jacobian_list = []\n",
    "        #np.full((self.model.x_mat.shape[0], len(free_par)), float('nan'))\n",
    "        print('Running.')\n",
    "        for obs in range(self.model.x_mat.shape[0]):\n",
    "            if obs % 1000 == 0:\n",
    "                print('Obs {}'.format(obs))\n",
    "            weight_jacobian_list.append(self.get_weight_obs_jacobian(free_par, obs))\n",
    "        print('Done.')\n",
    "        return weight_jacobian_list\n",
    "            \n",
    "    def wrap_data_log_lik_terms(self, free_par):\n",
    "        self.model.glmm_par.set_free(free_par)\n",
    "        return self.model.get_data_log_lik_terms()\n",
    "\n",
    "    def optimize_with_weights(self, weights, init_par, gtol=1e-6, print_every=1):\n",
    "        self.model.use_weights = True\n",
    "        self.model.weights = copy.deepcopy(weights)\n",
    "        return self.model.tr_optimize(\n",
    "            init_par, num_gh_points=self.num_gh_points, gtol=gtol, print_every=print_every)   \n",
    "    \n",
    "    def get_model_weight_grad(self, free_par_vec, weights):\n",
    "        self.model.use_weights = True\n",
    "        self.model.weights = copy.deepcopy(weights)\n",
    "        return self.model.objective.fun_free_grad(free_par_vec)\n",
    "\n",
    "    \n",
    "glmm_bootstrap_object = LogisticGLMMBootstrap(model)\n",
    "\n",
    "glmm_par_free = vb_data['glmm_par_free']\n",
    "elbo_hess = vb_sparse.unpack_csr_matrix(vb_data['elbo_hess_packed'])\n",
    "moment_jac = vb_data['moment_jac']\n",
    "print(elbo_hess.shape)\n",
    "\n",
    "moment_wrapper = logit_glmm.MomentWrapper(glmm_par)\n",
    "get_moment_jacobian = autograd.jacobian(moment_wrapper.get_moment_vector_from_free)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61895.0\n",
      "Iter  0  value:  23825.6902287\n",
      "Iter  1  value:  23825.6902251\n",
      "Iter  2  value:  23825.6902241\n",
      "Iter  3  value:  23825.689913\n",
      "Iter  4  value:  23825.6898997\n",
      "Iter  5  value:  23825.689899\n",
      "Iter  6  value:  23825.689899\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 23825.689899\n",
      "         Iterations: 6\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 7\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "base_weights = np.full(NObs, 1.0)\n",
    "print(np.sum(base_weights))\n",
    "base_opt = glmm_bootstrap_object.optimize_with_weights(\n",
    "    weights=base_weights, init_par=glmm_par_free, gtol=1e-8)\n",
    "base_free_par = base_opt.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'LogisticGLMM_lib' has no attribute 'SparseModelObjective'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-173af691db18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get a sparse Jacobian.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m sparse_model = logit_glmm.SparseModelObjective(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglmm_par\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_par\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_g_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     num_gh_points=model.num_gh_points, num_groups=1)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'LogisticGLMM_lib' has no attribute 'SparseModelObjective'"
     ]
    }
   ],
   "source": [
    "# Get a sparse Jacobian.\n",
    "sparse_model = logit_glmm.SparseModelObjective(\n",
    "    model.glmm_par, model.prior_par, model.x_mat, model.y_vec, model.y_g_vec,\n",
    "    num_gh_points=model.num_gh_points, num_groups=1)\n",
    "\n",
    "sparse_model.glmm_par.set_free(model.glmm_par.get_free())\n",
    "jac_time = time.time()\n",
    "weight_jacobian = sparse_model.get_sparse_weight_free_jacobian(print_every_n=100)\n",
    "jac_time = time.time() - jac_time\n",
    "\n",
    "print('Jacobian time: ', jac_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too bad you can't pickle sparse cholesky decomposition.\n",
    "\n",
    "from scikits.sparse.cholmod import cholesky\n",
    "\n",
    "inverse_time = time.time()\n",
    "\n",
    "print('Cholesky:')\n",
    "elbo_hess_chol = cholesky(elbo_hess)\n",
    "\n",
    "print('Solve:')\n",
    "param_boot_mat = -1 * elbo_hess_chol.solve_A(weight_jacobian.T)\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "inverse_time = time.time() - inverse_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('To dense:')\n",
    "# param_boot_mat_dense = np.asarray(param_boot_mat.todense())\n",
    "\n",
    "# print('Multiplication:')\n",
    "# moment_boot_mat = np.matmul(moment_jac, param_boot_mat_dense)\n",
    "\n",
    "# moment_jac_sparse = sp.sparse.csr_matrix(moment_jac)\n",
    "# print('Multiplication')\n",
    "# moment_boot_mat = moment_jac * param_boot_mat\n",
    "# print('Done')\n",
    "\n",
    "#print(moment_boot_mat.shape)\n",
    "\n",
    "\n",
    "# Moment boot mat would be huge in big applications.  Better to not even construct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluated at a linear combination of the bootstrap draw and base weights.\n",
    "# Note that at the full bootstrap draw, the response is quite nonlinear.\n",
    "\n",
    "use_jackknife = True\n",
    "if use_jackknife:\n",
    "    boot_draw = copy.deepcopy(base_weights)\n",
    "    boot_draw[0] = 0.\n",
    "else:\n",
    "    boot_draw = np.random.multinomial(NObs, [1. / NObs] * NObs, size=1) - 1.0\n",
    "    boot_draw = 1.0 * boot_draw + base_weights\n",
    "\n",
    "print('Total weight (there are {} observations): {}'.format(NObs, np.sum(boot_draw)))\n",
    "\n",
    "lr_param_diff = param_boot_mat * (np.squeeze(boot_draw) - 1.0)\n",
    "\n",
    "# Optionally, try taking a Newton step using the Hessian.\n",
    "model_weight_grad = glmm_bootstrap_object.get_model_weight_grad(\n",
    "    base_free_par + lr_param_diff, boot_draw)\n",
    "model_weight_grad_sp = sp.sparse.csc_matrix(np.expand_dims(model_weight_grad, axis=1))\n",
    "boot_newton_step_sp = -1 * elbo_hess_chol.solve_A(model_weight_grad_sp)\n",
    "boot_newton_step = np.asarray(boot_newton_step_sp)\n",
    "print(np.max(np.abs(boot_newton_step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glmm_par_opt_boot = glmm_bootstrap_object.optimize_with_weights(\n",
    "    init_par=base_free_par + lr_param_diff, weights=boot_draw, gtol=1e-8)\n",
    "boot_free_par = glmm_par_opt_boot.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual change in the moment vectors.\n",
    "moment_vec = moment_wrapper.get_moment_vector_from_free(base_free_par)\n",
    "moment_vec_boot = moment_wrapper.get_moment_vector_from_free(boot_free_par)\n",
    "\n",
    "# Use the linear approximation for the parameters, not the moments.\n",
    "moment_vec_boot_step = moment_wrapper.get_moment_vector_from_free(\n",
    "    base_free_par + lr_param_diff)\n",
    "boot_step_moment_diff = moment_vec_boot - moment_vec_boot_step\n",
    "\n",
    "# The difference based on a linear approximation to the moments.\n",
    "lr_moment_diff = \\\n",
    "    np.matmul(moment_jac, param_boot_mat * np.squeeze(boot_draw) - 1.0)\n",
    "\n",
    "true_moment_diff = moment_vec_boot - moment_vec\n",
    "print('True norm difference: {}'.format(np.linalg.norm(true_moment_diff)))\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Linearizing the moments')\n",
    "plt.plot(true_moment_diff, lr_moment_diff, 'r+', markersize=10)\n",
    "plt.plot(true_moment_diff, true_moment_diff, 'k.')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Linearizing the parameters')\n",
    "plt.plot(true_moment_diff, boot_step_moment_diff, 'bx', markersize=10)\n",
    "plt.plot(true_moment_diff, true_moment_diff, 'k.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment_indices = copy.deepcopy(moment_wrapper.moment_par)\n",
    "moment_indices.set_vector(np.arange(0, moment_indices.vector_size()))\n",
    "print(moment_indices)\n",
    "u_ind = moment_indices['e_u'].get()\n",
    "beta_ind = moment_indices['e_beta'].get()\n",
    "non_u_ind = list(set(moment_indices.get_vector()) - set(u_ind))\n",
    "print(non_u_ind)\n",
    "\n",
    "#true_moment_diff[u_ind]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(true_moment_diff[u_ind], lr_moment_diff[u_ind], 'r+', markersize=20)\n",
    "plt.plot(true_moment_diff[u_ind], true_moment_diff[u_ind], 'k.')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(true_moment_diff[non_u_ind], lr_moment_diff[non_u_ind], 'r+', markersize=20)\n",
    "plt.plot(true_moment_diff[non_u_ind], true_moment_diff[non_u_ind], 'k.')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(true_moment_diff[beta_ind], lr_moment_diff[beta_ind], 'r+', markersize=20)\n",
    "plt.plot(true_moment_diff[beta_ind], true_moment_diff[beta_ind], 'k.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
