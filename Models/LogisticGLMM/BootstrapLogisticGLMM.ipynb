{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "import LogisticGLMM_lib as logit_glmm\n",
    "import VariationalBayes.SparseObjectives as vb_sparse\n",
    "import VariationalBayes.ExponentialFamilies as ef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import autograd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "\n",
    "import copy\n",
    "from scipy import optimize\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "analysis_name = 'simulated_data_small'\n",
    "#analysis_name = 'criteo_subsampled'\n",
    "\n",
    "data_dir = os.path.join(os.environ['GIT_REPO_LOC'],\n",
    "                        'LinearResponseVariationalBayes.py/Models/LogisticGLMM/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_output_filename = os.path.join(data_dir, '%s_python_vb_results.pkl' % analysis_name)\n",
    "pkl_file = open(pickle_output_filename, 'rb')\n",
    "vb_data = pickle.load(pkl_file)\n",
    "\n",
    "json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "\n",
    "json_file = open(json_filename, 'r')\n",
    "json_dat = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "stan_dat = json_dat['stan_dat']\n",
    "\n",
    "K = stan_dat['K'][0]\n",
    "NObs = stan_dat['N'][0]\n",
    "NG = stan_dat['NG'][0]\n",
    "y_g_vec = np.array(stan_dat['y_group'])\n",
    "y_vec = np.array(stan_dat['y'])\n",
    "x_mat = np.array(stan_dat['x'])\n",
    "\n",
    "glmm_par = logit_glmm.get_glmm_parameters(K=K, NG=NG)\n",
    "\n",
    "# Define a class to contain prior parameters.\n",
    "prior_par = logit_glmm.get_default_prior_params(K)\n",
    "prior_par['beta_prior_mean'].set(np.array(stan_dat['beta_prior_mean']))\n",
    "\n",
    "prior_par['beta_prior_info'].set(np.array(stan_dat['beta_prior_info']))\n",
    "\n",
    "prior_par['mu_prior_mean'].set(stan_dat['mu_prior_mean'][0])\n",
    "prior_par['mu_prior_info'].set(stan_dat['mu_prior_info'][0])\n",
    "\n",
    "prior_par['tau_prior_alpha'].set(stan_dat['tau_prior_alpha'][0])\n",
    "prior_par['tau_prior_beta'].set(stan_dat['tau_prior_beta'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticGLMMLogPosterior(object):\n",
    "    def __init__(\n",
    "        self, glmm_par_draw, prior_par, x_mat, y_vec, y_g_vec):\n",
    "\n",
    "        self.glmm_par_draw = copy.deepcopy(glmm_par_draw)\n",
    "        self.prior_par = copy.deepcopy(prior_par)\n",
    "        self.x_mat = x_mat\n",
    "        self.y_vec = y_vec\n",
    "        self.y_g_vec = y_g_vec\n",
    "        self.K = x_mat.shape[1]\n",
    "\n",
    "        assert np.min(y_g_vec) == 0\n",
    "        assert np.max(y_g_vec) == self.glmm_par_draw['u'].size() - 1\n",
    "\n",
    "    def get_log_prior(self):\n",
    "        beta = self.glmm_par_draw['beta'].get()\n",
    "        mu = self.glmm_par_draw['mu'].get()\n",
    "        tau = self.glmm_par_draw['tau'].get()\n",
    "        log_tau = np.log(tau)\n",
    "        \n",
    "        cov_beta = np.zeros((self.K, self.K))\n",
    "        beta_prior_info = self.prior_par['beta_prior_info'].get()\n",
    "        beta_prior_mean = self.prior_par['beta_prior_mean'].get()\n",
    "        log_p_beta = ef.mvn_prior(\n",
    "            beta_prior_mean, beta_prior_info, beta, cov_beta)\n",
    "\n",
    "        log_p_mu = ef.uvn_prior(\n",
    "            self.prior_par['mu_prior_mean'].get(),\n",
    "            self.prior_par['mu_prior_info'].get(), mu, 0.0)\n",
    "\n",
    "        tau_prior_shape = self.prior_par['tau_prior_alpha'].get()\n",
    "        tau_prior_rate = self.prior_par['tau_prior_beta'].get()\n",
    "        log_p_tau = ef.gamma_prior(\n",
    "            tau_prior_shape, tau_prior_rate, tau, log_tau)\n",
    "\n",
    "        return log_p_beta + log_p_mu + log_p_tau\n",
    "\n",
    "    def get_log_lik(self):\n",
    "        beta = self.glmm_par_draw['beta'].get()\n",
    "        u = self.glmm_par_draw['u'].get()\n",
    "        mu = self.glmm_par_draw['mu'].get()\n",
    "        tau = self.glmm_par_draw['tau'].get()\n",
    "        log_tau = np.log(tau)\n",
    "\n",
    "        log_lik = 0.\n",
    "\n",
    "        # Log likelihood from data.\n",
    "        z = u[self.y_g_vec] + np.matmul(self.x_mat, beta)\n",
    "        log_lik += np.sum(self.y_vec * z - np.log1p(np.exp(z)))\n",
    "\n",
    "        # Log likelihood from random effect terms.\n",
    "        log_lik += -0.5 * tau * np.sum((mu - u) ** 2) + 0.5 * log_tau * len(u)\n",
    "\n",
    "        return log_lik\n",
    "\n",
    "    def get_log_posterior(self):\n",
    "        return np.squeeze(\n",
    "            self.get_log_lik() + \\\n",
    "            self.get_log_prior())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-696.14718055994535"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = logit_glmm.LogisticGLMM(\n",
    "    glmm_par, prior_par, x_mat, y_vec, y_g_vec, num_gh_points=vb_data['num_gh_points'])\n",
    "\n",
    "glmm_par_draw = vb.ModelParamsDict('GLMM Parameter Draw')\n",
    "glmm_par_draw.push_param(vb.ScalarParam('mu', val=0.0))\n",
    "glmm_par_draw.push_param(vb.ScalarParam('tau', val=1.0))\n",
    "glmm_par_draw.push_param(vb.VectorParam('beta', K, val=np.full(K, 0.)))\n",
    "glmm_par_draw.push_param(vb.VectorParam('u', NG))\n",
    "\n",
    "log_model = LogisticGLMMLogPosterior(glmm_par_draw, prior_par, x_mat, y_vec, y_g_vec)\n",
    "log_model.get_log_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 214)\n"
     ]
    }
   ],
   "source": [
    "class LogisticGLMMBootstrap(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.weights = np.full(model.x_mat.shape[0], 1.0)\n",
    "        self.num_gh_points = 5\n",
    "        self.model.set_gh_points(self.num_gh_points)\n",
    "        \n",
    "        self.get_weight_obs_jacobian = autograd.grad(\n",
    "            self.get_data_log_lik_term)\n",
    "        \n",
    "        self.obs = 0\n",
    "        \n",
    "    def get_data_log_lik_term(self, free_par, obs):\n",
    "        self.model.glmm_par.set_free(free_par)\n",
    "        return logit_glmm.get_data_log_lik_terms(\n",
    "                glmm_par = self.model.glmm_par,\n",
    "                x_mat = np.atleast_2d(self.model.x_mat[obs, :]),\n",
    "                y_vec = np.atleast_1d(self.model.y_vec[obs]),\n",
    "                y_g_vec = np.atleast_1d(self.model.y_g_vec[obs]),\n",
    "                gh_x = self.model.gh_x,\n",
    "                gh_w = self.model.gh_w)[0]\n",
    "    \n",
    "    def get_weight_jacobian_list(self, free_par):\n",
    "        weight_jacobian_list = []\n",
    "        #np.full((self.model.x_mat.shape[0], len(free_par)), float('nan'))\n",
    "        print('Running.')\n",
    "        for obs in range(self.model.x_mat.shape[0]):\n",
    "            if obs % 1000 == 0:\n",
    "                print('Obs {}'.format(obs))\n",
    "            weight_jacobian_list.append(self.get_weight_obs_jacobian(free_par, obs))\n",
    "        print('Done.')\n",
    "        return weight_jacobian_list\n",
    "            \n",
    "    def wrap_data_log_lik_terms(self, free_par):\n",
    "        self.model.glmm_par.set_free(free_par)\n",
    "        return self.model.get_data_log_lik_terms()\n",
    "\n",
    "    def optimize_with_weights(self, weights, init_par, preconditioner, gtol=1e-6, print_every=1):\n",
    "        self.model.use_weights = True\n",
    "        self.model.weights = copy.deepcopy(weights)\n",
    "        return self.model.tr_optimize_cond(\n",
    "            init_par, preconditioner=preconditioner,\n",
    "            num_gh_points=self.num_gh_points, gtol=gtol, print_every=print_every)   \n",
    "    \n",
    "    def get_model_weight_grad(self, free_par_vec, weights):\n",
    "        self.model.use_weights = True\n",
    "        self.model.weights = copy.deepcopy(weights)\n",
    "        return self.model.objective.fun_free_grad(free_par_vec)\n",
    "\n",
    "    \n",
    "glmm_bootstrap_object = LogisticGLMMBootstrap(model)\n",
    "\n",
    "glmm_par_free = vb_data['glmm_par_free']\n",
    "elbo_hess = vb_sparse.unpack_csr_matrix(vb_data['elbo_hess_packed'])\n",
    "moment_jac = vb_data['moment_jac']\n",
    "print(elbo_hess.shape)\n",
    "\n",
    "moment_wrapper = logit_glmm.MomentWrapper(glmm_par)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/sparse/linalg/dsolve/linsolve.py:102: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "elbo_hess = vb_sparse.unpack_csr_matrix(vb_data['elbo_hess_packed'])\n",
    "preconditioner = sp.sparse.diags([1 / np.sqrt(-1. * elbo_hess.diagonal())], [0])\n",
    "cond_init = sp.sparse.linalg.spsolve(preconditioner, glmm_par_free)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0\n",
      "Iter  0  value:  69.4079315229\n",
      "Iter  1  value:  69.407924802\n",
      "Iter  2  value:  69.4079246793\n",
      "Iter  3  value:  69.4079246793\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 69.407925\n",
      "         Iterations: 3\n",
      "         Function evaluations: 4\n",
      "         Gradient evaluations: 4\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "base_weights = np.full(NObs, 1.0)\n",
    "print(np.sum(base_weights))\n",
    "base_opt = glmm_bootstrap_object.optimize_with_weights(\n",
    "    preconditioner=preconditioner,\n",
    "    weights=base_weights, init_par=cond_init, gtol=1e-8)\n",
    "base_free_par = glmm_bootstrap_object.model.objective.uncondition_x(base_opt.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.0841829776763916\n"
     ]
    }
   ],
   "source": [
    "moment_jac_time = time.time()\n",
    "moment_jac = moment_wrapper.get_moment_jacobian(base_free_par)\n",
    "moment_jac_time = time.time() - moment_jac_time\n",
    "\n",
    "moment_jac_sp = sp.sparse.csr_matrix(moment_jac)\n",
    "\n",
    "print('Time: ', moment_jac_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 of 99\n",
      "Jacobian time:  0.9509596824645996\n"
     ]
    }
   ],
   "source": [
    "# Get a sparse Jacobian.\n",
    "# sparse_model = logit_glmm.SparseModelObjective(\n",
    "#     model.glmm_par, model.prior_par, model.x_mat, model.y_vec, model.y_g_vec,\n",
    "#     num_gh_points=model.num_gh_points, num_groups=1)\n",
    "\n",
    "# sparse_model.glmm_par.set_free(model.glmm_par.get_free())\n",
    "jac_time = time.time()\n",
    "weight_jacobian = model.get_sparse_weight_free_jacobian(base_free_par, print_every_n=200)\n",
    "jac_time = time.time() - jac_time\n",
    "\n",
    "print('Jacobian time: ', jac_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 of 99.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:14: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    }
   ],
   "source": [
    "hess_time = time.time()\n",
    "kl_hess = model.get_sparse_free_hessian(base_free_par, print_every_n=200)\n",
    "hess_time = time.time() - hess_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cholesky:\n",
      "Solve:\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: CholmodTypeConversionWarning: converting matrix of class csr_matrix to CSC format\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Too bad you can't pickle sparse cholesky decomposition.\n",
    "\n",
    "from scikits.sparse.cholmod import cholesky\n",
    "\n",
    "inverse_time = time.time()\n",
    "\n",
    "print('Cholesky:')\n",
    "kl_hess_chol = cholesky(kl_hess)\n",
    "\n",
    "print('Solve:')\n",
    "param_boot_mat = -1 * kl_hess_chol.solve_A(weight_jacobian.T)\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "inverse_time = time.time() - inverse_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total weight (there are 1000 observations): 999.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluated at a linear combination of the bootstrap draw and base weights.\n",
    "# Note that at the full bootstrap draw, the response is quite nonlinear.\n",
    "\n",
    "use_jackknife = True\n",
    "if use_jackknife:\n",
    "    boot_draw = copy.deepcopy(base_weights)\n",
    "    boot_draw[np.random.randint(NObs)] = 0.\n",
    "else:\n",
    "    boot_draw = np.random.multinomial(NObs, [1. / NObs] * NObs, size=1) - 1.0\n",
    "    boot_draw = 1.0 * boot_draw + base_weights\n",
    "\n",
    "print('Total weight (there are {} observations): {}'.format(NObs, np.sum(boot_draw)))\n",
    "\n",
    "lr_param_diff = param_boot_mat * (np.squeeze(boot_draw) - 1.0)\n",
    "\n",
    "# Optionally, try taking a Newton step using the Hessian.\n",
    "# model_weight_grad = glmm_bootstrap_object.get_model_weight_grad(\n",
    "#     base_free_par + lr_param_diff, boot_draw)\n",
    "# model_weight_grad_sp = sp.sparse.csc_matrix(np.expand_dims(model_weight_grad, axis=1))\n",
    "# boot_newton_step_sp = kl_hess_chol.solve_A(model_weight_grad_sp)\n",
    "# boot_newton_step = np.asarray(boot_newton_step_sp)\n",
    "# print(np.max(np.abs(boot_newton_step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/sparse/linalg/dsolve/linsolve.py:102: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  0  value:  69.4068131452\n",
      "Iter  1  value:  69.4068122618\n",
      "Iter  2  value:  69.4068122569\n",
      "Iter  3  value:  69.4068122569\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 69.406812\n",
      "         Iterations: 2\n",
      "         Function evaluations: 4\n",
      "         Gradient evaluations: 3\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "#preconditioner = sp.sparse.diags([1 / np.sqrt(kl_hess.diagonal())], [0])\n",
    "# cond_init = sp.sparse.linalg.spsolve(preconditioner, base_free_par + lr_param_diff)\n",
    "cond_init = sp.sparse.linalg.spsolve(preconditioner, base_free_par)\n",
    "\n",
    "glmm_par_opt_boot = glmm_bootstrap_object.optimize_with_weights(\n",
    "    preconditioner=preconditioner,\n",
    "    init_par=cond_init, weights=boot_draw, gtol=1e-8)\n",
    "boot_free_par = model.objective.uncondition_x(glmm_par_opt_boot.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "if False:\n",
    "    moment_vec = moment_wrapper.get_moment_vector_from_free(base_free_par)\n",
    "\n",
    "    num_t = 10\n",
    "    moment_vec_pert_list = []\n",
    "    t_list = np.linspace(0, 1, num_t)\n",
    "    for t in t_list:\n",
    "        moment_vec_pert_list.append(\n",
    "            moment_wrapper.get_moment_vector_from_free(base_free_par + t * lr_param_diff) -\n",
    "            moment_vec)\n",
    "\n",
    "    derivs = moment_jac_sp * lr_param_diff\n",
    "\n",
    "    # offset = 2000\n",
    "    # for ind in range(offset, offset + 50):\n",
    "    #     plt.plot(t_list, [moment_vec_pert_list[i][ind] for i in range(num_t)])\n",
    "\n",
    "    plt.figure()\n",
    "    offset = 50\n",
    "    for ind in range(offset, offset + 5):\n",
    "        plt.plot(t_list, [moment_vec_pert_list[i][ind] for i in range(num_t)], 'r+')\n",
    "        plt.plot(t_list, t_list * derivs[ind], 'k')\n",
    "\n",
    "    derivs[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True param difference: 0.00301796296367805\n",
      "True moment norm difference: 0.002713749238580099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-10, 10, -10, 10]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzQkSc0MDUlSM0NDktTM0JAkNfv/2UkXr0vVOq8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04e7ff4c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04ea18fdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The actual change in the moment vectors.\n",
    "moment_vec = moment_wrapper.get_moment_vector_from_free(base_free_par)\n",
    "moment_vec_boot = moment_wrapper.get_moment_vector_from_free(boot_free_par)\n",
    "true_moment_diff = moment_vec_boot - moment_vec\n",
    "print('True param difference: {}'.format(np.linalg.norm(base_free_par- boot_free_par)))\n",
    "print('True moment norm difference: {}'.format(np.linalg.norm(true_moment_diff)))\n",
    "\n",
    "# Use the linear approximation for the parameters, not the moments.\n",
    "moment_pred_from_params = \\\n",
    "    moment_wrapper.get_moment_vector_from_free(base_free_par + lr_param_diff)\n",
    "moment_pred_from_params_diff = moment_pred_from_params - moment_vec\n",
    "\n",
    "# The difference based on a linear approximation to the moments.\n",
    "# lr_moment_diff = \\\n",
    "#     moment_jac_sp * (param_boot_mat * (np.squeeze(boot_draw) - 1.0))\n",
    "moment_pred_from_moments_diff = moment_jac_sp * lr_param_diff\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.title('Linearizing the moments')\n",
    "plt.plot(true_moment_diff, moment_pred_from_moments_diff, 'r+', markersize=10)\n",
    "plt.plot(true_moment_diff, true_moment_diff, 'k.')\n",
    "#plt.axis([-10, 10, -10, 10])\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Linearizing the parameters')\n",
    "plt.plot(true_moment_diff, moment_pred_from_params_diff, 'bx', markersize=10)\n",
    "plt.plot(true_moment_diff, true_moment_diff, 'k.')\n",
    "#plt.axis([-10, 10, -10, 10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
