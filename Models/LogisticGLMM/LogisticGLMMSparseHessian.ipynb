{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "import LogisticGLMM_lib as logit_glmm\n",
    "from VariationalBayes.SparseObjectives import Objective, SparseObjective\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import autograd\n",
    "\n",
    "import copy\n",
    "from scipy import optimize\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['y', 'tau_prior_beta', 'y_group', 'K', 'mu_prior_info', 'tau_prior_alpha', 'NG', 'beta_prior_info', 'beta_prior_mean', 'N', 'x', 'mu_prior_mean'])\n",
      "0.324\n"
     ]
    }
   ],
   "source": [
    "# Load data saved by stan_results_to_json.R and run_stan.R in LRVBLogitGLMM.\n",
    "\n",
    "analysis_name = 'simulated_data_small'\n",
    "\n",
    "data_dir = os.path.join(os.environ['GIT_REPO_LOC'],\n",
    "                        'LinearResponseVariationalBayes.py/Models/LogisticGLMM/data')\n",
    "json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "\n",
    "json_file = open(json_filename, 'r')\n",
    "json_dat = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "stan_dat = json_dat['stan_dat']\n",
    "#vp_base = json_dat['vp_base']\n",
    "\n",
    "print(stan_dat.keys())\n",
    "K = stan_dat['K'][0]\n",
    "NObs = stan_dat['N'][0]\n",
    "NG = stan_dat['NG'][0]\n",
    "#N = NObs / NG\n",
    "y_g_vec = np.array(stan_dat['y_group'])\n",
    "y_vec = np.array(stan_dat['y'])\n",
    "x_mat = np.array(stan_dat['x'])\n",
    "\n",
    "glmm_par = logit_glmm.get_glmm_parameters(K=K, NG=NG)\n",
    "\n",
    "# Define a class to contain prior parameters.\n",
    "prior_par = logit_glmm.get_default_prior_params(K)\n",
    "prior_par['beta_prior_mean'].set(np.array(stan_dat['beta_prior_mean']))\n",
    "\n",
    "prior_par['beta_prior_info'].set(np.array(stan_dat['beta_prior_info']))\n",
    "\n",
    "prior_par['mu_prior_mean'].set(stan_dat['mu_prior_mean'][0])\n",
    "prior_par['mu_prior_info'].set(stan_dat['mu_prior_info'][0])\n",
    "\n",
    "prior_par['tau_prior_alpha'].set(stan_dat['tau_prior_alpha'][0])\n",
    "prior_par['tau_prior_beta'].set(stan_dat['tau_prior_beta'][0])\n",
    "\n",
    "# An index set to make sure jacobians match the order expected by R.\n",
    "prior_par_indices = copy.deepcopy(prior_par)\n",
    "prior_par_indices.set_name('Prior Indices')\n",
    "prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "\n",
    "free_par_vec = glmm_par.get_free()\n",
    "init_par_vec = copy.deepcopy(free_par_vec)\n",
    "\n",
    "print(np.mean(y_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_draws =  3\n",
      "\tFunction time: 0.0011065320984926074\n",
      "\tGrad time: 0.008107000699965283\n",
      "\tHessian vector product time: 0.01480242179823108\n",
      "\tPrior hess time:  0.06830835342407227\n"
     ]
    }
   ],
   "source": [
    "# Define moment parameters\n",
    "moment_wrapper = logit_glmm.MomentWrapper(glmm_par)\n",
    "get_moment_jacobian = autograd.jacobian(moment_wrapper.get_moment_vector)\n",
    "moment_jac = get_moment_jacobian(init_par_vec)\n",
    "\n",
    "\n",
    "model = logit_glmm.LogisticGLMM(glmm_par, prior_par, x_mat, y_vec, y_g_vec, num_gh_points=10)\n",
    "model.get_e_log_prior()\n",
    "model.get_log_lik()\n",
    "model.get_entropy()\n",
    "\n",
    "objective = Objective(model.glmm_par, model.get_kl)\n",
    "objective.fun_free(free_par_vec)\n",
    "\n",
    "# # PriorHess evaluates the second order derivative d2 EPrior / dpar dprior_par\n",
    "def get_e_log_prior(prior_vec, free_par):\n",
    "    model.glmm_par.set_free(free_par)\n",
    "    model.prior_par.set_vector(prior_vec)\n",
    "    return model.get_e_log_prior()\n",
    "\n",
    "get_prior_model_grad = autograd.grad(get_e_log_prior, argnum=0)\n",
    "get_prior_hess = autograd.jacobian(get_prior_model_grad, argnum=1)\n",
    "\n",
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "num_draws = 3\n",
    "model.set_gh_points(num_draws)\n",
    "print('num_draws = ', num_draws)\n",
    "print('\\tFunction time:',\n",
    "      timeit.timeit(lambda: objective.fun_free(free_par_vec), number=time_num) / time_num)\n",
    "\n",
    "print('\\tGrad time:', \n",
    "      timeit.timeit(lambda: objective.fun_free_grad(free_par_vec), number=time_num) / time_num)\n",
    "\n",
    "print('\\tHessian vector product time:',\n",
    "      timeit.timeit(lambda: objective.fun_free_hvp(free_par_vec, free_par_vec + 1), number=time_num) / time_num)\n",
    "\n",
    "prior_vec = model.prior_par.get_vector()\n",
    "prior_hess_time = time.time()\n",
    "get_prior_hess(prior_vec, init_par_vec)\n",
    "prior_hess_time = time.time() - prior_hess_time\n",
    "print('\\tPrior hess time: ', prior_hess_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local GLMM Parameters:\n",
      "\tbeta:\n",
      "beta_mean:\n",
      "[0 1 2 3 4]\n",
      "beta_info:\n",
      "[[  5.   6.   8.  11.  15.]\n",
      " [  6.   7.   9.  12.  16.]\n",
      " [  8.   9.  10.  13.  17.]\n",
      " [ 11.  12.  13.  14.  18.]\n",
      " [ 15.  16.  17.  18.  19.]]\n",
      "\tu:\n",
      "u_mean:\n",
      "[20]\n",
      "u_info:\n",
      "[21]\n"
     ]
    }
   ],
   "source": [
    "glmm_par.set_free(np.random.random(glmm_par.free_size()))\n",
    "glmm_indices = copy.deepcopy(glmm_par)\n",
    "glmm_indices.set_vector(np.arange(0, glmm_indices.vector_size()))\n",
    "\n",
    "def get_local_parameters(K, beta_diag_min=0., u_info_min=0.):\n",
    "    local_par = vb.ModelParamsDict('Local GLMM Parameters')\n",
    "    local_par.push_param(vb.MVNParam('beta', K, min_info=beta_diag_min))\n",
    "    local_par.push_param(vb.UVNParamVector('u', 1, min_info=u_info_min))\n",
    "    return local_par\n",
    "\n",
    "local_par = get_local_parameters(K)\n",
    "local_indices = get_local_parameters(K)\n",
    "local_indices.set_vector(np.arange(0, local_indices.vector_size()))\n",
    "print(local_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VariationalBayes.ExponentialFamilies as ef\n",
    "\n",
    "def get_global_entropy_args(info_mu, info_beta, tau_shape, tau_rate):\n",
    "    return \\\n",
    "        ef.univariate_normal_entropy(info_mu) + \\\n",
    "        ef.multivariate_normal_entropy(info_beta) + \\\n",
    "        ef.gamma_entropy(tau_shape, tau_rate)\n",
    "\n",
    "def get_local_entropy_args(info_u):\n",
    "    return ef.univariate_normal_entropy(info_u)\n",
    "\n",
    "def set_local_row(glmm_par, local_par):\n",
    "    local_par['beta'].set_vector(glmm_par['beta'].get_vector())\n",
    "\n",
    "    local_par['u'].mean.set(glmm_par['u'].mean.get()[row])\n",
    "    local_par['u'].info.set(glmm_par['u'].info.get()[row])\n",
    "\n",
    "\n",
    "class SparseModelObjective(object):\n",
    "    def __init__(self, glmm_par):\n",
    "        self.glmm_par = copy.deepcopy(glmm_par)\n",
    "        self.glmm_indices = copy.deepcopy(glmm_par)\n",
    "        self.glmm_indices.set_vector(np.arange(0, glmm_indices.vector_size()))\n",
    "\n",
    "        K = glmm_par['beta'].dim()\n",
    "        self.local_par = get_local_parameters(K)\n",
    "        self.local_indices = get_local_parameters(K)\n",
    "        self.local_indices.set_vector(np.arange(0, local_indices.vector_size()))\n",
    "        self.local_indices_vec = self.local_indices.get_vector()\n",
    "        \n",
    "    def get_global_entropy(self):\n",
    "        info_mu = self.glmm_par['mu'].info.get()\n",
    "        info_beta = self.glmm_par['beta'].info.get()\n",
    "        tau_shape = self.glmm_par['tau'].shape.get()\n",
    "        tau_rate = self.glmm_par['tau'].rate.get()\n",
    "        return \\\n",
    "            get_global_entropy_args(info_mu, info_beta, tau_shape, tau_rate)\n",
    "        \n",
    "    def get_full_local_entropy(self, glmm_vec):\n",
    "        self.glmm_par.set_vector(glmm_vec)\n",
    "        return get_local_entropy_args(self.glmm_par['u'].info.get())\n",
    "\n",
    "    def get_local_entropy(self, local_vec):\n",
    "        self.local_par.set_vector(local_vec)\n",
    "        return get_local_entropy_args(self.local_par['u'].info.get())\n",
    "    \n",
    "    def set_local_row(self, row):\n",
    "        set_local_row(self.glmm_par, self.local_par)\n",
    "        set_local_row(self.glmm_indices, self.local_indices)\n",
    "        \n",
    "\n",
    "sparse_model = SparseModelObjective(glmm_par)\n",
    "NG\n",
    "row = 3\n",
    "get_local_entropy_hess = autograd.hessian(sparse_model.get_local_entropy)\n",
    "get_full_local_entropy_hess = autograd.hessian(sparse_model.get_full_local_entropy)\n",
    "\n",
    "sparse_model.set_local_row(row)    \n",
    "#get_local_entropy_hess(sparse_model.local_par.get_vector())\n",
    "full_hess = get_full_local_entropy_hess(sparse_model.glmm_par.get_vector())\n",
    "\n",
    "# print(sparse_model.local_indices)\n",
    "# print('------------')\n",
    "# print(sparse_model.local_par['u'].mean.get())\n",
    "#print(sparse_model.local_par)\n",
    "\n",
    "# u_mean_index = sparse_model.local_indices['u'].mean.get()\n",
    "# print(sparse_model.glmm_par.get_vector()[u_mean_index])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (124, 124)\t0.0709064592585\n",
      "  (125, 125)\t0.30843128106\n",
      "  (126, 126)\t0.170395171435\n",
      "  (127, 127)\t0.0762675194972\n",
      "  (128, 128)\t0.0793623607453\n",
      "  (129, 129)\t0.233580961547\n",
      "  (130, 130)\t0.109760102242\n",
      "  (131, 131)\t0.238864040393\n",
      "  (132, 132)\t0.263165679478\n",
      "  (133, 133)\t0.128640724095\n",
      "  (134, 134)\t0.0826709790661\n",
      "  (135, 135)\t0.341343549446\n",
      "  (136, 136)\t0.457760551703\n",
      "  (137, 137)\t0.19545263501\n",
      "  (138, 138)\t0.167708023358\n",
      "  (139, 139)\t0.188791973066\n",
      "  (140, 140)\t0.155220492036\n",
      "  (141, 141)\t0.125624892418\n",
      "  (142, 142)\t0.189375479719\n",
      "  (143, 143)\t0.168896862176\n",
      "  (144, 144)\t0.0725012452164\n",
      "  (145, 145)\t0.393225646206\n",
      "  (146, 146)\t0.123275392336\n",
      "  (147, 147)\t0.472770313778\n",
      "  (148, 148)\t0.212410482812\n",
      "  :\t:\n",
      "  (199, 199)\t0.384092350634\n",
      "  (200, 200)\t0.110579407801\n",
      "  (201, 201)\t0.248077506781\n",
      "  (202, 202)\t0.0926683535621\n",
      "  (203, 203)\t0.324188337609\n",
      "  (204, 204)\t0.233475968546\n",
      "  (205, 205)\t0.285096667609\n",
      "  (206, 206)\t0.480392770738\n",
      "  (207, 207)\t0.187559260645\n",
      "  (208, 208)\t0.114189723575\n",
      "  (209, 209)\t0.0986227976029\n",
      "  (210, 210)\t0.199737045663\n",
      "  (211, 211)\t0.186428241531\n",
      "  (212, 212)\t0.282758285271\n",
      "  (213, 213)\t0.483501993276\n",
      "  (214, 214)\t0.268391419168\n",
      "  (215, 215)\t0.193947582966\n",
      "  (216, 216)\t0.120295048308\n",
      "  (217, 217)\t0.12148865772\n",
      "  (218, 218)\t0.466573004087\n",
      "  (219, 219)\t0.240349216042\n",
      "  (220, 220)\t0.0928649843131\n",
      "  (221, 221)\t0.0759724026814\n",
      "  (222, 222)\t0.209090699027\n",
      "  (223, 223)\t0.388958105151\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "hess_rows = [] # These will be the z indices\n",
    "hess_cols = [] # These will be the data indices\n",
    "\n",
    "obs_hess_dim = sparse_model.local_par.vector_size()\n",
    "full_hess_dim = sparse_model.glmm_par.vector_size()\n",
    "\n",
    "for row in range(NG):\n",
    "    sparse_model.set_local_row(row)    \n",
    "    full_indices = sparse_model.local_indices.get_vector()\n",
    "    row_hess_val = get_local_entropy_hess(sparse_model.local_par.get_vector())\n",
    "\n",
    "    for row in range(obs_hess_dim):\n",
    "        for col in range(obs_hess_dim):\n",
    "            if row_hess_val[row, col] != 0:\n",
    "                hess_vals.append(row_hess_val[row, col])\n",
    "                hess_rows.append(int(full_indices[row]))\n",
    "                hess_cols.append(int(full_indices[col]))\n",
    "\n",
    "\n",
    "sparse_hess = csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                         (full_hess_dim, full_hess_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(full_hess - sparse_hess))\n",
    "#plt.plot(full_hess[:], sparse_hess.todense()[:], 'k.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
