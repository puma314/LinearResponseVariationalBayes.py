{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from VariationalBayes import ScalarParam, ModelParamsDict, VectorParam, PosDefMatrixParam\n",
    "from VariationalBayes.NormalParams import MVNParam, UVNParam, UVNParamVector\n",
    "from VariationalBayes.GammaParams import GammaParam\n",
    "from VariationalBayes.ExponentialFamilies import \\\n",
    "    UnivariateNormalEntropy, MultivariateNormalEntropy, GammaEntropy, \\\n",
    "    MVNPrior, UVNPrior, GammaPrior\n",
    "\n",
    "\n",
    "from autograd import grad, hessian, jacobian, hessian_vector_product\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy as asp\n",
    "import scipy as sp\n",
    "\n",
    "import copy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'y_group', u'mu_prior_var', u'mu_prior_t', u'mu_prior_var_c', u'K', u'beta_prior_var', u'tau_prior_beta', u'N', u'mu_prior_mean_c', u'mu_prior_epsilon', u'mu_prior_mean', u'y', u'x', u'NG', u'beta_prior_mean', u'tau_prior_alpha']\n",
      "10000\n",
      "0.4668\n"
     ]
    }
   ],
   "source": [
    "# Load data saved by stan_results_to_json.R and run_stan.R in LRVBLogitGLMM.\n",
    "import os\n",
    "import json\n",
    "\n",
    "simulate_data = False\n",
    "prior_par = ModelParamsDict('Prior Parameters')\n",
    "\n",
    "if not simulate_data:\n",
    "    #analysis_name = 'simulated_data_small'\n",
    "    analysis_name = 'simulated_data_large'\n",
    "\n",
    "    data_dir = os.path.join(os.environ['GIT_REPO_LOC'], 'LRVBLogitGLMM/LogitGLMMLRVB/inst/data/')\n",
    "    json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "    json_output_filename = os.path.join(data_dir, '%s_python_vb_results.json' % analysis_name)\n",
    "\n",
    "    json_file = open(json_filename, 'r')\n",
    "    json_dat = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    stan_dat = json_dat['stan_dat']\n",
    "    vp_base = json_dat['vp_base']\n",
    "\n",
    "    print stan_dat.keys()\n",
    "    K = stan_dat['K'][0]\n",
    "    NObs = stan_dat['N'][0]\n",
    "    NG = stan_dat['NG'][0]\n",
    "    N = NObs / NG\n",
    "    y_g_vec = np.array(stan_dat['y_group'])\n",
    "    y_vec = np.array(stan_dat['y'])\n",
    "    x_mat = np.array(stan_dat['x'])\n",
    "    \n",
    "    # Define a class to contain prior parameters.\n",
    "    prior_par.push_param(VectorParam('beta_prior_mean', K, val=np.array(stan_dat['beta_prior_mean'])))\n",
    "    beta_prior_info = np.linalg.inv(np.array(stan_dat['beta_prior_var']))\n",
    "    prior_par.push_param(PosDefMatrixParam('beta_prior_info', K, val=beta_prior_info))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('mu_prior_mean', val=stan_dat['mu_prior_mean'][0]))\n",
    "    prior_par.push_param(ScalarParam('mu_prior_info', val=1 / stan_dat['mu_prior_var'][0]))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('tau_prior_alpha', val=stan_dat['tau_prior_alpha'][0]))\n",
    "    prior_par.push_param(ScalarParam('tau_prior_beta', val=stan_dat['tau_prior_beta'][0]))\n",
    "\n",
    "    # An index set to make sure jacobians match the order expected by R.\n",
    "    prior_par_indices = copy.deepcopy(prior_par)\n",
    "    prior_par_indices.set_name('Prior Indices')\n",
    "    prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "else:\n",
    "    # Simulate data instead of loading it if you like\n",
    "    N = 200     # observations per group\n",
    "    K = 5      # dimension of regressors\n",
    "    NG = 200      # number of groups\n",
    "\n",
    "    # Generate data\n",
    "    def Logistic(u):\n",
    "        return np.exp(u) / (1 + np.exp(u))\n",
    "\n",
    "    NObs = NG * N\n",
    "    true_beta = np.array(range(5))\n",
    "    true_beta = true_beta - np.mean(true_beta)\n",
    "    true_mu = 0.\n",
    "    true_tau = 40.0\n",
    "    true_u = np.random.normal(true_mu, 1 / np.sqrt(true_tau), NG)\n",
    "\n",
    "    x_mat = np.random.random(K * NObs).reshape(NObs, K) - 0.5\n",
    "    y_g_vec = [ g for g in range(NG) for n in range(N) ]\n",
    "    true_rho = Logistic(np.matmul(x_mat, true_beta) + true_u[y_g_vec])\n",
    "    y_vec = np.random.random(NObs) < true_rho\n",
    "    \n",
    "    prior_par.push_param(VectorParam('beta_prior_mean', K, val=np.zeros(K)))\n",
    "    prior_par.push_param(PosDefMatrixParam('beta_prior_info', K, val=0.01 * np.eye(K)))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('mu_prior_mean', val=0))\n",
    "    prior_par.push_param(ScalarParam('mu_prior_info', val=0.5))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('tau_prior_alpha', val=3.0))\n",
    "    prior_par.push_param(ScalarParam('tau_prior_beta', val=10.0))\n",
    "\n",
    "print N * NG\n",
    "print np.mean(y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build an object to contain a variational approximation to a K-dimensional multivariate normal.\n",
    "glmm_par = ModelParamsDict('GLMM Parameters')\n",
    "\n",
    "glmm_par.push_param(UVNParam('mu', min_info=vp_base['mu_info_min'][0]))\n",
    "glmm_par.push_param(GammaParam('tau',\n",
    "                               min_shape=vp_base['tau_alpha_min'][0],\n",
    "                               min_rate=vp_base['tau_beta_min'][0]))\n",
    "glmm_par.push_param(MVNParam('beta', K, min_info=vp_base['beta_diag_min'][0]))\n",
    "glmm_par.push_param(UVNParamVector('u', NG, min_info=vp_base['u_info_min'][0]))\n",
    "\n",
    "# Initialize with ADVI.  Don't forget to add the ADVI computation time to your final VB time!\n",
    "advi_init = False\n",
    "if advi_init:\n",
    "    advi_fit = json_dat['advi_results']\n",
    "    glmm_par['mu'].mean.set(advi_fit['mu_mean'][0])\n",
    "    glmm_par['mu'].info.set(1 / advi_fit['mu_var'][0])\n",
    "\n",
    "    tau_mean = advi_fit['tau_mean'][0]\n",
    "    tau_var = advi_fit['tau_var'][0]\n",
    "    glmm_par['tau'].shape.set((tau_mean ** 2) / tau_var)\n",
    "    glmm_par['tau'].rate.set(tau_var / tau_mean)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.array(advi_fit['beta_mean']))\n",
    "    glmm_par['beta'].info.set(np.array(advi_fit['beta_info']))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.array(advi_fit['u_mean']))\n",
    "    glmm_par['u'].info.set(1 / np.array(advi_fit['u_var']))\n",
    "\n",
    "    free_par_vec = glmm_par.get_free()\n",
    "else:\n",
    "    glmm_par['mu'].mean.set(0.0)\n",
    "    glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "    glmm_par['tau'].shape.set(2.0)\n",
    "    glmm_par['tau'].rate.set(2.0)\n",
    "\n",
    "    glmm_par['beta'].mean.set(np.full(K, 0.0))\n",
    "    glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "    glmm_par['u'].mean.set(np.full(NG, 0.0))\n",
    "    glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "free_par_vec = glmm_par.get_free()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define moment parameters\n",
    "\n",
    "moment_par = ModelParamsDict('Moment Parameters')\n",
    "moment_par.push_param(VectorParam('e_beta', K))\n",
    "# moment_par.push_param(PosDefMatrixParam('e_beta_outer', K))\n",
    "moment_par.push_param(ScalarParam('e_mu'))\n",
    "# moment_par.push_param(ScalarParam('e_mu2'))\n",
    "moment_par.push_param(ScalarParam('e_tau'))\n",
    "moment_par.push_param(ScalarParam('e_log_tau'))\n",
    "moment_par.push_param(VectorParam('e_u', NG))\n",
    "# moment_par.push_param(VectorParam('e_u2', NG))\n",
    "\n",
    "def set_moments(glmm_par, moment_par):\n",
    "    moment_par['e_beta'].set(glmm_par['beta'].e())\n",
    "    #moment_par['e_beta_outer'].set(glmm_par['beta'].e_outer())\n",
    "    moment_par['e_mu'].set(glmm_par['mu'].e())\n",
    "    #moment_par['e_mu2'].set(glmm_par['mu'].e_outer())\n",
    "    moment_par['e_tau'].set(glmm_par['tau'].e())\n",
    "    moment_par['e_log_tau'].set(glmm_par['tau'].e_log())\n",
    "    moment_par['e_u'].set(glmm_par['u'].e())\n",
    "    #moment_par['e_u2'].set((glmm_par['u'].e_outer()))\n",
    "    \n",
    "set_moments(glmm_par, moment_par)\n",
    "\n",
    "# Moment indices.\n",
    "moment_indices = copy.deepcopy(moment_par)\n",
    "moment_indices.set_vector(1 + np.array(range(moment_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It is better to do this by differentiating the gradient.\n",
    "\n",
    "# def decode_combined_parameters(combined_free_par_vec, glmm_par, prior_par):\n",
    "#     assert glmm_par.free_size() + prior_par.vector_size() == len(combined_free_par_vec) \n",
    "#     glmm_par.set_free(combined_free_par_vec[0:glmm_par.free_size()])\n",
    "#     prior_par.set_vector(combined_free_par_vec[glmm_par.free_size():])\n",
    "    \n",
    "# def encode_combined_parameters(glmm_par, prior_par):\n",
    "#     combined_free_par_vec = np.full(glmm_par.free_size() + prior_par.vector_size(), float('nan'))\n",
    "#     combined_free_par_vec[0:glmm_par.free_size()] = glmm_par.get_free()\n",
    "#     combined_free_par_vec[glmm_par.free_size():] = prior_par.get_vector()\n",
    "#     return combined_free_par_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14516.0995561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.6657256909229563"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ELogPrior(prior_par, glmm_par_elbo):\n",
    "    e_beta = glmm_par_elbo['beta'].mean.get()\n",
    "    info_beta = glmm_par_elbo['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "    beta_prior_info = prior_par['beta_prior_info'].get()\n",
    "    beta_prior_mean = prior_par['beta_prior_mean'].get()\n",
    "    e_log_p_beta = MVNPrior(beta_prior_mean, beta_prior_info, e_beta, cov_beta)\n",
    "    \n",
    "    e_mu = glmm_par_elbo['mu'].mean.get()\n",
    "    info_mu = glmm_par_elbo['mu'].info.get()\n",
    "    var_mu = 1 / info_mu\n",
    "    e_log_p_mu = UVNPrior(prior_par['mu_prior_mean'].get(), prior_par['mu_prior_info'].get(), e_mu, var_mu) \n",
    "\n",
    "    e_tau = glmm_par_elbo['tau'].e()\n",
    "    e_log_tau = glmm_par_elbo['tau'].e_log()\n",
    "    tau_prior_shape = prior_par['tau_prior_alpha'].get()\n",
    "    tau_prior_rate = prior_par['tau_prior_beta'].get()\n",
    "    e_log_p_tau = GammaPrior(tau_prior_shape, tau_prior_rate, e_tau, e_log_tau)\n",
    "    \n",
    "    return  e_log_p_beta + e_log_p_mu + e_log_p_tau\n",
    "           \n",
    "\n",
    "def DataLogLikelihood(x_mat, y_vec, e_beta, cov_beta, e_u, var_u, std_draws):\n",
    "    z_mean = e_u + np.matmul(x_mat, e_beta)\n",
    "    z_sd = np.sqrt(var_u + np.einsum('nk,kj,nj->n', x_mat, cov_beta, x_mat))\n",
    "    z = np.einsum('i,j->ij', z_sd, std_draws) + np.expand_dims(z_mean, 1)\n",
    "\n",
    "    # The sum is over observations and draws, so dividing by the draws size\n",
    "    # gives the sum of sample expectations over the draws.\n",
    "    # p = exp(z) / (1 + exp(z))\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_term = -np.sum(np.log1p(np.exp(z))) / std_draws.size\n",
    "    y_term = np.sum(y_vec * z_mean)\n",
    "    return y_term + logit_term\n",
    "\n",
    "\n",
    "def RandomEffectLogLikelihood(e_u, var_u, e_mu, var_mu, e_tau, e_log_tau):\n",
    "    return -0.5 * e_tau * np.sum(((e_mu - e_u) ** 2) + var_mu + var_u) + 0.5 * e_log_tau * len(e_u)\n",
    "\n",
    "    \n",
    "def Elbo(y_vec, x_mat, y_g_vec, glmm_par_elbo, std_draws, prior_par):\n",
    "    e_beta = glmm_par_elbo['beta'].mean.get()\n",
    "    info_beta = glmm_par_elbo['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "    \n",
    "    e_u = glmm_par_elbo['u'].mean.get()\n",
    "    info_u = glmm_par_elbo['u'].info.get()\n",
    "    var_u = 1 / info_u\n",
    "    \n",
    "    e_mu = glmm_par_elbo['mu'].mean.get()\n",
    "    info_mu = glmm_par_elbo['mu'].info.get()\n",
    "    var_mu = 1 / info_mu\n",
    "    \n",
    "    e_tau = glmm_par_elbo['tau'].e()\n",
    "    e_log_tau = glmm_par_elbo['tau'].e_log()\n",
    "        \n",
    "    ll = \\\n",
    "        DataLogLikelihood(x_mat, y_vec, e_beta, cov_beta,\n",
    "                          e_u[y_g_vec], var_u[y_g_vec], std_draws) + \\\n",
    "        RandomEffectLogLikelihood(e_u, var_u, e_mu, var_mu, e_tau, e_log_tau)\n",
    "    if np.isnan(ll):\n",
    "        return -np.inf\n",
    "\n",
    "    e_log_prior = ELogPrior(prior_par, glmm_par_elbo)\n",
    "    if np.isnan(e_log_prior):\n",
    "        return -np.inf\n",
    "    \n",
    "    tau_shape = glmm_par_elbo['tau'].shape.get()\n",
    "    tau_rate = glmm_par_elbo['tau'].rate.get()\n",
    "    entropy = \\\n",
    "        UnivariateNormalEntropy(info_mu) + \\\n",
    "        MultivariateNormalEntropy(info_beta) + \\\n",
    "        UnivariateNormalEntropy(info_u) + \\\n",
    "        GammaEntropy(tau_shape, tau_rate)\n",
    "\n",
    "    return ll[0] + e_log_prior[0] + entropy\n",
    "\n",
    "\n",
    "class KLWrapper(object):\n",
    "    def __init__(self, glmm_par, prior_par, x_mat, y_vec, y_g_vec, num_draws):\n",
    "        self.__glmm_par_ad = copy.deepcopy(glmm_par)\n",
    "        self.__prior_par_ad = copy.deepcopy(prior_par)\n",
    "        self.x_mat = x_mat\n",
    "        self.y_vec = y_vec\n",
    "        self.y_g_vec = y_g_vec\n",
    "        self.set_draws(num_draws)\n",
    "        \n",
    "    def set_draws(self, num_draws):\n",
    "        draw_spacing = 1 / float(num_draws + 1)\n",
    "        target_quantiles = np.linspace(draw_spacing, 1 - draw_spacing, num_draws)\n",
    "        self.std_draws = sp.stats.norm.ppf(target_quantiles)\n",
    "\n",
    "    def Eval(self, free_par_vec, verbose=False):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        kl = -Elbo(self.y_vec, self.x_mat, self.y_g_vec,\n",
    "                   self.__glmm_par_ad, self.std_draws, self.__prior_par_ad)\n",
    "        if verbose: print kl\n",
    "            \n",
    "        # TODO: this is returning an array when it should be a scalar.\n",
    "        return kl\n",
    "    \n",
    "    def ExpectedLogPrior(self, free_par_vec, prior_par_vec):\n",
    "        # Encode the glmm parameters first and the prior second.\n",
    "        decode_combined_parameters(combined_free_par_vec, self.__glmm_par_ad, self.__prior_par_ad)\n",
    "\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        self.__prior_par_ad.set_vector(prior_par_vec)\n",
    "        e_log_prior = ELogPrior(self.__prior_par_ad, self.__glmm_par_ad)\n",
    "        return e_log_prior[0]\n",
    "        \n",
    "        \n",
    "class MomentWrapper(object):\n",
    "    def __init__(self, glmm_par, moment_par):\n",
    "        self.__glmm_par_ad = copy.deepcopy(glmm_par)\n",
    "        self.__moment_par = copy.deepcopy(moment_par)\n",
    "\n",
    "    # Return a posterior moment of interest as a function of unconstrained parameters.\n",
    "    def GetMoments(self, free_par_vec):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        set_moments(self.__glmm_par_ad, self.__moment_par)\n",
    "        return self.__moment_par.get_vector()\n",
    "    \n",
    "    def GetMomentParameters(self, free_par_vec):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        set_moments(self.__glmm_par_ad, self.__moment_par)\n",
    "        return self.__moment_par\n",
    "\n",
    "\n",
    "kl_wrapper = KLWrapper(glmm_par, prior_par, x_mat, y_vec, y_g_vec, 10)\n",
    "KLGrad = grad(kl_wrapper.Eval)\n",
    "KLHess = hessian(kl_wrapper.Eval)\n",
    "KLHessVecProd = hessian_vector_product(kl_wrapper.Eval)  \n",
    "print kl_wrapper.Eval(free_par_vec)\n",
    "\n",
    "moment_wrapper = MomentWrapper(glmm_par, moment_par)\n",
    "MomentJacobian = jacobian(moment_wrapper.GetMoments)\n",
    "\n",
    "combined_free_par_vec = encode_combined_parameters(glmm_par, prior_par)\n",
    "\n",
    "# PriorHess evaluates the second order derivative d2 EPrior / dpar dprior_par\n",
    "PriorModelGrad = grad(kl_wrapper.ExpectedLogPrior, argnum=0)\n",
    "PriorHess = jacobian(PriorModelGrad, argnum=1)\n",
    "\n",
    "kl_wrapper.ExpectedLogPrior(free_par_vec, prior_par.get_vector())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function time:\n",
      "0.0253336906433\n",
      "Grad time:\n",
      "0.0538583993912\n",
      "Hessian vector product time:\n",
      "0.114082121849\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "print 'Function time:'\n",
    "print timeit.timeit(lambda: kl_wrapper.Eval(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Grad time:'\n",
    "print timeit.timeit(lambda: KLGrad(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Hessian vector product time:'\n",
    "print timeit.timeit(lambda: KLHessVecProd(free_par_vec, free_par_vec + 1), number=time_num) / time_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "def tr_optimize(trust_init, num_draws):\n",
    "    kl_wrapper.set_draws(num_draws)\n",
    "    vb_opt = optimize.minimize(\n",
    "        lambda par: kl_wrapper.Eval(par, verbose=True),\n",
    "        trust_init, method='trust-ncg', jac=KLGrad, hessp=KLHessVecProd,\n",
    "        tol=1e-6, options={'maxiter': 100, 'disp': True, 'gtol': 1e-6 })\n",
    "    return vb_opt.x\n",
    "\n",
    "def get_moment_vec(vb_opt_x):\n",
    "    glmm_par_opt.set_free(vb_opt_x)\n",
    "    set_moments(glmm_par_opt, moment_par)\n",
    "    return moment_par.get_vector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region\n",
      "16320.689584\n",
      "9038.30934811\n",
      "5009.20455721\n",
      "3813.34808885\n",
      "3003.48432409\n",
      "2392.18151882\n",
      "2308.02385314\n",
      "1937.79661974\n",
      "1914.25804238\n",
      "1766.0002339\n",
      "1763.67259786\n",
      "1694.19685044\n",
      "1693.44038432\n",
      "1688.44471877\n",
      "1687.5955049\n",
      "1686.48239351\n",
      "1686.40448757\n",
      "1686.32247727\n",
      "1686.17676412\n",
      "1686.17419633\n",
      "1686.16570804\n",
      "1686.16524403\n",
      "1686.16519242\n",
      "1686.16519235\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1686.165192\n",
      "         Iterations: 23\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n",
      "         Hessian evaluations: 0\n",
      "Done.\n",
      "0.937956269582\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "init_par_vec = copy.deepcopy(free_par_vec)\n",
    "\n",
    "# Optimize.\n",
    "num_mc_draws = 50\n",
    "\n",
    "print 'Running Newton Trust Region'\n",
    "vb_time = time.time()\n",
    "opt_x = tr_optimize(init_par_vec, num_mc_draws)\n",
    "vb_time = time.time() - vb_time\n",
    "\n",
    "print 'Done.'\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(opt_x)\n",
    "set_moments(glmm_par_opt, moment_par)\n",
    "\n",
    "print vb_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Investigate the performance of different numbers of draws.   It doesn't appear to\n",
    "    # converge.\n",
    "    opt_x_20 = tr_optimize(init_par_vec, 20)\n",
    "    opt_x_60 = tr_optimize(opt_x_20, 60)\n",
    "    opt_x_100 = tr_optimize(opt_x_60, 100)\n",
    "    opt_x_200 = tr_optimize(opt_x_100, 200)\n",
    "    opt_x_400 = tr_optimize(opt_x_200, 400)\n",
    "    opt_x_800 = tr_optimize(opt_x_400, 800)\n",
    "    \n",
    "    mom_20 = get_moment_vec(opt_x_20)\n",
    "    mom_60 = get_moment_vec(opt_x_60)\n",
    "    mom_100 = get_moment_vec(opt_x_100)\n",
    "    mom_200 = get_moment_vec(opt_x_200)\n",
    "    mom_400 = get_moment_vec(opt_x_400)\n",
    "    mom_800 = get_moment_vec(opt_x_800)\n",
    "\n",
    "    print np.max(np.abs((mom_20 - mom_60) / mom_20))\n",
    "    print np.max(np.abs((mom_60 - mom_100) / mom_60))\n",
    "    print np.max(np.abs((mom_100 - mom_200) / mom_100))\n",
    "    print np.max(np.abs((mom_200 - mom_400) / mom_200))\n",
    "    print np.max(np.abs((mom_400 - mom_800) / mom_400))\n",
    "\n",
    "    print '-------\\n'\n",
    "    print np.max(np.abs((mom_20 - mom_60)))\n",
    "    print np.max(np.abs((mom_60 - mom_100)))\n",
    "    print np.max(np.abs((mom_100 - mom_200)))\n",
    "    print np.max(np.abs((mom_200 - mom_400)))\n",
    "    print np.max(np.abs((mom_400 - mom_800)))\n",
    "\n",
    "    #diff_inds = np.where(np.abs(mom_60 - mom_100) > 1e-2)\n",
    "    #print diff_inds\n",
    "    #print moment_indices\n",
    "\n",
    "    #print (get_moment_vec(opt_x_60) - get_moment_vec(opt_x_100)) / np.abs(get_moment_vec(opt_x_100))\n",
    "    get_moment_vec(opt_x_200)\n",
    "    u200 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_400)\n",
    "    u400 = copy.deepcopy(moment_par['e_u'].get())\n",
    "    get_moment_vec(opt_x_800)\n",
    "    u800 = copy.deepcopy(moment_par['e_u'].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23864.4423785\n",
      "-23881.1136719\n",
      "0.00840905862156\n",
      "(array([   1,    3,    6, ..., 9994, 9998, 9999]),)\n",
      "0.462969813322\n",
      "0.469542304929\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8lfXd//HXhxBI2CMB2UP2DHAAEVtHraIVcVWh2mqr\noq22Ve8Ovb1/etfOu9pat6WKWFsRR0Wq1tFatwgBwl4BGQmbkITs9fn9kaONCOQAIdcZ7+fjkUdy\nrnHOO+eh71xc1/d8L3N3REQkcTQJOoCIiDQuFb+ISIJR8YuIJBgVv4hIglHxi4gkGBW/iEiCUfGL\niCQYFb+ISIJR8YuIJJimQQc4mLS0NO/du3fQMUREYsaiRYv2uHt6JNtGZfH37t2bzMzMoGOIiMQM\nM9sc6bY61SMikmBU/CIiCUbFLyKSYFT8IiIJRsUvIpJgVPwiIglGxS8ikmBU/CIiUWD+xr3MfP8T\namqO/+1wVfwiIgErKKnk5jlZPDV/M+VVNcf99aLyk7siIonC3fnvF5eze385f/veyaQ2Szrur1nv\nEb+ZzTSzXWa24hDrf2xmWeGvFWZWbWYdwus2mdny8DrNwSAicoAXFufyyvLt3HLWAEZ0b9corxnJ\nqZ5ZwKRDrXT3u909w90zgNuAd9w9r84mp4fXh44tqohIfNm0p5g7X1rB+D4duO7LJzba69Zb/O7+\nLpBX33Zh04DZx5RIRCQBVFbXcNOcLJKaGPdelkFSE2u0126wi7tm1oLafxm8UGexA2+Y2SIzm17P\n/tPNLNPMMnfv3t1QsUREotID/1pP1tZ8fn3RCLq2S23U127IUT2TgQ8OOM1ziruPBs4BbjCzLx9q\nZ3ef4e4hdw+lp0c0pbSISExauCmPB/+dzSVjuvO1EV0a/fUbsvincsBpHnfPDX/fBbwIjGvA1xMR\niTkFpZXc9EwW3du34H/PHxpIhgYpfjNrC5wKvFRnWUsza/3pz8BZwEFHBomIJIo7XlrBjsIy7pua\nQavmwYyor/dVzWw2cBqQZmY5wJ1AMoC7Pxre7ELgDXcvrrNrZ+BFM/v0dZ5299caLrqISGyZuySX\nl7K28V9fHcConu0Dy1Fv8bv7tAi2mUXtsM+6yzYCI482mIhIPNmaV8L/m7uCsb3b873T+wWaRVM2\niIgcZ1XhoZsAv7+0cYduHoymbBAROc4e+vcGFm3ex31TM+jRoUXQcXTELyJyPC3avI/731rPhaO6\nMSWjW9BxABW/iMhxs7+skpvmLKFL2xR+NiWYoZsHo1M9IiLHgbtzx0sryd1XyrPXTaBNSnLQkT6j\nI34RkePghcW5vLgkl5vOHECod4eg43yOil9EpIFl7yri/81dwUl9O3BDwEM3D0bFLyLSgMoqq7nx\n6cWkNkvivqmjAh+6eTA6xy8i0oB+9epq1uzYzxNXjaVzm5Sg4xyUjvhFRBrIayu28+ePNnPtl/pw\n+qBOQcc5JBW/iEgDyNlXwk+eX8aI7m358dmDgo5zWCp+EZFjVFldww9mL6HG4YFpo2jWNLqrVef4\nRUSO0b1vrmPxlnwemDaKXh1bBh2nXtH9Z0lEJMq9t343j7yzgaljezB5ZNeg40RExS8icpR27y/n\n5jlLOTG9FXdOjp4pGeqjUz0iIkehpsa55dks9pdV8tdrxpPaLCnoSBHTEb+IyFH447sbeW/9Hu6c\nPJSBJ7QOOs4RUfGLiByhRZv3cc8ba/na8C5MG9cj6DhHTMUvInIECkoq+cHs2qmWf3XRcML3FY8p\n9Ra/mc00s11mtuIQ608zswIzywp/3VFn3SQzW2tm2WZ2a0MGFxFpbO7OrX9bxs7CMh6YNoq2qdEz\n1fKRiOSIfxYwqZ5t3nP3jPDXXQBmlgQ8BJwDDAGmmdmQYwkrIhKkJz/cxD9W7OBHZw9kVM/2Qcc5\navUWv7u/C+QdxXOPA7LdfaO7VwDPAFOO4nlERAK3ZMs+fvnqar4yqBPTv9Q36DjHpKHO8U8ws6Vm\n9g8z+3Qwazdga51tcsLLRERiyr7iCm58egmdWqfwu0tH0iQKp1o+Eg0xjn8x0Mvdi8zsXGAu0P9I\nn8TMpgPTAXr27NkAsUREjt2n4/V37y/nuesn0K5Fs6AjHbNjPuJ390J3Lwr//CqQbGZpQC5Qd5xT\n9/CyQz3PDHcPuXsoPT39WGOJiDSIR97ZwL/X7uZ/zhvMyB7tgo7TII65+M3sBAuPZzKzceHn3Ass\nBPqbWR8zawZMBeYd6+uJiDSWjzbs5XdvrGXyyK5886ReQcdpMPWe6jGz2cBpQJqZ5QB3AskA7v4o\ncAnwXTOrAkqBqe7uQJWZ3Qi8DiQBM9195XH5LUREGtiu/WV8f/YSeqe15NcxOl7/UOotfnefVs/6\nB4EHD7HuVeDVo4smIhKMqvD8+kXltfPwtGoeX9OaxddvIyLSAO795zrmb8zjnq+PjLl5eCKhKRtE\nROr495pdPPTvDVwW6sElY7oHHee4UPGLiITl7Cvh5mezGNylDT+bEjvz6x8pFb+ICFBRVcMNTy+h\nutp5+PLRpCTHzvz6R0rn+EVEgF+9upqlW/N59IrR9EmL/vvmHgsd8YtIwpu3dBuzPtzEdyb2YdKw\nLkHHOe5U/CKS0NbsKOSnzy9jbO/23HrOoKDjNAoVv4gkrILSSq57ahGtU5ry0DdG06xpYlSizvGL\nSEKqqXFunpPFtvxSnpl+Ep3apAQdqdEkxp83EZED3P/Wet5as4s7zhvCmF4dgo7TqFT8IpJw/rV6\nJ3/453ouHt2dK+Jo8rVIqfhFJKFs2lPMTXOyGNq1Db+8cFhcTb4WKRW/iCSMkooqrv/LIpKaGI9e\nMSauP6R1OCp+EUkI7s6tLyxn3c79PDBtFD06tAg6UmBU/CKSEGZ+sIl5S7fxo7MH8qX+iX2XPxW/\niMS9+Rv38qtXV3P20M5899QTg44TOBW/iMS13PxSbvjrYnp3bME9Xx+ZkBdzD6TiF5G4VVpRzbVP\nZlJRXcOMb4VonZIcdKSooE/uikhccnd+/PxSVu8oZOZVYzkxvVXQkaJGvUf8ZjbTzHaZ2YpDrL/c\nzJaZ2XIz+9DMRtZZtym8PMvMMhsyuIjI4Tz89gZeXradn04axOkDOwUdJ6pEcqpnFjDpMOs/AU51\n9+HAz4EZB6w/3d0z3D10dBFFRI7MP1ft5J431nJBRleu+3LfoONEnXpP9bj7u2bW+zDrP6zzcD4Q\nnzepFJGYsH7nfm6ak8Wwrm35zcUjdDH3IBr64u7VwD/qPHbgDTNbZGbTG/i1REQ+p6Ckkmv/nElK\nchIzvpW4n8ytT4Nd3DWz06kt/lPqLD7F3XPNrBPwppmtcfd3D7H/dGA6QM+ePRsqlogkiKrqGm6c\nvZht+WXMnj6eLm1Tg44UtRrkiN/MRgCPAVPcfe+ny909N/x9F/AiMO5Qz+HuM9w95O6h9PTE/lSd\niBy5f876ORvWr+EXFwxLuGmWj9QxF7+Z9QT+BnzT3dfVWd7SzFp/+jNwFnDQkUEiIsci84V7mbT1\n9/y21wIuHdsj6DhRr95TPWY2GzgNSDOzHOBOIBnA3R8F7gA6Ag+HL6JUhUfwdAZeDC9rCjzt7q8d\nh99BRBLY6vfnkrHsLpalhBh/zb1Bx4kJkYzqmVbP+muAaw6yfCMw8ot7iIg0jNy1i+n+z+vZktSD\nXt99juTkZkFHigmaskFEYlLh7lySnrmMMprT7FvP0badzutHSsUvIjGnsqyIXTMupG1NPjvPnUX3\n3gODjhRTVPwiElO8ppq1j1xO34p1LB57N8PGnR50pJij4heRmLL8yf9iWMHbvN3r+0w876qg48Qk\nFb+IxIw1r9zPiM1P8E6byZx25c+CjhOzVPwiEhO2LHyZfgvuZFHyaMZ+7zGaJKm+jpbeORGJens/\nyaLDK9fySZPudL/2WVqkpAQdKabpRiwiEtWK87ZR9dTXqfFmVE+dQ+dOmtLlWOmIX0SiVlVZETsf\nvZA21flsOvtxBg0aEnSkuKDiF5Go5DXVrHnkcnqXr2XBmN8y9uQzg44UN1T8IhKVljxxy2fDNk89\n/9tBx4krKn4RiTpZc//A6K2zeL/t+Rq2eRyo+EUkqqx+fy7DlvyMrOZjGHuDhm0eD3pHRSRqbFmT\n+dlsm32uf57mzZoHHSkuqfhFJCrs2bGF5GemUk5zUq58nrbtNdvm8aLiF5HA7c/fQ+GfzqeNF5I/\n5Sm69hoQdKS4puIXkUCVlxaR8/AFdK/awvrTH6HfqC8HHSnuqfhFJDDVVZWsfvBSBpavYGno12Sc\ndnHQkRKCil9EAuE1NSx5+Coyij9g/sAfM3bydUFHShgqfhEJRObMWwjlvcwHXb/Nyd+4Peg4CSWi\n4jezmWa2y8xWHGK9mdn9ZpZtZsvMbHSddVea2frw15UNFVxEYlfmM79gbM4TfNRuMidf8/ug4ySc\nSI/4ZwGTDrP+HKB/+Gs68AiAmXUA7gTGA+OAO82s/dGGFZHYt/SVGYTW3E1mi1MI3fAE1kQnHhpb\nRO+4u78L5B1mkynAn73WfKCdmXUBzgbedPc8d98HvMnh/4CISBxb/e4LDFlwK8uTRzD0xmdJTk4O\nOlJCaqg/td2ArXUe54SXHWr5F5jZdDPLNLPM3bt3N1AsEYkW6zP/Re9/Xc+mpF70+N5cUlu0DDpS\nwoqaf2O5+wx3D7l7KD1dN1oQiScbV2XS6eVvsqdJR9pe8xLt2ncMOlJCa6jizwV61HncPbzsUMtF\nJEFs3biGFs9eSiXJNPnmXDp17Rl0pITXUMU/D/hWeHTPSUCBu28HXgfOMrP24Yu6Z4WXiUgC2JGz\nkSZPnU8q5RRf+izd+g4KOpIQ4T13zWw2cBqQZmY51I7USQZw90eBV4FzgWygBPh2eF2emf0cWBh+\nqrvc/XAXiUUkTuzZsZWKx8+jQ00hOy98lhOHjA86koRFVPzuPq2e9Q7ccIh1M4GZRx5NRGJV4d5d\n7J9xHp1r9rDl3KcYlKH5d6JJ1FzcFZH4UFSQx85HzqVrdS4bvjKDQePPDjqSHEDFLyINprSokJyH\nzqN35UZWnPIgw798QdCR5CBU/CLSIMpKi9nwwPn0L1/F4rF3M+arU4OOJIeg4heRY1ZWVsqa+y5k\nSFkWmaN+yfjzrg46khyGil9Ejkl5eRkr77uYjLKPWTz8fxh/wUHHeUgUUfGLyFGrKC9n5X0XM6b0\nAzIH30rokh8FHUkioOIXkaNSWVHOivsuYnTJ+ywc9FNCl90WdCSJkIpfRI5Y3dL/eOBPGDv1v4OO\nJEdAxS8iR6SyopyV91/MqOL3mT/gJ4yfprtnxZqIPrkrIgLhc/r3X8So4vf5aMCPmaBbJsYkFb+I\nRKSstIRVD1zC6JIP+HjgT5igI/2YpeIXkXqVFhex7oELGF22kMzBtzJeF3JjmopfRA6reH8BGx84\nn+HlS8kc+TNCF90UdCQ5Rip+ETmkwoI8ch/8GkMqVrMk9GtCk78bdCRpACp+ETmogr072fnIefSr\n3MCyCfcyZtK3g44kDUTFLyJfsGfHVgr+NJneVVtZ9aWHGHXmYW/JITFGxS8in7Nt01qqn7yALjV7\nWXvGnxh56kVBR5IGpuIXkc98smYJLZ65mFaUsuVrTzN83JlBR5LjQMUvIgCsWfwuneZ9AyeJvZe8\nyKBhJwUdSY6TiKZsMLNJZrbWzLLN7NaDrL/XzLLCX+vMLL/Ouuo66+Y1ZHgRaRjL3n+F7i9dSrml\nUP7NV+ij0o9r9R7xm1kS8BDwVSAHWGhm89x91afbuPvNdbb/PjCqzlOUuntGw0UWkYa0+LU/M/Sj\nW9iZ1JkWV/+dtG59g44kx1kkR/zjgGx33+juFcAzwJTDbD8NmN0Q4UTk+Fr43D2M/OgHbGp2Iu1u\neEulnyAiKf5uwNY6j3PCy77AzHoBfYC36ixOMbNMM5tvZrrzskgU8JoaPnz8x4xd+XOWp46jxw/f\noE3HzkHHkkbS0Bd3pwLPu3t1nWW93D3XzPoCb5nZcnffcOCOZjYdmA7Qs2fPBo4lIp+qrKwg8+Gr\nOXnfPBa2O4eM7z1JcrPmQceSRhTJEX8u0KPO4+7hZQczlQNO87h7bvj7RuBtPn/+v+52M9w95O6h\n9PT0CGKJyJEq2l/Ait9NZsK+eSzodiWhHzyt0k9AkRT/QqC/mfUxs2bUlvsXRueY2SCgPfBRnWXt\nzax5+Oc0YCKw6sB9ReT42719C9v+8BVGlH7MoqG3M+7a+7EmuhdTIqr3VI+7V5nZjcDrQBIw091X\nmtldQKa7f/pHYCrwjLt7nd0HA380sxpq/8j8pu5oIBFpHJvXLiH5mUvpXlPAylMfYcwZmoIhkdnn\nezo6hEIhz8zMDDqGSFxY9eGrdHvjGqpoyr4pf6HfqC8HHUmOAzNb5O6hSLbVv/NE4tjHLz5Iv9ev\nIL9Je8qvfF2lL4CmbBCJS9XV1cx//BYmbpvFipRR9Lj+edq2Tws6lkQJFb9InCkuKmTVI1cwsfgd\nMjtMJuP6x2mqkTtSh4pfJI5s37qBollfZ0zVRjIH3kJo2h1gFnQsiTIqfpE4sTbzLTq+/G26eDkr\nT/sjodMvCzqSRCkVv0gcWDD3YUYuuYO9TTpQfNkLDB8c0eAOSVAqfpEYVllZwcIZN3Ly7jmsbD6C\nrtc+S9f0LkHHkiin4heJUXt2bWfbY1M5uSKLBZ0uZfQ1D+oirkRExS8Sg9YueY82L32bQb6PJaN/\nwbgp3w86ksQQFb9IjFnwtwcYsfRnFDRpS84Ff2NUxqlBR5IYo+IXiRFlpcUsfex7jN87l5UpGXS7\nZjad07sGHUtikIpfJAbkbFxD6V8vZ3x1NvO7fJPQd35P0+RmQceSGKXiF4lymW88Tf8PfkQbc5ae\n8jAnnXl50JEkxqn4RaJUeXkZmY/fzMRdT7MxuS+pl/+VkX2GBB1L4oCKXyQKbdu0lsK/fIuJVWtY\nmH4RI69+iGYpLYKOJXFCxS8SZRa/Not+8/+bNtSQNf5exp7znaAjSZxR8YtEieKiQpY9fgMT9s1j\nXdMBtPzGLDL6Dg06lsQhFb9IFFib9QHNX5rOBM9hQddvMuqqe0hulhJ0LIlTKn6RAFVXVzP/6Z8z\nNvsBCqw1q898knGnXBB0LIlzEd160cwmmdlaM8s2s1sPsv4qM9ttZlnhr2vqrLvSzNaHv65syPAi\nsWzH1mxW/98ZTNxwL6tbjaf5jfMZrNKXRlDvEb+ZJQEPAV8FcoCFZjbP3VcdsOkcd7/xgH07AHcC\nIcCBReF99zVIepEY5DU1LJj3RwZn3UVfryZz5P8y5oIfYk10C2xpHJGc6hkHZLv7RgAzewaYAhxY\n/AdzNvCmu+eF930TmATMPrq4IrFt946tbPnz9YwveZ81yUNoPe0xQrqAK40skkOMbsDWOo9zwssO\ndLGZLTOz582sxxHuKxLX3J0FrzxB0qMnM7x4Pgv6/ZABP32Pbip9CUBDXdz9OzDb3cvN7DrgSeCM\nI3kCM5sOTAfo2bNnA8USCd7eXbl88uT3GFf8Nhua9qP4kj8ybpDukCXBieSIPxfoUedx9/Cyz7j7\nXncvDz98DBgT6b51nmOGu4fcPZSenh5JdpGo5jU1LPz7DJo8fBIjit7n4z430PunH9FDpS8Bi+SI\nfyHQ38z6UFvaU4Fv1N3AzLq4+/bww/OB1eGfXwd+ZWbtw4/PAm475tQiUW7b5mx2zr6BsWXzWdd0\nAAUXPcz4IWODjiUCRFD87l5lZjdSW+JJwEx3X2lmdwGZ7j4P+IGZnQ9UAXnAVeF988zs59T+8QC4\n69MLvSLxqKqyko+f/S0Z6+6nHc6CQT9mzNdvJampPjIj0cPcPegMXxAKhTwzMzPoGCJHZP3SD6mZ\n90MGVq9jRWqI9KkP0bnXoKBjSYIws0XuHtF5RB2GiByj/QV5rHz6NsbumEOBtWbp2LsZcc41Gpcv\nUUvFL3KUvKaGBa88Tp9Fv+Ik8vg4bQqDrvgdI9trcIJENxW/yFHYsGIhJfP+i/EVS9nQ9EQKz32c\n8aOPaASzSGBU/CJHoCBvD6tm38bYXc9TYqksGvo/jLrwZpro4q3EEP3XKhKBqspKFs29jwEr72O8\n72dR2vkMnPZbxqSdEHQ0kSOm4hc5DHdn6Tsv0ubd/2V8zWZWNRvGvvN+y9gRE4OOJnLUVPwih5C9\nYgH7X76dUWUL2G6dyDrpXkaedZVG60jMU/GLHGBX7id88tzthPa9SomlsrD/TWRc8lO6NNfNziU+\nqPhFwvbt3cXq5+9i1LY5jKKazBMuY/BldzG2Q+ego4k0KBW/JLz9hfksfeG3DN80i5MoYXHbM+ly\nwS8Y31efupX4pOKXhFVaXMTiF3/PoOzHOIUClrc8iTbn3UVoyPigo4kcVyp+SThlpcVkzb2PE9fO\nYCL7WJWSQf5X72T4GH0ASxKDil8SRlFRIUvn/oEB2TM5iX2sajacvac/zJAJ5wYdTaRRqfgl7hXs\ny2P5S79j8KanmEgBq5qPZM+pDzJkwtfALOh4Io1OxS9xa+/OHNbOu4dhOXM4xUpYkRoi74yfMmTs\nWUFHEwmUil/izqZ1y9jx2j2M2vsqJ1HFstan0PasWxk24pSgo4lEBRW/xAWvqWH5/DeofO9+RpV8\nSFeSWNrxHDpP+hEZAzKCjicSVVT8EtMqysvIeu0J2i17jBHV2eTTisye36b/eTcztnPPoOOJRCUV\nv8Sk7Tmf8MlrDzIg53nGkc+WJt3JHPY/DDvnOsa1bBN0PJGopuKXmFFTXcPyD1+lfP6fGFX0Hl2s\nmmWp49g27jqGn3ohPZskBR1RJCZEVPxmNgm4D0gCHnP33xyw/hbgGqAK2A18x903h9dVA8vDm25x\n9/MbKLskiIK9O1n12p/okj2bkZ5DIS1Z2uXrdDvrB4zoOzToeCIxp97iN7Mk4CHgq0AOsNDM5rn7\nqjqbLQFC7l5iZt8FfgtcFl5X6u66uiZHpLq6mhUfvkLFgicYUfgeE6yS9U0Hsnj4Lxl61pWEUlsH\nHVEkZkVyxD8OyHb3jQBm9gwwBfis+N3933W2nw9c0ZAhJXHkbFzD5rceo3fOPEayk0JaktVpCumn\nTqf/MM2hI9IQIin+bsDWOo9zgMP9H3g18I86j1PMLJPa00C/cfe5B9vJzKYD0wF69tRojERSWJDH\nmrf+QovVzzGsYhld3VidmsHu4T9i8BnfYHxqq6AjisSVBr24a2ZXACHg1DqLe7l7rpn1Bd4ys+Xu\nvuHAfd19BjADIBQKeUPmkuhTVlbKinf+hi9/jmH7P2CcVZBjXfi413X0OfNahvboH3REkbgVSfHn\nAj3qPO4eXvY5ZnYmcDtwqruXf7rc3XPD3zea2dvAKOALxS/xr7KyglUfvkLpkucYvO9tQlZMPq1Z\nnn4ebU66goGjT6e7bmsoctxFUvwLgf5m1ofawp8KfKPuBmY2CvgjMMndd9VZ3h4ocfdyM0sDJlJ7\n4VcSREVFBas/eoWypX9jQN7bjKSQYlJY0/5UUkZdyqCTz2dccrOgY4oklHqL392rzOxG4HVqh3PO\ndPeVZnYXkOnu84C7gVbAc1Y72+GnwzYHA380sxqgCbXn+Fcd9IUkbhQX7Wf1By9RvXIeAws/YCRF\nlHhzVrWZSNPhFzH4SxcyRuftRQJj7tF3Oj0UCnlmZmbQMeQI5O3MIfvDF0la/xqDixfSwsoppCXr\n204kedhkBky8iJQWKnuR48XMFrl7KJJt9cldOSo11TWsWz6fPYv/Ttq2txhQuZZx5uyiAyvTz6Xl\nyAsYMP4cxjRrHnRUETmAil8ilr9nB+vnv0zN+jfpWzCfQeQDsL5pfxb0mk7H0VPoN2ICnXSBViSq\nqfjlkEqLi8he/C+KVv+Ljrs+4sTK9Yw1J59WbGwzjq39zqTP+Mn079wTDb4UiR0qfvlMRXk52Uvf\nJ3/lP2m9/UMGlK9kuFVS5U3IbjaIhT2vpv3Ic+mXcSqjm+o/HZFYpf97E1hJcSEbs96heO27tNq5\nkD5lqxhitR/B+CSpN0tPuJiUgWdwYugsBrVpH3BaEWkoKv4EsmfbZjYtf4/yjR/RYW8mJ1auZ5hV\nU+PGxqTeLOs0meYnnkLfMWfRJ70bfYIOLCLHhYo/TpUWFbJl5YcUZM+n2Y7FdClaSWffQxpQ4Ul8\n0mwgi7tdTmq/L9E74wz6dUijX9ChRaRRqPjjQEVZCZvXLGZPdiZNcheRVrCcXtWbGWg1AOTSiS2t\nhrOxyxg6DphA72ETGJjaMuDUIhIUFX8scadw11a2rV1A0ZalJO1eRceidXStyqG/1dAfKKQlm1MG\nsTDtDFJ6j6PbsIl0O6EH3YLOLiJRQ8UfparKitmevZS9GxZRtX05LfPX0LVsA20p4tM7ym4jnW0p\nJ7K181dI6TGSrgNCnNBnCMN1C0IROQwVf8BKKqrYuLuYDbuLsNXz6Jr7Oukl2XSvzqGHOT2AEm/O\n5qa9Wdn2NKo7DaV1rwy6DQzRJS2drrVzI4mIREzF3wgqq2vYll/K5r0lbM4r4ZPdxWTvLmLDriJy\n80s/2+7mpgsYm7ySHan9yGn/VZqcMJz0fmPo0W8Ig5OTA/wNRCSeqPgbSHF5FVvySti8t4QtecXh\n77WPc/NLqa75z2R4qclJ9E1vSah3e6am9+DETq3o16kVvTqcTfPkpnQJ8PcQkfin4o9ATY2zt7iC\nbfmlbMsvJTe/lO0FZXUel7GnqPxz+7RrkUyvDi0Y2aMd54/sSs8OLejZsQW9Oragc+sUmjTRKRoR\nCUbCF39VdQ17iyvYWVjGzsLy8PcytuWHi72glO35ZVRU13xuv9TkJLq2S6Fru1QGd2lDjw61pd6r\nQ0t6dmxB21SdmhGR6BSXxV9T4+wrqWBPUQV7isrZU1TO7v3ln3u8p6icXYW132sOuCVBE4PObWpL\nfUT3dkwalkLXtql0bZdK13YpdGuXStvUZEwXVkUkBsVN8bs75z3wPjsLy8kr/mKZAyQnGR1bNiet\ndTPSWjVnaJe2dG7TnE5tUujcJqX259YppLVqRtMkTS0sIvEpborfzBjQuTXDu7UlrVVz0lo1I611\n8/DPzUkw2lRzAAADo0lEQVRv1Zw2qU11lC4iCS9uih/g3ssygo4gIhL1IjqfYWaTzGytmWWb2a0H\nWd/czOaE139sZr3rrLstvHytmZ3dcNFFRORo1Fv8ZpYEPAScAwwBppnZkAM2uxrY5+79gHuB/wvv\nOwSYCgwFJgEPh59PREQCEskR/zgg2903unsF8Aww5YBtpgBPhn9+HviK1Z5MnwI84+7l7v4JkB1+\nPhERCUgkxd8N2FrncU542UG3cfcqoADoGOG+IiLSiKJmzKKZTTezTDPL3L17d9BxRETiViTFnwv0\nqPO4e3jZQbcxs6ZAW2BvhPsC4O4z3D3k7qH09PTI0ouIyBGLpPgXAv3NrI+ZNaP2Yu28A7aZB1wZ\n/vkS4C139/DyqeFRP32A/sCChokuIiJHo95x/O5eZWY3Aq8DScBMd19pZncBme4+D3gceMrMsoE8\nav84EN7uWWAVUAXc4O7Vx+l3ERGRCFjtgXl0MbPdwOYj3C0N2HMc4sQivRe19D7U0vvwH/H8XvRy\n94jOk0dl8R8NM8t091DQOaKB3otaeh9q6X34D70XtaJmVI+IiDQOFb+ISIKJp+KfEXSAKKL3opbe\nh1p6H/5D7wVxdI5fREQiE09H/CIiEoG4Kn4z+7mZLTOzLDN7w8y6Bp0pCGZ2t5mtCb8XL5pZu6Az\nBcXMvm5mK82sxswSbjRHfVOqJwozm2lmu8xsRdBZokFcFT9wt7uPcPcM4GXgjqADBeRNYJi7jwDW\nAbcFnCdIK4CLgHeDDtLYIpxSPVHMonZqeCHOit/dC+s8bAkk5AUMd38jPEsqwHxq50hKSO6+2t3X\nBp0jIJFMqZ4Q3P1damcVEOLs1osAZvZL4FvUTg19esBxosF3gDlBh5BAHGxa9PEBZZEoEnPFb2b/\nBE44yKrb3f0ld78duN3MbgNuBO5s1ICNpL73IbzN7dTOkfTXxszW2CJ5L0TkP2Ku+N39zAg3/Svw\nKnFa/PW9D2Z2FXAe8BWP8zG7R/DfRKKJeFp0SSxxdY7fzPrXeTgFWBNUliCZ2STgJ8D57l4SdB4J\nTCRTqksCiqsPcJnZC8BAoIba2T2vd/eEO8IJT4/dnNqb4QDMd/frA4wUGDO7EHgASAfygSx3PzvY\nVI3HzM4F/sB/plT/ZcCRAmFms4HTqJ2dcydwp7s/HmioAMVV8YuISP3i6lSPiIjUT8UvIpJgVPwi\nIglGxS8ikmBU/CIiCUbFLyKSYFT8IiIJRsUvIpJg/j80LLBV2zfq9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f219d388e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine why the means are different for different number of simulations.\n",
    "def get_logit_terms(num_draws):\n",
    "    kl_wrapper.set_draws(num_draws)\n",
    "    std_draws = kl_wrapper.std_draws\n",
    "\n",
    "    e_beta = glmm_par_opt['beta'].mean.get()\n",
    "    info_beta = glmm_par_opt['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "\n",
    "    e_u = glmm_par_opt['u'].mean.get()[y_g_vec]\n",
    "    info_u = glmm_par_opt['u'].info.get()[y_g_vec]\n",
    "    var_u = 1 / info_u\n",
    "\n",
    "    z_mean = e_u + np.matmul(x_mat, e_beta)\n",
    "    z_sd = np.sqrt(var_u + np.einsum('nk,kj,nj->n', x_mat, cov_beta, x_mat))\n",
    "    z = np.einsum('i,j->ij', z_sd, std_draws) + np.expand_dims(z_mean, 1)\n",
    "\n",
    "    # The sum is over observations and draws, so dividing by the draws size\n",
    "    # gives the sum of sample expectations over the draws.\n",
    "    # p = exp(z) / (1 + exp(z))\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_terms = np.log1p(np.exp(z))\n",
    "    logit_term = -np.sum(logit_terms) / std_draws.size\n",
    "\n",
    "    return logit_term, logit_terms, z\n",
    "    \n",
    "logit_term_50, logit_terms_50, z_50 = get_logit_terms(50)    \n",
    "logit_term_800, logit_terms_800, z_800 = get_logit_terms(800)\n",
    "\n",
    "print logit_term_50\n",
    "print logit_term_800\n",
    "\n",
    "logit_terms_50_mean = np.mean(logit_terms_50, 1)\n",
    "logit_terms_800_mean = np.mean(logit_terms_800, 1)\n",
    "\n",
    "print np.max(np.abs(logit_terms_50_mean - logit_terms_800_mean))\n",
    "print np.where(np.abs(logit_terms_50_mean - logit_terms_800_mean) > 1e-3)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ind = 3\n",
    "plt.plot(z_800[ind, :], logit_terms_800[ind, :])\n",
    "plt.plot(z_50[ind, :], logit_terms_50[ind, :])\n",
    "print logit_terms_50_mean[ind]\n",
    "print logit_terms_800_mean[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Hessian:\n",
      "\n",
      "Log prior Hessian:\n",
      "\n",
      "hess_time: 97.330698\n",
      "cg_time: inf\n"
     ]
    }
   ],
   "source": [
    "# Get the Hessians at the number of draws used for optimization.\n",
    "\n",
    "kl_wrapper.set_draws(num_mc_draws)\n",
    "\n",
    "hess_time = time.time()\n",
    "print 'KL Hessian:\\n'\n",
    "kl_hess = KLHess(opt_x)\n",
    "\n",
    "print 'Log prior Hessian:\\n'\n",
    "log_prior_hess = PriorHess(opt_x, prior_par.get_vector())\n",
    "\n",
    "hess_time =  time.time() - hess_time\n",
    "elbo_hess = -kl_hess\n",
    "\n",
    "print 'hess_time: %f' % hess_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moment_jac = MomentJacobian(opt_x)\n",
    "lrvb_cov = np.matmul(moment_jac, np.linalg.solve(kl_hess, moment_jac.T))\n",
    "\n",
    "prior_indices = copy.deepcopy(prior_par)\n",
    "prior_indices.set_vector(1 + np.array(range(prior_indices.vector_size())))\n",
    "\n",
    "vp_indices = copy.deepcopy(glmm_par_opt)\n",
    "vp_indices.set_vector(1 + np.array(range(vp_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/LRVBLogitGLMM/LogitGLMMLRVB/inst/data/simulated_data_large_python_vb_results.json\n"
     ]
    }
   ],
   "source": [
    "if not simulate_data:\n",
    "    run_name = 'production'\n",
    "    result_dict = { 'glmm_par_opt': glmm_par_opt.dictval(), 'run_name': run_name,\n",
    "                    'vb_time': vb_time, 'hess_time': hess_time, 'num_mc_draws': num_mc_draws, \n",
    "                    'moment_indices': moment_indices.dictval(),\n",
    "                    'prior_indices': prior_indices.dictval(),\n",
    "                    'vp_indices': vp_indices.dictval(),\n",
    "                    'lrvb_cov': lrvb_cov.tolist(), 'moment_jac': moment_jac.tolist(),\n",
    "                    'elbo_hess': elbo_hess.tolist(), 'log_prior_hess': log_prior_hess.tolist() }\n",
    "\n",
    "    result_json = json.dumps(result_dict)\n",
    "    json_file = open(json_output_filename, 'w')\n",
    "    json_file.write(result_json)\n",
    "    json_file.close()\n",
    "\n",
    "    print(json_output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
