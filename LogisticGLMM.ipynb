{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from VariationalBayes import ScalarParam, ModelParamsDict, VectorParam, PosDefMatrixParam\n",
    "from VariationalBayes.NormalParams import MVNParam, UVNParam, UVNParamVector\n",
    "from VariationalBayes.GammaParams import GammaParam\n",
    "from VariationalBayes.ExponentialFamilies import \\\n",
    "    UnivariateNormalEntropy, MultivariateNormalEntropy, GammaEntropy, \\\n",
    "    MVNPrior, UVNPrior, GammaPrior\n",
    "\n",
    "\n",
    "from autograd import grad, hessian, jacobian, hessian_vector_product\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy as asp\n",
    "import scipy as sp\n",
    "\n",
    "import copy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'y_group', u'mu_prior_var', u'mu_prior_t', u'mu_prior_var_c', u'K', u'beta_prior_var', u'tau_prior_beta', u'N', u'mu_prior_mean_c', u'mu_prior_epsilon', u'mu_prior_mean', u'y', u'x', u'NG', u'beta_prior_mean', u'tau_prior_alpha']\n",
      "1000\n",
      "0.324\n"
     ]
    }
   ],
   "source": [
    "# Load data saved by stan_results_to_json.R and run_stan.R in LRVBLogitGLMM.\n",
    "import os\n",
    "import json\n",
    "\n",
    "simulate_data = False\n",
    "prior_par = ModelParamsDict('Prior Parameters')\n",
    "\n",
    "if not simulate_data:\n",
    "    analysis_name = 'simulated_data_small'\n",
    "    #analysis_name = 'simulated_data_large'\n",
    "\n",
    "    data_dir = os.path.join(os.environ['GIT_REPO_LOC'], 'LRVBLogitGLMM/LogitGLMMLRVB/inst/data/')\n",
    "    json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "    json_output_filename = os.path.join(data_dir, '%s_python_vb_results.json' % analysis_name)\n",
    "\n",
    "    json_file = open(json_filename, 'r')\n",
    "    json_dat = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    stan_dat = json_dat['stan_dat']\n",
    "    vp_base = json_dat['vp_base']\n",
    "\n",
    "    print stan_dat.keys()\n",
    "    K = stan_dat['K'][0]\n",
    "    NObs = stan_dat['N'][0]\n",
    "    NG = stan_dat['NG'][0]\n",
    "    N = NObs / NG\n",
    "    y_g_vec = np.array(stan_dat['y_group'])\n",
    "    y_vec = np.array(stan_dat['y'])\n",
    "    x_mat = np.array(stan_dat['x'])\n",
    "    \n",
    "    # Define a class to contain prior parameters.\n",
    "    prior_par.push_param(VectorParam('beta_prior_mean', K, val=np.array(stan_dat['beta_prior_mean'])))\n",
    "    prior_par.push_param(PosDefMatrixParam('beta_prior_var', K, val=np.array(stan_dat['beta_prior_var'])))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('mu_prior_mean', val=stan_dat['mu_prior_mean'][0]))\n",
    "    prior_par.push_param(ScalarParam('mu_prior_var', val=stan_dat['mu_prior_var'][0]))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('tau_prior_alpha', val=stan_dat['tau_prior_alpha'][0]))\n",
    "    prior_par.push_param(ScalarParam('tau_prior_beta', val=stan_dat['tau_prior_beta'][0]))\n",
    "\n",
    "    # An index set to make sure jacobians match the order expected by R.\n",
    "    prior_par_indices = copy.deepcopy(prior_par)\n",
    "    prior_par_indices.set_name('Prior Indices')\n",
    "    prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "else:\n",
    "    # Simulate data instead of loading it if you like\n",
    "    N = 200     # observations per group\n",
    "    K = 5      # dimension of regressors\n",
    "    NG = 200      # number of groups\n",
    "\n",
    "    # Generate data\n",
    "    def Logistic(u):\n",
    "        return np.exp(u) / (1 + np.exp(u))\n",
    "\n",
    "    NObs = NG * N\n",
    "    true_beta = np.array(range(5))\n",
    "    true_beta = true_beta - np.mean(true_beta)\n",
    "    true_mu = 0.\n",
    "    true_tau = 40.0\n",
    "    true_u = np.random.normal(true_mu, 1 / np.sqrt(true_tau), NG)\n",
    "\n",
    "    x_mat = np.random.random(K * NObs).reshape(NObs, K) - 0.5\n",
    "    y_g_vec = [ g for g in range(NG) for n in range(N) ]\n",
    "    true_rho = Logistic(np.matmul(x_mat, true_beta) + true_u[y_g_vec])\n",
    "    y_vec = np.random.random(NObs) < true_rho\n",
    "    \n",
    "    prior_par.push_param(VectorParam('beta_prior_mean', K, val=np.zeros(K)))\n",
    "    prior_par.push_param(PosDefMatrixParam('beta_prior_var', K, val=10 * np.eye(K)))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('mu_prior_mean', val=0))\n",
    "    prior_par.push_param(ScalarParam('mu_prior_var', val=2))\n",
    "\n",
    "    prior_par.push_param(ScalarParam('tau_prior_alpha', val=3.0))\n",
    "    prior_par.push_param(ScalarParam('tau_prior_beta', val=10.0))\n",
    "\n",
    "print N * NG\n",
    "print np.mean(y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'mu_info', u'tau_beta', u'tau_beta_min', u'tau_alpha', u'beta_info', u'tau_alpha_min', u'beta_diag_min', u'n_groups', u'encoded_size', u'beta_loc', u'u_info_min', u'u', u'k_reg', u'mu_loc', u'mu_info_min']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'u_info_min' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f2ca5c11b8c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mvp_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mvp_base\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu_info_min\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'u_info_min' is not defined"
     ]
    }
   ],
   "source": [
    "print vp_base.keys()\n",
    "print vp_base['u_info_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "GLMM Parameters:\n",
      "\tmu:\n",
      "mu_mean: 0.1\n",
      "mu_info: 1.0\n",
      "\ttau:\n",
      "tau_shape: 2.1\n",
      "tau_rate: 2.1\n",
      "\tbeta:\n",
      "beta_mean:\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "beta_info:\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "\tu:\n",
      "u_mean:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "u_info:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print vp_base['mu_info_min'][0]\n",
    "\n",
    "# Build an object to contain a variational approximation to a K-dimensional multivariate normal.\n",
    "glmm_par = ModelParamsDict('GLMM Parameters')\n",
    "\n",
    "glmm_par.push_param(UVNParam('mu', min_info=vp_base['mu_info_min'][0]))\n",
    "glmm_par.push_param(GammaParam('tau', min_shape=vp_base['tau_alpha_min'][0], min_rate=vp_base['tau_beta_min'][0]))\n",
    "glmm_par.push_param(MVNParam('beta', K, min_info=vp_base['beta_diag_min'][0]))\n",
    "glmm_par.push_param(UVNParamVector('u', NG, min_info=vp_base['u_info_min'][0]))\n",
    "\n",
    "glmm_par['mu'].mean.set(0.1)\n",
    "glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "glmm_par['tau'].shape.set(2.1)\n",
    "glmm_par['tau'].rate.set(2.1)\n",
    "\n",
    "glmm_par['beta'].mean.set(np.full(K, 0.))\n",
    "glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "glmm_par['u'].mean.set(np.full(NG, 0.))\n",
    "glmm_par['u'].info.set(np.full(NG, 1))\n",
    "\n",
    "print glmm_par\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define moment parameters\n",
    "\n",
    "moment_par = ModelParamsDict('Moment Parameters')\n",
    "moment_par.push_param(VectorParam('e_beta', K))\n",
    "moment_par.push_param(PosDefMatrixParam('e_beta_outer', K))\n",
    "moment_par.push_param(ScalarParam('e_mu'))\n",
    "moment_par.push_param(ScalarParam('e_mu2'))\n",
    "moment_par.push_param(ScalarParam('e_tau'))\n",
    "moment_par.push_param(ScalarParam('e_log_tau'))\n",
    "moment_par.push_param(VectorParam('e_u', NG))\n",
    "moment_par.push_param(VectorParam('e_u2', NG))\n",
    "\n",
    "def set_moments(glmm_par, moment_par):\n",
    "    moment_par['e_beta'].set(glmm_par['beta'].e())\n",
    "    moment_par['e_beta_outer'].set(glmm_par['beta'].e_outer())\n",
    "    moment_par['e_mu'].set(glmm_par['mu'].e())\n",
    "    moment_par['e_mu2'].set(glmm_par['mu'].e_outer())\n",
    "    moment_par['e_tau'].set(glmm_par['tau'].e())\n",
    "    moment_par['e_log_tau'].set(glmm_par['tau'].e_log())\n",
    "    moment_par['e_u'].set(glmm_par['u'].e())\n",
    "    moment_par['e_u2'].set((glmm_par['u'].e_outer()))\n",
    "    \n",
    "set_moments(glmm_par, moment_par)\n",
    "\n",
    "# Moment indices.\n",
    "moment_indices = copy.deepcopy(moment_par)\n",
    "moment_indices.set_vector(1 + np.array(range(moment_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode_combined_parameters(combined_free_par_vec, glmm_par, prior_par):\n",
    "    assert glmm_par.free_size() + prior_par.vector_size() == len(combined_free_par_vec) \n",
    "    glmm_par.set_free(combined_free_par_vec[0:glmm_par.free_size()])\n",
    "    prior_par.set_vector(combined_free_par_vec[glmm_par.free_size():])\n",
    "\n",
    "    \n",
    "def encode_combined_parameters(glmm_par, prior_par):\n",
    "    combined_free_par_vec = np.full(glmm_par.free_size() + prior_par.vector_size(), float('nan'))\n",
    "    combined_free_par_vec[0:glmm_par.free_size()] = glmm_par.get_free()\n",
    "    combined_free_par_vec[glmm_par.free_size():] = prior_par.get_vector()\n",
    "    return combined_free_par_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917.484930118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.5432527520990904"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ELogPrior(prior_par, glmm_par_elbo):\n",
    "    e_beta = glmm_par_elbo['beta'].mean.get()\n",
    "    info_beta = glmm_par_elbo['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "    beta_prior_info = np.linalg.inv(prior_par['beta_prior_var'].get())\n",
    "    beta_prior_mean = prior_par['beta_prior_mean'].get()\n",
    "    e_log_p_beta = MVNPrior(beta_prior_mean, beta_prior_info, e_beta, cov_beta)\n",
    "    \n",
    "    e_mu = glmm_par_elbo['mu'].mean.get()\n",
    "    info_mu = glmm_par_elbo['mu'].info.get()\n",
    "    var_mu = 1 / info_mu\n",
    "    e_log_p_mu = UVNPrior(prior_par['mu_prior_mean'].get(), 1 / prior_par['mu_prior_var'].get(), e_mu, var_mu) \n",
    "\n",
    "    e_tau = glmm_par_elbo['tau'].e()\n",
    "    e_log_tau = glmm_par_elbo['tau'].e_log()\n",
    "    tau_prior_shape = prior_par['tau_prior_alpha'].get()\n",
    "    tau_prior_rate = prior_par['tau_prior_beta'].get()\n",
    "    e_log_p_tau = GammaPrior(tau_prior_shape, tau_prior_rate, e_tau, e_log_tau)\n",
    "    \n",
    "    return  e_log_p_beta + e_log_p_mu + e_log_p_tau\n",
    "           \n",
    "\n",
    "def DataLogLikelihood(x_mat, y_vec, e_beta, cov_beta, e_u, var_u, std_draws):\n",
    "    z_mean = e_u + np.matmul(x_mat, e_beta)\n",
    "    z_sd = np.sqrt(var_u + np.einsum('nk,kj,nj->n', x_mat, cov_beta, x_mat))\n",
    "    z = np.einsum('i,j->ij', z_sd, std_draws) + np.expand_dims(z_mean, 1)\n",
    "\n",
    "    # The sum is over observations and draws, so dividing by the draws size\n",
    "    # gives the sum of sample expectations over the draws.\n",
    "    # p = exp(z) / (1 + exp(z))\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_term = -np.sum(np.log1p(np.exp(z))) / std_draws.size\n",
    "    y_term = np.sum(y_vec * z_mean)\n",
    "    return y_term + logit_term\n",
    "\n",
    "\n",
    "def RandomEffectLogLikelihood(e_u, var_u, e_mu, var_mu, e_tau, e_log_tau):\n",
    "    return -0.5 * e_tau * np.sum(((e_mu - e_u) ** 2) + var_mu + var_u) + 0.5 * e_log_tau * len(e_u)\n",
    "\n",
    "    \n",
    "def Elbo(y_vec, x_mat, y_g_vec, glmm_par_elbo, std_draws, prior_par):\n",
    "    e_beta = glmm_par_elbo['beta'].mean.get()\n",
    "    info_beta = glmm_par_elbo['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "    \n",
    "    e_u = glmm_par_elbo['u'].mean.get()\n",
    "    info_u = glmm_par_elbo['u'].info.get()\n",
    "    var_u = 1 / info_u\n",
    "    \n",
    "    e_mu = glmm_par_elbo['mu'].mean.get()\n",
    "    info_mu = glmm_par_elbo['mu'].info.get()\n",
    "    var_mu = 1 / info_mu\n",
    "    \n",
    "    e_tau = glmm_par_elbo['tau'].e()\n",
    "    e_log_tau = glmm_par_elbo['tau'].e_log()\n",
    "        \n",
    "    ll = \\\n",
    "        DataLogLikelihood(x_mat, y_vec, e_beta, cov_beta,\n",
    "                          e_u[y_g_vec], var_u[y_g_vec], std_draws) + \\\n",
    "        RandomEffectLogLikelihood(e_u, var_u, e_mu, var_mu, e_tau, e_log_tau)\n",
    "\n",
    "    e_log_prior = ELogPrior(prior_par, glmm_par_elbo)\n",
    "    \n",
    "    tau_shape = glmm_par_elbo['tau'].shape.get()\n",
    "    tau_rate = glmm_par_elbo['tau'].rate.get()\n",
    "    entropy = \\\n",
    "        UnivariateNormalEntropy(info_mu) + \\\n",
    "        MultivariateNormalEntropy(info_beta) + \\\n",
    "        UnivariateNormalEntropy(info_u) + \\\n",
    "        GammaEntropy(tau_shape, tau_rate)\n",
    "\n",
    "    return ll[0] + e_log_prior[0] + entropy\n",
    "\n",
    "\n",
    "class KLWrapper():\n",
    "    def __init__(self, glmm_par, moment_par, prior_par, x_mat, y_vec, y_g_vec, num_draws):\n",
    "        self.__glmm_par_ad = copy.deepcopy(glmm_par)\n",
    "        self.__moment_par = copy.deepcopy(moment_par)\n",
    "        self.__prior_par_ad = copy.deepcopy(prior_par)\n",
    "        self.x_mat = x_mat\n",
    "        self.y_vec = y_vec\n",
    "        self.y_g_vec = y_g_vec\n",
    "        draw_spacing = 1 / float(num_draws + 1)\n",
    "        target_quantiles = np.linspace(draw_spacing, 1 - draw_spacing, num_draws)\n",
    "        self.std_draws = sp.stats.norm.ppf(target_quantiles)\n",
    "\n",
    "    def Eval(self, free_par_vec, verbose=False):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        kl = -Elbo(self.y_vec, self.x_mat, self.y_g_vec,\n",
    "                   self.__glmm_par_ad, self.std_draws, self.__prior_par_ad)\n",
    "        if verbose: print kl\n",
    "            \n",
    "        # TODO: this is returning an array when it should be a scalar.\n",
    "        return kl\n",
    "    \n",
    "    def ExpectedLogPrior(self, combined_free_par_vec):\n",
    "        # Encode the glmm parameters first and the prior second.\n",
    "        decode_combined_parameters(combined_free_par_vec, self.__glmm_par_ad, self.__prior_par_ad)\n",
    "        e_log_prior = ELogPrior(self.__prior_par_ad, self.__glmm_par_ad)\n",
    "        return e_log_prior[0]\n",
    "        \n",
    "    # Return a posterior moment of interest as a function of\n",
    "    # unconstrained parameters.  In this case it is a bit silly,\n",
    "    # but in full generality posterior moments may be a complicated\n",
    "    # function of moment parameters.\n",
    "    def GetMoments(self, free_par_vec):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        set_moments(self.__glmm_par_ad, self.__moment_par)\n",
    "        return self.__moment_par.get_vector()\n",
    "\n",
    "\n",
    "# TODO: get the log prior derivatives, too.\n",
    "\n",
    "kl_wrapper = KLWrapper(glmm_par, moment_par, prior_par, x_mat, y_vec, y_g_vec, 10)\n",
    "KLGrad = grad(kl_wrapper.Eval)\n",
    "KLHess = hessian(kl_wrapper.Eval)\n",
    "MomentJacobian = jacobian(kl_wrapper.GetMoments)\n",
    "KLHessVecProd = hessian_vector_product(kl_wrapper.Eval)  \n",
    "free_par_vec = glmm_par.get_free()\n",
    "print kl_wrapper.Eval(free_par_vec)\n",
    "\n",
    "combined_free_par_vec = encode_combined_parameters(glmm_par, prior_par)\n",
    "PriorHess = hessian(kl_wrapper.ExpectedLogPrior)\n",
    "kl_wrapper.ExpectedLogPrior(combined_free_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function time:\n",
      "0.00148417949677\n",
      "Grad time:\n",
      "0.00717601776123\n",
      "Hessian vector product time:\n",
      "0.0137457132339\n",
      "Moment jacobian time:\n",
      "0.339930701256\n",
      "Prior Hessian time:\n",
      "1.25546884537\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "print 'Function time:'\n",
    "print timeit.timeit(lambda: kl_wrapper.Eval(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Grad time:'\n",
    "print timeit.timeit(lambda: KLGrad(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Hessian vector product time:'\n",
    "print timeit.timeit(lambda: KLHessVecProd(free_par_vec, free_par_vec + 1), number=time_num) / time_num\n",
    "\n",
    "print 'Moment jacobian time:'\n",
    "print timeit.timeit(lambda: MomentJacobian(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "time_num = 1\n",
    "print 'Prior Hessian time:'\n",
    "print timeit.timeit(lambda: PriorHess(combined_free_par_vec), number=time_num) / time_num\n",
    "\n",
    "# so slow\n",
    "# print 'Hessian time:'\n",
    "# print timeit.timeit(lambda: KLHess(free_par_vec), number=time_num) / time_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFGS\n",
      "917.489980118\n",
      "573.748050203\n",
      "1210.38249472\n",
      "508.777179013\n",
      "431.601965347\n",
      "336.713383719\n",
      "299.178221204\n",
      "238.76290226\n",
      "218.661302557\n",
      "184.611999864\n",
      "195.300228484\n",
      "170.215139503\n",
      "150.981197263\n",
      "135.610930691\n",
      "113.321530164\n",
      "98.0523800522\n",
      "87.2579695768\n",
      "80.8103672595\n",
      "76.5591051797\n",
      "74.6197136489\n",
      "73.6960236352\n",
      "72.231798695\n",
      "69.8151870091\n",
      "73.1395409838\n",
      "68.1604222002\n",
      "69.4182174503\n",
      "67.0497939729\n",
      "65.6087236898\n",
      "64.8327476475\n",
      "64.6532373374\n",
      "64.4431381686\n",
      "64.3166870499\n",
      "64.164979181\n",
      "63.9916812521\n",
      "63.8989211711\n",
      "63.8317504319\n",
      "63.808953448\n",
      "63.7900766486\n",
      "63.7699482082\n",
      "63.7380975897\n",
      "63.6858977592\n",
      "63.5962274028\n",
      "63.4357638849\n",
      "63.2686801644\n",
      "63.0908181472\n",
      "62.8131704977\n",
      "62.5462992414\n",
      "62.4556003893\n",
      "62.3772955962\n",
      "62.335410936\n",
      "62.3091527973\n",
      "62.2909199917\n",
      "62.2700147566\n",
      "62.2486172582\n",
      "62.2108744904\n",
      "62.1434893059\n",
      "62.0245521688\n",
      "61.8327873079\n",
      "61.9289367121\n",
      "61.7146960858\n",
      "61.5757177393\n",
      "61.4172600974\n",
      "61.3163151315\n",
      "61.2278997381\n",
      "61.131673126\n",
      "61.0224888129\n",
      "60.9754482593\n",
      "60.9435159687\n",
      "60.924916558\n",
      "60.9067394644\n",
      "60.8777740038\n",
      "60.8290739921\n",
      "60.7421535794\n",
      "60.597979667\n",
      "61.0431265662\n",
      "60.5319623612\n",
      "60.4308177959\n",
      "60.3006741221\n",
      "60.2780684505\n",
      "60.2385505275\n",
      "60.1937415498\n",
      "60.1812205282\n",
      "60.1597661551\n",
      "60.1321328603\n",
      "60.1255835116\n",
      "60.1231086285\n",
      "60.1210311894\n",
      "60.1192100469\n",
      "60.1162792249\n",
      "60.1117472781\n",
      "60.1060700306\n",
      "60.101649173\n",
      "60.0987406368\n",
      "60.0976927898\n",
      "60.0966077975\n",
      "60.0947809826\n",
      "60.0915954452\n",
      "60.0859975253\n",
      "60.0763146618\n",
      "60.0603424667\n",
      "60.0367701744\n",
      "60.0088631243\n",
      "59.9676298392\n",
      "59.9253836131\n",
      "59.8939786259\n",
      "59.854165241\n",
      "59.8095662643\n",
      "59.7991120873\n",
      "59.7811584404\n",
      "59.7435798719\n",
      "59.7197927493\n",
      "59.7138058625\n",
      "59.7086271639\n",
      "59.705600833\n",
      "59.70276943\n",
      "59.6988308009\n",
      "59.6921396562\n",
      "59.6808569619\n",
      "59.6626113399\n",
      "59.6358652799\n",
      "59.6069392028\n",
      "59.609592659\n",
      "59.5873555801\n",
      "59.5527220038\n",
      "59.4972416321\n",
      "59.473097042\n",
      "59.4603070879\n",
      "59.4273765933\n",
      "59.3892215016\n",
      "59.3342651476\n",
      "59.3259639925\n",
      "59.3102476779\n",
      "59.2807642559\n",
      "59.2391754008\n",
      "59.166133683\n",
      "59.0708036209\n",
      "59.0028178511\n",
      "58.9066245363\n",
      "58.7897774039\n",
      "59.9061325009\n",
      "58.7632812304\n",
      "58.719350744\n",
      "58.6569425332\n",
      "58.5998571088\n",
      "58.5170281133\n",
      "58.416879434\n",
      "58.2960910882\n",
      "58.1873005734\n",
      "58.0985214732\n",
      "57.9906870708\n",
      "57.8682169025\n",
      "57.8604415121\n",
      "57.8380734242\n",
      "57.7999854882\n",
      "57.7737764449\n",
      "57.7483710094\n",
      "57.7361554539\n",
      "57.7276478445\n",
      "57.7251678494\n",
      "57.7240929934\n",
      "57.7233943069\n",
      "57.7229999644\n",
      "57.7228457316\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 57.722846\n",
      "         Iterations: 150\n",
      "         Function evaluations: 163\n",
      "         Gradient evaluations: 163\n",
      "Running Newton Trust Region\n",
      "57.7228457316\n",
      "57.7227750125\n",
      "57.7227731584\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "57.7227731565\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 57.722773\n",
      "         Iterations: 16\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 4\n",
      "         Hessian evaluations: 0\n",
      "Done.\n",
      "0.212271285057\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "init_par_vec = free_par_vec\n",
    "\n",
    "# Optimize.\n",
    "vb_time = time.time()\n",
    "print 'Running BFGS'\n",
    "vb_opt_bfgs = optimize.minimize(\n",
    "    lambda par: kl_wrapper.Eval(par, verbose=True), init_par_vec,\n",
    "    method='bfgs', jac=KLGrad, tol=1e-2, options={'maxiter': 5000, 'disp': True})\n",
    "\n",
    "init_par_vec = free_par_vec\n",
    "print 'Running Newton Trust Region'\n",
    "vb_opt = optimize.minimize(\n",
    "    lambda par: kl_wrapper.Eval(par, verbose=True),\n",
    "    vb_opt_bfgs.x, method='trust-ncg', jac=KLGrad, hessp=KLHessVecProd,\n",
    "    tol=1e-8, options={'maxiter': 5000, 'disp': True})\n",
    "\n",
    "vb_time = time.time() - vb_time\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "glmm_par_opt.set_free(vb_opt.x)\n",
    "moment_jac = MomentJacobian(vb_opt.x)\n",
    "\n",
    "print 'Done.'\n",
    "\n",
    "print vb_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(glmm_par_opt)\n",
    "if simulate_data:\n",
    "    print true_beta\n",
    "    print glmm_par_opt['beta']\n",
    "    print '---------------\\n'\n",
    "    print true_tau\n",
    "    print glmm_par_opt['tau'].e()\n",
    "\n",
    "    e_u = glmm_par_opt['u'].e()\n",
    "    info_u = glmm_par_opt['u'].info.get()\n",
    "    var_u = 1 / info_u\n",
    "    e_beta = glmm_par_opt['beta'].e()\n",
    "    e_beta_outer = glmm_par_opt['beta'].e_outer()\n",
    "    std_draws = kl_wrapper.std_draws\n",
    "\n",
    "    rho_mean = e_u[y_g_vec] + np.matmul(x_mat, e_beta)\n",
    "    rho_sd = np.sqrt(var_u[y_g_vec] + np.einsum('nk,kj,nj->n', x_mat, e_beta_outer, x_mat))\n",
    "    z = np.einsum('i,j->ij', rho_sd, std_draws) + np.expand_dims(rho_mean, 1)\n",
    "    logit_term = -np.einsum('ij->i', np.log1p(np.exp(z))) / std_draws.size\n",
    "\n",
    "    print rho_sd\n",
    "    print var_u[y_g_vec]\n",
    "    # print np.mean(var_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the random effect estimates.  This requires simulated data.\n",
    "if simulate_data:\n",
    "    from ggplot import *\n",
    "    import pandas as pd\n",
    "    %matplotlib inline\n",
    "    \n",
    "    print glmm_par_opt['mu'].e()\n",
    "    print true_mu\n",
    "\n",
    "    print glmm_par_opt['tau'].e()\n",
    "    print true_tau\n",
    "\n",
    "    plot_df = pd.DataFrame({ 'opt': glmm_par_opt['u'].mean.get(), 'true': true_u })\n",
    "    print ggplot(plot_df, aes(x='true', y='opt')) + geom_point() + geom_abline(slope=1, intercept=0)\n",
    "    \n",
    "    plot_df = pd.DataFrame({ 'opt': glmm_par_opt['beta'].mean.get(), 'true': true_beta })\n",
    "    print ggplot(plot_df, aes(x='true', y='opt')) + geom_point() + geom_abline(slope=1, intercept=0)\n",
    "    \n",
    "    plot_df = pd.DataFrame({ 'opt': logit_term, 'true': np.log(1 - true_rho) })\n",
    "    print ggplot(plot_df, aes(x='true', y='opt')) + geom_point() + geom_abline(slope=1, intercept=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LRVB with conjugate gradient.  This turns out to be way slower with any appreciable number of moments.\n",
    "if False:\n",
    "    from scipy.sparse.linalg import LinearOperator\n",
    "    import sys\n",
    "\n",
    "    # This will actually compute Hess^1 * moment_jac.T, leading to perhaps confusing\n",
    "    # naming of \"columns\".  \n",
    "    ObjHessVecProdLO = LinearOperator((vb_opt.x.size, vb_opt.x.size), lambda par: KLHessVecProd(vb_opt.x, par))\n",
    "    # print moment_jac.T.shape\n",
    "    # print ObjHessVecProdLO.shape\n",
    "    # cg_res, info = scipy.sparse.linalg.cg(ObjHessVecProdLO, moment_jac.T)\n",
    "\n",
    "    cg_time = time.time()\n",
    "    lrvb_term = np.full(moment_jac.T.shape, float('nan'))\n",
    "    for col in range(moment_jac.shape[0]):\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()\n",
    "        cg_res, info = sp.sparse.linalg.cg(ObjHessVecProdLO, moment_jac[col, :])\n",
    "        assert info == 0\n",
    "        lrvb_term[:, col] = cg_res\n",
    "    cg_time = time.time() - cg_time\n",
    "\n",
    "    print 'all done dude'\n",
    "else:\n",
    "    cg_time = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hess_time: 2.824652\n",
      "cg_time: inf\n"
     ]
    }
   ],
   "source": [
    "# Slow, but probably faster than using CG.\n",
    "combined_free_par_vec = encode_combined_parameters(glmm_par_opt, prior_par)\n",
    "\n",
    "hess_time = time.time()\n",
    "kl_hess = KLHess(vb_opt.x)\n",
    "log_prior_hess_full = PriorHess(combined_free_par_vec)\n",
    "hess_time =  time.time() - hess_time\n",
    "elbo_hess = -kl_hess\n",
    "\n",
    "print 'hess_time: %f' % hess_time\n",
    "print 'cg_time: %f' % cg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glmm_inds = range(glmm_par_opt.free_size())\n",
    "prior_inds = range(glmm_par_opt.free_size(), len(combined_free_par_vec))\n",
    "log_prior_hess = log_prior_hess_full[np.ix_(prior_inds, glmm_inds)]\n",
    "\n",
    "lrvb_cov = np.matmul(moment_jac, np.linalg.solve(kl_hess, moment_jac.T))\n",
    "\n",
    "prior_indices = copy.deepcopy(prior_par)\n",
    "prior_indices.set_vector(1 + np.array(range(prior_indices.vector_size())))\n",
    "\n",
    "vp_indices = copy.deepcopy(glmm_par_opt)\n",
    "vp_indices.set_vector(1 + np.array(range(vp_indices.vector_size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/LRVBLogitGLMM/LogitGLMMLRVB/inst/data/simulated_data_small_python_vb_results.json\n"
     ]
    }
   ],
   "source": [
    "if not simulate_data:\n",
    "    result_dict = { 'glmm_par_opt': glmm_par_opt.dictval(),\n",
    "                    'vb_time': vb_time,'hess_time': hess_time, \n",
    "                    'moment_indices': moment_indices.dictval(),\n",
    "                    'prior_indices': prior_indices.dictval(),\n",
    "                    'vp_indices': vp_indices.dictval(),\n",
    "                    'lrvb_cov': lrvb_cov.tolist(), 'moment_jac': moment_jac.tolist(),\n",
    "                    'elbo_hess': elbo_hess.tolist(), 'log_prior_hess': log_prior_hess.tolist() }\n",
    "\n",
    "    result_json = json.dumps(result_dict)\n",
    "    json_file = open(json_output_filename, 'w')\n",
    "    json_file.write(result_json)\n",
    "    json_file.close()\n",
    "\n",
    "    print(json_output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.47116024586\n",
      "(224, 224)\n",
      "[[-0.00155647  0.        ]\n",
      " [ 0.         -0.00382605]]\n",
      "[[ 52.49841289 -52.29835763]\n",
      " [-52.29835763  52.59365634]]\n"
     ]
    }
   ],
   "source": [
    "glmm_par_elbo = copy.deepcopy(glmm_par)\n",
    "\n",
    "def DebugFun(glmm_par_elbo, prior_par):\n",
    "    e_beta = glmm_par_elbo['beta'].mean.get()\n",
    "    info_beta = glmm_par_elbo['beta'].info.get()\n",
    "    cov_beta = np.linalg.inv(info_beta)\n",
    "\n",
    "    e_u = glmm_par_elbo['u'].mean.get()\n",
    "    info_u = glmm_par_elbo['u'].info.get()\n",
    "    var_u = 1 / info_u\n",
    "\n",
    "    e_mu = glmm_par_elbo['mu'].mean.get()\n",
    "    info_mu = glmm_par_elbo['mu'].info.get()\n",
    "    var_mu = 1 / info_mu\n",
    "\n",
    "    e_tau = glmm_par_elbo['tau'].e()\n",
    "    e_log_tau = glmm_par_elbo['tau'].e_log()\n",
    "    \n",
    "    tau_shape = glmm_par_elbo['tau'].shape.get()\n",
    "    tau_rate = glmm_par_elbo['tau'].rate.get()\n",
    "\n",
    "    e_tau = glmm_par_elbo['tau'].e()\n",
    "    e_log_tau = glmm_par_elbo['tau'].e_log()\n",
    "    tau_prior_shape = prior_par['tau_prior_alpha'].get()\n",
    "    tau_prior_rate = prior_par['tau_prior_beta'].get()\n",
    "    e_log_p_tau = GammaPrior(tau_prior_shape, tau_prior_rate, e_tau, e_log_tau)\n",
    "\n",
    "#     return RandomEffectLogLikelihood(e_u, var_u, e_mu, var_mu, e_tau, e_log_tau)[0] + \\\n",
    "#            GammaEntropy(tau_shape, tau_rate) + \\\n",
    "#            e_log_p_tau[0]\n",
    "#     return RandomEffectLogLikelihood(e_u, var_u, e_mu, var_mu, e_tau, e_log_tau)[0]\n",
    "    return GammaEntropy(tau_shape, tau_rate)\n",
    "\n",
    "\n",
    "\n",
    "class DebugWrapper():\n",
    "    def __init__(self, glmm_par, prior_par, x_mat, y_vec, y_g_vec, num_draws):\n",
    "        self.__glmm_par_ad = copy.deepcopy(glmm_par)\n",
    "        self.__prior_par_ad = copy.deepcopy(prior_par)\n",
    "        self.x_mat = x_mat\n",
    "        self.y_vec = y_vec\n",
    "        self.y_g_vec = y_g_vec\n",
    "        draw_spacing = 1 / float(num_draws + 1)\n",
    "        target_quantiles = np.linspace(draw_spacing, 1 - draw_spacing, num_draws)\n",
    "        self.std_draws = sp.stats.norm.ppf(target_quantiles)\n",
    "\n",
    "    def Eval(self, free_par_vec):\n",
    "        self.__glmm_par_ad.set_free(free_par_vec)\n",
    "        return DebugFun(self.__glmm_par_ad, self.__prior_par_ad)\n",
    "\n",
    "\n",
    "debug_wrapper = DebugWrapper(glmm_par, prior_par, x_mat, y_vec, y_g_vec, 10)\n",
    "DebugGrad = grad(debug_wrapper.Eval)\n",
    "DebugHess = hessian(debug_wrapper.Eval)\n",
    "KLHessVecProd = hessian_vector_product(debug_wrapper.Eval)  \n",
    "\n",
    "free_par_vec = glmm_par_opt.get_free()\n",
    "print debug_wrapper.Eval(free_par_vec)\n",
    "foo = DebugHess(free_par_vec)\n",
    "print foo.shape\n",
    "\n",
    "\n",
    "prior_indices = copy.deepcopy(prior_par)\n",
    "prior_indices.set_vector(1 + np.array(range(prior_indices.vector_size())))\n",
    "\n",
    "vp_indices = copy.deepcopy(glmm_par_opt)\n",
    "vp_indices.set_vector(1 + np.array(range(vp_indices.vector_size())))\n",
    "\n",
    "inds = np.hstack([ vp_indices['tau'].shape.get() - 1, vp_indices['tau'].rate.get() - 1 ])\n",
    "print foo[np.ix_(inds, inds)]\n",
    "\n",
    "print kl_hess[np.ix_(inds, inds)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
