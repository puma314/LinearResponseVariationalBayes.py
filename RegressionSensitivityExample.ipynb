{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd import grad, hessian, jacobian, hessian_vector_product\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy as asp\n",
    "import scipy as sp\n",
    "\n",
    "import copy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data from a linear model y_vec = x_mat * beta + Gaussian noise\n",
    "\n",
    "N = 200     # observations per group\n",
    "K = 5      # dimension of regressors\n",
    "\n",
    "# Generate data\n",
    "true_beta = np.random.random(K)\n",
    "true_beta = true_beta - np.mean(true_beta)\n",
    "true_y_info = 1.0\n",
    "\n",
    "x_mat = np.random.random(K * N).reshape(N, K) - 0.5\n",
    "true_mean = np.matmul(x_mat, true_beta)\n",
    "y_vec = np.random.normal(true_mean, 1 / np.sqrt(true_y_info), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all the relevant data in a class.\n",
    "\n",
    "class LeastSquaresObjective(object):\n",
    "    def __init__(self, x_mat, y_vec):\n",
    "        self.x_mat = copy.deepcopy(x_mat)\n",
    "        self.y_vec = copy.deepcopy(y_vec)\n",
    "        self.weights = np.full(x_mat.shape[0], 1.0)\n",
    "        self.objective_grad = grad(self.objective)\n",
    "        self.objective_hess = hessian(self.objective)\n",
    "        self.objective_hvp = hessian_vector_product(self.objective)\n",
    "        \n",
    "    def objective(self, beta):\n",
    "        # Of course, this could be made much faster by caching x^T x and Y^T x.\n",
    "        return np.sum(self.weights * (y_vec - np.matmul(x_mat, beta))**2)\n",
    "        \n",
    "    def weighted_objective(self, beta, weights):\n",
    "        self.weights = weights\n",
    "        return self.objective(beta)\n",
    "\n",
    "ls = LeastSquaresObjective(x_mat, y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 223.000023\n",
      "         Iterations: 7\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "# Optimize the objective using only Hessian vector products via the conugate gradient trust region algorithm.\n",
    "\n",
    "print 'Running Newton Trust Region'\n",
    "beta_opt_tr = optimize.minimize(\n",
    "    ls.objective, np.full(K, 0.0), method='trust-ncg',\n",
    "    jac=ls.objective_grad, hessp=ls.objective_hvp,\n",
    "    tol=1e-6, options={'maxiter': 100, 'disp': True, 'gtol': 1e-6 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.65076139  0.03547502  0.26804489  0.60808264 -0.91844248]\n",
      "[-0.65076139  0.03547502  0.26804489  0.60808264 -0.91844248]\n"
     ]
    }
   ],
   "source": [
    "# We know the optimum in closed form in this case.\n",
    "beta_opt = np.linalg.solve(np.matmul(x_mat.transpose(), x_mat), np.matmul(x_mat.transpose(), y_vec))\n",
    "\n",
    "# The known optimum should be the same as the one found by the conjugate gradient trust region method.\n",
    "print beta_opt\n",
    "print beta_opt_tr.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "(5, 200)\n"
     ]
    }
   ],
   "source": [
    "# Get the sensitivity to the weights by differentiating the first order condition.\n",
    "id_weights = np.full(N, 1.0)\n",
    "weighted_objective_grad = grad(ls.weighted_objective)\n",
    "\n",
    "# Sanity check: these should be the same.\n",
    "print np.max(np.abs(weighted_objective_grad(beta_opt, id_weights) - ls.objective_grad(beta_opt)))\n",
    "\n",
    "# Get the second derivative d2 objective / dbeta dweights, called h_bw\n",
    "dobj_dbeta_dweight = jacobian(weighted_objective_grad, argnum=1)\n",
    "h_bw = dobj_dbeta_dweight(beta_opt, id_weights)\n",
    "\n",
    "# The hessian with respect to beta.\n",
    "h_bb = ls.objective_hess(beta_opt)\n",
    "\n",
    "# This is the sensitivity of beta to changing each weight.\n",
    "w_sensitivity = -1 * np.linalg.solve(h_bb, h_bw)\n",
    "print w_sensitivity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 220.981716\n",
      "         Iterations: 7\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "# Re-fit the model with a datapoint removed.\n",
    "\n",
    "remove_row = 0\n",
    "new_weights = np.full(N, 1.)\n",
    "new_weights[remove_row] = 0.\n",
    "ls.weights = copy.deepcopy(new_weights)\n",
    "\n",
    "beta_opt_perturb = optimize.minimize(\n",
    "    ls.objective, np.full(K, 0.0), method='trust-ncg',\n",
    "    jac=ls.objective_grad, hessp=ls.objective_hvp,\n",
    "    tol=1e-6, options={'maxiter': 100, 'disp': True, 'gtol': 1e-6 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00490503 -0.04509636 -0.04351522  0.03124869  0.00784744]\n",
      "[-0.00474112 -0.04358936 -0.04206106  0.03020444  0.0075852 ]\n"
     ]
    }
   ],
   "source": [
    "# These should be roughly the same, indicating that the sensitivity matrix\n",
    "# is a good estimator of the effect of removing a data point.\n",
    "\n",
    "print beta_opt_tr.x - beta_opt_perturb.x\n",
    "print w_sensitivity[:, remove_row]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
