{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from VariationalBayes import VectorParam, ScalarParam, PosDefMatrixParam, ModelParamsDict\n",
    "from autograd import grad, hessian, jacobian, hessian_vector_product\n",
    "import math\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import copy\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.622459331202\n",
      "[ 0.95257413  0.98201379]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01814993, -0.04858735])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build an object to contain a variational approximation to a K-dimensional multivariate normal.\n",
    "\n",
    "K = 30\n",
    "mvn_par = ModelParamsDict()\n",
    "\n",
    "mvn_par.push_param(VectorParam('e_mu', K))\n",
    "mvn_par.push_param(VectorParam('var_mu', K, lb=0))\n",
    "\n",
    "mvn_par['e_mu'].set(np.full(K, 0.1))\n",
    "mvn_par['var_mu'].set(np.full(K, 2.))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@primitive\n",
    "def Logistic(rho):\n",
    "    exp_rho = np.exp(rho) \n",
    "    return exp_rho / (1 + exp_rho)\n",
    "\n",
    "@primitive\n",
    "def LogisticGradient(logit_rho):\n",
    "    return logit_rho * (1 - logit_rho)\n",
    "\n",
    "def LogisticHessian(logit_rho_gradient, logit_rho):\n",
    "    return logit_rho_gradient * (1 - 2 * logit_rho)\n",
    "\n",
    "def Logistic_vjp(g, ans, vs, gvs, x):\n",
    "    return np.full(x.shape, g) * LogisticGradient(ans)\n",
    "\n",
    "def LogisticGradient_vjp(g, ans, vs, gvs, x):\n",
    "    return np.full(x.shape, g) * LogisticGradient(ans)\n",
    "\n",
    "print Logistic(0.5)\n",
    "print Logistic(np.array([3., 4.]))\n",
    "\n",
    "# From Stan:\n",
    "#     inline double log1m_inv_logit(double u) {\n",
    "#       using std::exp;\n",
    "#       if (u > 0.0)\n",
    "#         return -u - log1p(exp(-u));  // prevent underflow\n",
    "#       return -log1p(exp(u));\n",
    "#     }\n",
    "\n",
    "def Log1mInvLogit(u):\n",
    "    return -np.log1p(np.exp(-u))\n",
    "    \n",
    "Log1mInvLogit(5.0)\n",
    "Log1mInvLogit(np.array([4., 3.]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "\n",
    "N = 20000\n",
    "true_mu = np.random.rand(K).T - 0.5\n",
    "x_mat = np.full([N, K], float('nan'))\n",
    "y_vec = np.full([N], float('nan'))\n",
    "for n in range(N):\n",
    "    x_mat[n, :] = np.random.random(K) - 0.5\n",
    "    y_vec[n] = np.random.random(1) < Logistic(np.dot(x_mat[n, :], true_mu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the variational objective\n",
    "def LogLikelihood(x_row, y, e_mu, mu_var, std_draws):\n",
    "    # logit(rho) is the probability of y being 1, which has a normal distribution under q().\n",
    "    rho_mean = np.dot(x_row, e_mu)\n",
    "    rho_sd = np.sqrt(np.sum(x_row * x_row * mu_var))\n",
    "    # e_log_1mrho = np.mean([ Log1mInvLogit(std_draw * rho_sd + rho_mean) for std_draw in std_draws ])\n",
    "    e_log_1mrho = np.mean(Log1mInvLogit(std_draws * rho_sd + rho_mean))\n",
    "    return y * rho_mean + e_log_1mrho\n",
    "\n",
    "\n",
    "def UnivariateNormalExpectedEntropy(var_mu):\n",
    "    return 0.5 * np.log(var_mu)\n",
    "\n",
    "\n",
    "def Elbo(y_vec, x_mat, mvn_par_elbo, num_draws=10):\n",
    "    var_mu = mvn_par_elbo['var_mu'].get()\n",
    "    e_mu = mvn_par_elbo['e_mu'].get()\n",
    "\n",
    "    num_draws = 10\n",
    "    draw_spacing = 1 / float(num_draws + 1)\n",
    "    target_quantiles = np.linspace(draw_spacing, 1 - draw_spacing, num_draws)\n",
    "    std_draws = scipy.stats.norm.ppf(target_quantiles)\n",
    "\n",
    "    assert y_vec.size == x_mat.shape[0]\n",
    "    assert e_mu.size == x_mat.shape[1]\n",
    "\n",
    "    ll = 0\n",
    "    for n in range(y_vec.size):\n",
    "        ll += LogLikelihood(x_mat[n, :], y_vec[n], e_mu, var_mu, std_draws)\n",
    "\n",
    "    entropy = sum([ UnivariateNormalExpectedEntropy(var_mu_k) for var_mu_k in var_mu])\n",
    "\n",
    "    return ll + entropy\n",
    "\n",
    "\n",
    "class KLWrapper():\n",
    "    def __init__(self, mvn_par, x_mat, y_vec, num_draws):\n",
    "        self.__mvn_par_ad = copy.deepcopy(mvn_par)\n",
    "        self.x_mat = x_mat\n",
    "        self.y_vec = y_vec\n",
    "        self.num_draws = num_draws\n",
    "        \n",
    "    def Eval(self, free_par_vec, verbose=False):\n",
    "        self.__mvn_par_ad.set_free(free_par_vec)\n",
    "        kl = -Elbo(self.y_vec, self.x_mat, self.__mvn_par_ad, num_draws=self.num_draws)\n",
    "        if verbose: print kl\n",
    "        return kl\n",
    "    \n",
    "    # Return a posterior moment of interest as a function of\n",
    "    # unconstrained parameters.  In this case it is a bit silly,\n",
    "    # but in full generality posterior moments may be a complicated\n",
    "    # function of moment parameters.\n",
    "    def GetMu(self, free_par_vec):\n",
    "        self.__mvn_par_ad.set_free(free_par_vec)\n",
    "        return self.__mvn_par_ad['e_mu'].get()\n",
    "\n",
    "    \n",
    "kl_wrapper = KLWrapper(mvn_par, x_mat, y_vec, 10)\n",
    "KLGrad = grad(kl_wrapper.Eval)\n",
    "KLHess = hessian(kl_wrapper.Eval)\n",
    "MomentJacobian = jacobian(kl_wrapper.GetMu)\n",
    "KLHessVecProd = hessian_vector_product(kl_wrapper.Eval)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57330.7052117\n"
     ]
    }
   ],
   "source": [
    "# Check that the AD functions are working:\n",
    "mvn_par['e_mu'].set(true_mu)\n",
    "mvn_par['var_mu'].set(np.abs(true_mu) * 0.1)\n",
    "free_par_vec = mvn_par.get_free()\n",
    "print kl_wrapper.Eval(free_par_vec)\n",
    "if K < 10:\n",
    "    print KLGrad(free_par_vec)\n",
    "    print KLHess(free_par_vec)\n",
    "    print MomentJacobian(free_par_vec)\n",
    "    print KLHessVecProd(free_par_vec, free_par_vec + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function time:\n",
      "0.469360494614\n",
      "Grad time:\n",
      "7.69713962078\n",
      "Hessian vector product time:\n",
      "20.4013067961\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "time_num = 10\n",
    "\n",
    "print 'Function time:'\n",
    "print timeit.timeit(lambda: kl_wrapper.Eval(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Grad time:'\n",
    "print timeit.timeit(lambda: KLGrad(free_par_vec), number=time_num) / time_num\n",
    "\n",
    "print 'Hessian vector product time:'\n",
    "print timeit.timeit(lambda: KLHessVecProd(free_par_vec, free_par_vec + 1), number=time_num) / time_num\n",
    "\n",
    "if K < 10:\n",
    "    print 'Hessian time:'\n",
    "    print timeit.timeit(lambda: KLHess(free_par_vec), number=time_num) / time_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set initial values.\n",
    "\n",
    "# Is there not a better way than reduce?\n",
    "true_means = reduce(lambda x, y: x + y, x_draws) / N\n",
    "\n",
    "mvn_par['e_mu'].set(np.full(K, 1.0))\n",
    "init_par_vec = mvn_par.get_free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimize.\n",
    "\n",
    "print 'Running BFGS'\n",
    "vb_opt_bfgs = optimize.minimize(\n",
    "    lambda par: kl_wrapper.Eval(par, verbose=True), init_par_vec,\n",
    "    method='bfgs', jac=KLGrad, tol=1e-6)\n",
    "print 'Running Newton Trust Region'\n",
    "vb_opt = optimize.minimize(\n",
    "    lambda par: kl_wrapper.Eval(par, verbose=True),\n",
    "    vb_opt_bfgs.x, method='trust-ncg', jac=KLGrad, hess=KLHess)\n",
    "mvn_par_opt = copy.deepcopy(mvn_par)\n",
    "mvn_par_opt.set_free(vb_opt.x)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The mean parameters match, as expected.\n",
    "print mvn_par_opt['e_mu']\n",
    "print true_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LRVB\n",
    "moment_jac = MomentJacobian(vb_opt.x)\n",
    "opt_hess = KLHess(vb_opt.x)\n",
    "mu_cov = np.matmul(moment_jac, np.linalg.solve(opt_hess, moment_jac.T))\n",
    "\n",
    "# The VB variance is underestimated.\n",
    "print np.diag(mu_cov)\n",
    "print mvn_par_opt['var_mu']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
