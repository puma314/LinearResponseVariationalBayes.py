{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "\n",
    "from VariationalBayes.Parameters import convert_vector_to_free_hessian\n",
    "\n",
    "# from VariationalBayes.ParameterDictionary import ModelParamsDict\n",
    "# from VariationalBayes import PosDefMatrixParam, PosDefMatrixParamVector\n",
    "# from VariationalBayes import SimplexParam\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 100\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefdf8f38d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.29321891  0.17084884]\n",
      " [ 0.59107578  0.23059353]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        # Autograd derivatives\n",
    "        self.kl_free_grad = autograd.grad(self.kl_free)\n",
    "        self.kl_free_hessian = autograd.hessian(self.kl_free) # This will be slow.\n",
    "        self.kl_free_hvp = autograd.hessian_vector_product(self.kl_free)\n",
    "\n",
    "        self.kl_free_global_grad = autograd.grad(self.kl_free_global)\n",
    "        self.kl_free_global_hessian = autograd.hessian(self.kl_free_global)\n",
    "        self.kl_free_global_hvp = autograd.hessian_vector_product(self.kl_free_global)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "        self.kl_vector_global_jac = autograd.jacobian(self.kl_vector_global_local, argnum=0)\n",
    "        self.kl_vector_global_hessian = autograd.hessian(self.kl_vector_global_local, argnum=0)\n",
    "        self.kl_vector_global_local_hessian = autograd.jacobian(self.kl_vector_global_jac, argnum=1)\n",
    "\n",
    "        self.kl_vector_jac = autograd.jacobian(self.kl_vector)\n",
    "        self.kl_vector_hessian = autograd.hessian(self.kl_vector)\n",
    "\n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def kl_vector_hessian_sparse(self):\n",
    "        global_vec = self.params['global'].get_vector()\n",
    "        local_vec = self.params['local'].get_vector()\n",
    "    \n",
    "        global_hess = self.kl_vector_global_hessian(global_vec, local_vec)\n",
    "        global_local_hess = self.kl_vector_global_local_hessian(global_vec, local_vec)\n",
    "        local_hess_sparse = self.kl_vector_local_hessian_sparse()\n",
    "        sp_hess =  sp.sparse.bmat([ [global_hess,         global_local_hess],\n",
    "                                    [global_local_hess.T, local_hess_sparse]])\n",
    "        return np.array(sp_hess.toarray())\n",
    "    \n",
    "\n",
    "    # This takes free_params as an argument so it can be used in optimization.\n",
    "    def kl_free_hessian_sparse(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        kl_vector_hessian_sparse = self.kl_vector_hessian_sparse()\n",
    "        kl_vector_jac = self.kl_vector_jac(self.params.get_vector())\n",
    "        kl_hessian_sparse = convert_vector_to_free_hessian(\n",
    "            self.params, free_params, kl_vector_jac, kl_vector_hessian_sparse)\n",
    "\n",
    "        # If you don't convert to an array, it returns a matrix type, which\n",
    "        # seems to cause mysterious problems with scipy.optimize.minimize.\n",
    "        return np.array(kl_hessian_sparse)\n",
    "\n",
    "    # Wrappers for autodiff follow. The nomeclature is\n",
    "    # {function}_{free | vector}_{|global|local}_{|sparse}\n",
    "\n",
    "    def kl_free(self, free_params, verbose=False):\n",
    "        self.params.set_free(free_params)\n",
    "        kl = self.kl()\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_free_global(self, global_free_params, verbose=False):\n",
    "        self.params['global'].set_free(global_free_params)\n",
    "        kl = self.kl(include_local_entropy=False)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_vector_global_local(self, global_vec_params, local_vec_params,\n",
    "                               verbose=False, include_local_entropy=True):\n",
    "        self.params['global'].set_vector(global_vec_params)\n",
    "        self.params['local'].set_vector(local_vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_vector(self, vec_params, \n",
    "                  verbose=False, include_local_entropy=True):\n",
    "        self.params.set_vector(vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian:  0.5714678764343262\n",
      "Hessian vector product 0.02129054069519043\n"
     ]
    }
   ],
   "source": [
    "obj = Objective(x, params, prior_params)\n",
    "obj.optimize_z()\n",
    "\n",
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "obj.kl_free(free_par)\n",
    "\n",
    "grad = obj.kl_free_grad(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = obj.kl_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "grad = obj.kl_free_global_grad(global_free_par)\n",
    "hvp = obj.kl_free_global_hvp(global_free_par, grad)\n",
    "\n",
    "# Not as slow!  You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = obj.kl_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Hessian: ', sparse_hess_time)\n",
    "print('Hessian vector product', hvp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 100)\n",
      "(200, 100)\n",
      "1.7763568394e-15\n"
     ]
    }
   ],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(obj.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = obj.params['global'].get_free()\n",
    "free_par_local = obj.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    obj.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    obj.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(obj.x.shape)\n",
    "print(likelihood_by_obs_free_local_jac_sparse.shape)\n",
    "print(loglik_vector_local_weight_hessian_sparse.shape)\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAECCAYAAAAGmJmkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJLWUC4WkIhcKSUUuFJKKXCgkFf0fIcoTuuJBQgMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefdd8b7898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    # Compare the full and sparse Hessians\n",
    "    kl_vector_hessian = autograd.hessian(obj.kl_vector)\n",
    "    hessian = obj.kl_free_hessian(free_par) # Slow\n",
    "    vector_hessian = kl_vector_hessian(vec_par)  # Slow\n",
    "\n",
    "    # The slow full Hessian and sparse Hessian agree.\n",
    "    plt.matshow(sparse_hessian != 0)\n",
    "    assert np.max(np.abs(hessian - sparse_hessian)) < 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kl: 113.23371477427305\t\tkl_diff = -1101.0967098605045\t\tdiff = 4.252520108386069\n",
      " kl: 110.60024459827547\t\tkl_diff = -2.633470175997587\t\tdiff = 0.1855301154769795\n",
      " kl: 98.89008253500015\t\tkl_diff = -11.71016206327532\t\tdiff = 0.4713396172864548\n",
      " kl: 72.42181207287516\t\tkl_diff = -26.46827046212499\t\tdiff = 2.8561592701479475\n",
      " kl: 62.51170266122781\t\tkl_diff = -9.910109411647348\t\tdiff = 1.8932518164527616\n",
      " kl: 59.73752234829849\t\tkl_diff = -2.7741803129293174\t\tdiff = 0.46469352825834953\n",
      " kl: 58.68897080876041\t\tkl_diff = -1.0485515395380816\t\tdiff = 0.2416918785851685\n",
      " kl: 58.04561699371745\t\tkl_diff = -0.643353815042957\t\tdiff = 0.2740651799695639\n",
      " kl: 57.50952454773164\t\tkl_diff = -0.5360924459858154\t\tdiff = 0.2493263202697671\n",
      " kl: 56.792252489706364\t\tkl_diff = -0.7172720580252729\t\tdiff = 0.2412179610932217\n",
      " kl: 55.288620751958334\t\tkl_diff = -1.5036317377480302\t\tdiff = 0.27564041705604225\n",
      " kl: 52.02039932064825\t\tkl_diff = -3.2682214313100815\t\tdiff = 0.3084643033549934\n",
      " kl: 46.750825597830065\t\tkl_diff = -5.269573722818187\t\tdiff = 0.38244787954986603\n",
      " kl: 40.951790566457184\t\tkl_diff = -5.799035031372881\t\tdiff = 1.047573468152588\n",
      " kl: 38.276564321571854\t\tkl_diff = -2.67522624488533\t\tdiff = 1.9644550461597787\n",
      " kl: 38.271671351560606\t\tkl_diff = -0.00489297001124811\t\tdiff = 0.11125968132314501\n",
      " kl: 38.271670703433045\t\tkl_diff = -6.481275605096926e-07\t\tdiff = 0.0007479393434671328\n",
      " kl: 38.27167070319878\t\tkl_diff = -2.3426593998010503e-10\t\tdiff = 8.876524671741493e-06\n",
      " kl: 38.27167070319878\t\tkl_diff = 0.0\t\tdiff = 0.0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Perform EM.\n",
    "\n",
    "obj.params.set_free(init_par_vec)\n",
    "obj.optimize_z()\n",
    "global_param_vec = obj.params['global'].get_vector()\n",
    "kl = obj.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = obj.params['global'].get_free()\n",
    "\n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: obj.kl_free_global(par, verbose=False),\n",
    "       x0=global_free_par, jac=obj.kl_free_global_grad, hessp=obj.kl_free_global_hvp,\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #    lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #    x0=global_free_par, jac=obj.global_kl_grad, hess=obj.global_kl_hessian,\n",
    "    #    method='trust-ncg', options={'maxiter': 50})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #   x0=global_free_par, method='nelder-mead', options={'maxiter': 500})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #  x0=global_free_par, method='bfgs', options={'maxiter': 50})\n",
    "    obj.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    obj.optimize_z()\n",
    "\n",
    "    new_global_param_vec = obj.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = obj.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = obj.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "38.2716707032\n",
      "done\n",
      "38.2716707032\n"
     ]
    }
   ],
   "source": [
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "# vb_opt = optimize.minimize(\n",
    "#     lambda par: obj.kl_free(par, verbose=True),\n",
    "#     x0=em_free_par, jac=obj.kl_free_grad, hessp=obj.kl_free_hvp,\n",
    "#     method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "# print('Done')\n",
    "# obj.params.set_free(vb_opt.x)\n",
    "\n",
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    lambda par: obj.kl_free(par, verbose=True),\n",
    "    x0=em_free_par, jac=obj.kl_free_grad, hess=obj.kl_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(obj.kl_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefdd853e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = obj.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = obj.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = obj.kl_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n"
     ]
    }
   ],
   "source": [
    "weight_jac = obj.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(obj.x.shape[0]), rm_row)\n",
    "obj.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(obj.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(obj.params['global'])\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_obj = Objective(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.1030519358\n",
      "39.0967127491\n",
      "39.0960758068\n",
      "39.0947038429\n",
      "39.0947034586\n",
      "39.0947034586\n",
      "39.0947034353\n",
      "39.0947034338\n",
      "39.0947034338\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "init_par = rm_obj.params.get_free()\n",
    "\n",
    "rm_obj.kl_free(init_par)\n",
    "rm_obj.kl_free_hessian_sparse(init_par)\n",
    "\n",
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "# rm_vb_opt = optimize.minimize(\n",
    "#     lambda par: rm_obj.kl_free(par, verbose=True),\n",
    "#     x0=init_par, jac=rm_obj.kl_free_grad, hessp=rm_obj.kl_free_hvp,\n",
    "#     method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    lambda par: rm_obj.kl_free(par, verbose=True),\n",
    "    x0=init_par, jac=rm_obj.kl_free_grad, hess=rm_obj.kl_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_obj.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual sensitivity:\t [ -3.76804655e-03  -1.00472509e-03  -8.11225398e-09   2.39350841e-08]\n",
      "Predicted sensitivity:\t [ -3.71838417e-03  -9.91281347e-04  -3.43433977e-09   2.17974244e-08]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_obj.get_interesting_moments(rm_vb_opt.x) - obj.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
