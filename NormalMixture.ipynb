{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "\n",
    "# from VariationalBayes.ParameterDictionary import ModelParamsDict\n",
    "# from VariationalBayes import PosDefMatrixParam, PosDefMatrixParamVector\n",
    "# from VariationalBayes import SimplexParam\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 100\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAkHgA0AgCHwACMT/A2NiQc+gHfT0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4adcc237b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.69557792  0.73910634]\n",
      " [ 0.44743215  0.6899555 ]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = \\\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num))\n",
    "\n",
    "single_local_params = \\\n",
    "    vb.SimplexParam(name='e_z', shape=(1, k_num), val=np.full((1, k_num), 1. / k_num))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "\n",
    "single_obs_params = vb.ModelParamsDict('mixture model single obs')\n",
    "single_obs_params.push_param(params['global'])\n",
    "single_obs_params.push_param(single_local_params)\n",
    "\n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def data_log_likelihood(mu, info, e_z, x, weights):\n",
    "    k_num = e_z.shape[1]\n",
    "    assert k_num == mu.shape[0]\n",
    "    assert k_num == info.shape[0]\n",
    "    log_lik = 0.0\n",
    "    # I would be interested to see how this could work without a loop.\n",
    "    e_z_weighted = weights * e_z\n",
    "    for k in range(k_num):\n",
    "        x_centered = x - np.expand_dims(mu[k, :], axis=0)\n",
    "        log_lik = log_lik - \\\n",
    "            0.5 * np.einsum('ni, ij, nj, n', \\\n",
    "                            x_centered, info[k, :, :], x_centered, e_z_weighted[:, k])\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        assert sign > 0\n",
    "        log_lik = log_lik + 0.5 * logdet * np.sum(e_z_weighted[:, k])\n",
    "    return log_lik\n",
    "\n",
    "def indicator_log_likelihood(e_z, pi):\n",
    "    return np.sum(np.matmul(e_z, np.log(pi.T)))\n",
    "\n",
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['e_z'].get()\n",
    "data_log_likelihood(true_mu, true_info, e_z, x, weights)\n",
    "indicator_log_likelihood(e_z, true_pi)\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from VariationalBayes.Parameters import convert_vector_to_free_hessian\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        # Autograd derivatives\n",
    "        self.kl_grad = autograd.grad(self.kl_wrapper)\n",
    "        self.kl_hessian = autograd.hessian(self.kl_wrapper)\n",
    "        self.kl_hvp = autograd.hessian_vector_product(self.kl_wrapper)\n",
    "\n",
    "        self.global_kl_grad = autograd.grad(self.global_kl_wrapper)\n",
    "        self.global_kl_hvp = autograd.hessian_vector_product(self.global_kl_wrapper)\n",
    "        self.global_kl_hessian = autograd.hessian(self.global_kl_wrapper)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.expected_log_likelihood, argnum=0)\n",
    "\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "        self.get_global_vector_jacobian = autograd.jacobian(self.vector_kl_wrapper, argnum=0)\n",
    "        self.get_global_vector_hessian = autograd.hessian(self.vector_kl_wrapper, argnum=0)\n",
    "        self.get_global_local_vector_hessian = \\\n",
    "            autograd.jacobian(self.get_global_vector_jacobian, argnum=1)\n",
    "\n",
    "        self.get_vector_jacobian = autograd.jacobian(self.full_vector_kl_wrapper)\n",
    "        self.get_vector_hessian = autograd.hessian(self.full_vector_kl_wrapper)\n",
    "\n",
    "            \n",
    "    def expected_log_likelihood(self, e_z, mu, info, pi, weights):\n",
    "        elbo = 0.0\n",
    "\n",
    "        # Data:\n",
    "        elbo += data_log_likelihood(mu, info, e_z, self.x, weights)\n",
    "        elbo += indicator_log_likelihood(e_z, pi)\n",
    "        \n",
    "        # Priors:\n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        elbo += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        elbo += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        elbo += info_prior(info, self.prior_params['dof'].get())\n",
    "\n",
    "        return elbo\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['e_z'].get()\n",
    "\n",
    "        natural_parameters = obj.get_z_nat_params(e_z, mu, info, pi, self.weights)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['e_z'].get()\n",
    "\n",
    "        elbo = self.expected_log_likelihood(e_z, mu, info, pi, self.weights)\n",
    "        \n",
    "        if include_local_entropy:\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "\n",
    "    def kl_wrapper(self, free_params, verbose=False):\n",
    "        self.params.set_free(free_params)\n",
    "        kl = self.kl()\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def global_kl_wrapper(self, global_free_params, verbose=False):\n",
    "        self.params['global'].set_free(global_free_params)\n",
    "        kl = self.kl(include_local_entropy=False)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def vector_kl_wrapper(self, global_vec_params, local_vec_params,\n",
    "                          verbose=False, include_local_entropy=True):\n",
    "        self.params['global'].set_vector(global_vec_params)\n",
    "        self.params['e_z'].set_vector(local_vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def full_vector_kl_wrapper(self, vec_params, \n",
    "                               verbose=False, include_local_entropy=True):\n",
    "        self.params.set_vector(vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def get_sparse_local_vector_hessian(self, local_vec):\n",
    "        self.params['e_z'].set_vector(local_vec)\n",
    "        e_z = self.params['e_z'].get()\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            row_inds = self.params['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (len(local_vec), len(local_vec)))\n",
    "                \n",
    "    def get_sparse_vector_hessian(self, vec_params):\n",
    "        self.params.set_vector(vec_params)\n",
    "        global_vec = obj.params['global'].get_vector()\n",
    "        local_vec = obj.params['e_z'].get_vector()\n",
    "        global_vec_hess = obj.get_global_vector_hessian(global_vec, local_vec)\n",
    "        global_local_vec_hess = obj.get_global_local_vector_hessian(global_vec, local_vec)\n",
    "        local_vector_hessian = self.get_sparse_local_vector_hessian(local_vec)\n",
    "        sp_hess =  sp.sparse.bmat([ [global_vec_hess,         global_local_vec_hess],\n",
    "                                    [global_local_vec_hess.T, local_vector_hessian]])\n",
    "        return np.array(sp_hess.toarray())\n",
    "    \n",
    "    def get_sparse_hessian(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        vec_par = self.params.get_vector()\n",
    "        sparse_vector_hessian = obj.get_sparse_vector_hessian(vec_par)\n",
    "        vector_jac = obj.get_vector_jacobian(vec_par)\n",
    "        # If you don't convert to an array, it returns a matrix type, which\n",
    "        # seems to cause mysterious problems with scipy.optimize.minimize.\n",
    "\n",
    "        sparse_hessian = convert_vector_to_free_hessian(\n",
    "            self.params, free_params, vector_jac, sparse_vector_hessian)\n",
    "        return np.array(sparse_hessian)\n",
    "\n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian:  0.759526252746582\n",
      "Hessian vector product 0.035076141357421875\n"
     ]
    }
   ],
   "source": [
    "obj = Objective(x, params, prior_params)\n",
    "obj.optimize_z()\n",
    "\n",
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "obj.kl_wrapper(free_par)\n",
    "\n",
    "grad = obj.kl_grad(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = obj.kl_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "grad = obj.global_kl_grad(global_free_par)\n",
    "hvp = obj.global_kl_hvp(global_free_par, grad)\n",
    "\n",
    "# Not as slow!  You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = obj.get_sparse_hessian(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Hessian: ', sparse_hess_time)\n",
    "print('Hessian vector product', hvp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7763568394e-15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAECCAYAAAAGmJmkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJLWUC4WkIhcKSUUuFJKKXCgkFf0fIcoTuuJBQgMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4adaa5dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if True:\n",
    "    # Compare the full and sparse Hessians\n",
    "    get_vector_hessian = autograd.hessian(obj.full_vector_kl_wrapper)\n",
    "    hessian = obj.kl_hessian(free_par) # Slow\n",
    "    vector_hessian = get_vector_hessian(vec_par)  # Slow\n",
    "\n",
    "    # The slow full Hessian and sparse Hessian agree.\n",
    "    plt.matshow(sparse_hessian != 0)\n",
    "    print(np.max(np.abs(hessian - sparse_hessian)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kl: 162.12143728821763\t\tkl_diff = -717.4326042235858\t\tdiff = 2.508131678503486\n",
      " kl: 150.71015012133972\t\tkl_diff = -11.411287166877912\t\tdiff = 0.4609320319204535\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Perform EM.\n",
    "\n",
    "obj.params.set_free(init_par_vec)\n",
    "obj.optimize_z()\n",
    "global_param_vec = obj.params['global'].get_vector()\n",
    "kl = obj.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = obj.params['global'].get_free()\n",
    "\n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "       x0=global_free_par, jac=obj.global_kl_grad, hessp=obj.global_kl_hvp,\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #    lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #    x0=global_free_par, jac=obj.global_kl_grad, hess=obj.global_kl_hessian,\n",
    "    #    method='trust-ncg', options={'maxiter': 50})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #   x0=global_free_par, method='nelder-mead', options={'maxiter': 500})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #  x0=global_free_par, method='bfgs', options={'maxiter': 50})\n",
    "    obj.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    obj.optimize_z()\n",
    "\n",
    "    new_global_param_vec = obj.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = obj.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-0:\n",
    "        break\n",
    "\n",
    "em_free_par = obj.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.710150121\n",
      "131.529613949\n",
      "121.750411123\n",
      "119.685829189\n",
      "113.834441593\n",
      "112.325144171\n",
      "110.200912018\n",
      "107.552594375\n",
      "104.964049777\n",
      "100.662329651\n",
      "93.3769466219\n",
      "90.7399627093\n",
      "79.7457059215\n",
      "76.0323775031\n",
      "61.1782618379\n",
      "58.9387157425\n",
      "58.5113054095\n",
      "49.0803955201\n",
      "48.7167460088\n",
      "48.1865799229\n",
      "44.6375834911\n",
      "44.5143057061\n",
      "44.366288174\n",
      "42.6236863642\n",
      "42.5320293251\n",
      "42.3426227811\n",
      "298.552551194\n",
      "41.2867574185\n",
      "50.4829588223\n",
      "40.625321063\n",
      "38.761615084\n",
      "38.3918069145\n",
      "38.1848153516\n",
      "37.7024294639\n",
      "37.065230271\n",
      "36.6556936047\n",
      "35.5087110479\n",
      "35.1742807447\n",
      "34.6129886357\n",
      "32.9939716264\n",
      "32.4198088646\n",
      "31.4351998462\n",
      "30.0931293506\n",
      "29.9036663658\n",
      "28.4777759993\n",
      "27.7769731217\n",
      "27.6384452873\n",
      "24.7942498458\n",
      "23.9911813931\n",
      "23.9471521532\n",
      "21.083549851\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "vb_opt_cg = optimize.minimize(\n",
    "    lambda par: obj.kl_wrapper(par, verbose=True),\n",
    "    x0=em_free_par, jac=obj.kl_grad, hessp=obj.kl_hvp,\n",
    "    method='trust-ncg', options={'maxiter': 50})\n",
    "\n",
    "print('Done')\n",
    "obj.params.set_free(vb_opt_cg.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(111, 111)\n",
      "(111,)\n",
      "(111,)\n"
     ]
    }
   ],
   "source": [
    "def jac_trans(free_par):\n",
    "    return obj.kl_grad(free_par)\n",
    "    #return np.expand_dims(obj.kl_grad(em_free_par), axis=1)\n",
    "\n",
    "print(type(obj.get_sparse_hessian(em_free_par)))\n",
    "print(obj.get_sparse_hessian(em_free_par).shape)\n",
    "print(obj.kl_grad(em_free_par).shape)\n",
    "print(jac_trans(em_free_par).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.710150121\n",
      "131.529613949\n",
      "121.750411123\n",
      "119.685829189\n",
      "113.834441593\n",
      "112.325144171\n",
      "110.200912018\n",
      "107.552594375\n",
      "104.964049777\n",
      "100.662329651\n",
      "93.3769466219\n",
      "90.7399627093\n",
      "79.7457059215\n",
      "76.0323775031\n",
      "61.1782618379\n",
      "58.9387157425\n",
      "58.5113054095\n",
      "49.0803955201\n",
      "48.7167460088\n",
      "48.1865799229\n",
      "44.6375835193\n",
      "44.5143057093\n",
      "44.3662881791\n",
      "42.6236863774\n",
      "42.532029331\n",
      "42.3426227981\n",
      "298.552486545\n",
      "41.286756325\n",
      "50.4829367872\n",
      "40.6253186316\n",
      "38.7616146545\n",
      "38.3918069694\n",
      "38.1848155337\n",
      "37.7024294583\n",
      "37.0652300871\n",
      "36.6556935864\n",
      "35.5087134049\n",
      "35.1742806353\n",
      "34.6129727398\n",
      "32.9939622386\n",
      "32.4198089774\n",
      "31.4352371548\n",
      "30.0931338082\n",
      "29.9036659599\n",
      "28.4777830286\n",
      "27.7769857933\n",
      "27.6384535067\n",
      "24.7943183682\n",
      "23.9912049521\n",
      "23.947155806\n",
      "21.0830147083\n",
      "done\n",
      "21.0830147083\n"
     ]
    }
   ],
   "source": [
    "vb_opt_hess = optimize.minimize(\n",
    "    lambda par: obj.kl_wrapper(par, verbose=True),\n",
    "    x0=em_free_par, jac=jac_trans, hess=obj.get_sparse_hessian,\n",
    "    method='trust-ncg', options={'maxiter': 50})\n",
    "\n",
    "print('done')\n",
    "print(obj.kl_wrapper(vb_opt_hess.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ada968588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = obj.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the sensitivity operator with conjugate gradient to avoid constructing the Hessian.\n",
    "\n",
    "moment_jac = obj.get_moment_jacobian(vb_opt.x)\n",
    "sensitivity_operator = np.full_like(moment_jac, float('nan'))\n",
    "free_param_size = len(vb_opt.x)\n",
    "\n",
    "KLHessVecProdLO = LinearOperator((free_param_size, free_param_size),\n",
    "                                 lambda vec: obj.kl_hvp(vb_opt.x, vec))\n",
    "for ind in range(sensitivity_operator.shape[0]):\n",
    "    cg_res, info = sp.sparse.linalg.cg(KLHessVecProdLO, moment_jac[ind, :].T)\n",
    "    sensitivity_operator[ind, :] = cg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj.params.set_free(vb_opt.x)\n",
    "def kl_weight_fun(weights):\n",
    "    obj.weights = weights\n",
    "    return obj.kl()\n",
    "\n",
    "default_weights = np.full((n_num, 1), 1.0)\n",
    "get_kl_weight_grad = autograd.grad(kl_weight_fun)\n",
    "kl_weight_grad = get_kl_weight_grad(default_weights)\n",
    "mu_weight_sens = np.matmul(sensitivity_operator, kl_weight_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
