{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "\n",
    "from VariationalBayes.Parameters import convert_vector_to_free_hessian\n",
    "\n",
    "# from VariationalBayes.ParameterDictionary import ModelParamsDict\n",
    "# from VariationalBayes import PosDefMatrixParam, PosDefMatrixParamVector\n",
    "# from VariationalBayes import SimplexParam\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 100\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f016bd307b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.61177594  0.79390327]\n",
      " [ 0.4567483   0.97602364]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        # Autograd derivatives\n",
    "        self.kl_free_grad = autograd.grad(self.kl_free)\n",
    "        self.kl_free_hessian = autograd.hessian(self.kl_free) # This will be slow.\n",
    "        self.kl_free_hvp = autograd.hessian_vector_product(self.kl_free)\n",
    "\n",
    "        self.kl_free_global_grad = autograd.grad(self.kl_free_global)\n",
    "        self.kl_free_global_hessian = autograd.hessian(self.kl_free_global)\n",
    "        self.kl_free_global_hvp = autograd.hessian_vector_product(self.kl_free_global)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "        self.kl_vector_global_jac = autograd.jacobian(self.kl_vector_global_local, argnum=0)\n",
    "        self.kl_vector_global_hessian = autograd.hessian(self.kl_vector_global_local, argnum=0)\n",
    "        self.kl_vector_global_local_hessian = autograd.jacobian(self.kl_vector_global_jac, argnum=1)\n",
    "\n",
    "        self.kl_vector_jac = autograd.jacobian(self.kl_vector)\n",
    "        self.kl_vector_hessian = autograd.hessian(self.kl_vector)\n",
    "\n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def kl_vector_hessian_sparse(self):\n",
    "        global_vec = self.params['global'].get_vector()\n",
    "        local_vec = self.params['local'].get_vector()\n",
    "    \n",
    "        global_hess = self.kl_vector_global_hessian(global_vec, local_vec)\n",
    "        global_local_hess = self.kl_vector_global_local_hessian(global_vec, local_vec)\n",
    "        local_hess_sparse = self.kl_vector_local_hessian_sparse()\n",
    "        sp_hess =  sp.sparse.bmat([ [global_hess,         global_local_hess],\n",
    "                                    [global_local_hess.T, local_hess_sparse]])\n",
    "        return np.array(sp_hess.toarray())\n",
    "    \n",
    "\n",
    "    # This takes free_params as an argument so it can be used in optimization.\n",
    "    def kl_free_hessian_sparse(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        kl_vector_hessian_sparse = self.kl_vector_hessian_sparse()\n",
    "        kl_vector_jac = self.kl_vector_jac(self.params.get_vector())\n",
    "        kl_hessian_sparse = convert_vector_to_free_hessian(\n",
    "            self.params, free_params, kl_vector_jac, kl_vector_hessian_sparse)\n",
    "\n",
    "        # If you don't convert to an array, it returns a matrix type, which\n",
    "        # seems to cause mysterious problems with scipy.optimize.minimize.\n",
    "        return np.array(kl_hessian_sparse)\n",
    "\n",
    "    # Wrappers for autodiff follow. The nomeclature is\n",
    "    # {function}_{free | vector}_{|global|local}_{|sparse}\n",
    "\n",
    "    def kl_free(self, free_params, verbose=False):\n",
    "        self.params.set_free(free_params)\n",
    "        kl = self.kl()\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_free_global(self, global_free_params, verbose=False):\n",
    "        self.params['global'].set_free(global_free_params)\n",
    "        kl = self.kl(include_local_entropy=False)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_vector_global_local(self, global_vec_params, local_vec_params,\n",
    "                               verbose=False, include_local_entropy=True):\n",
    "        self.params['global'].set_vector(global_vec_params)\n",
    "        self.params['local'].set_vector(local_vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_vector(self, vec_params, \n",
    "                  verbose=False, include_local_entropy=True):\n",
    "        self.params.set_vector(vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian:  0.5805191993713379\n",
      "Hessian vector product 0.020444869995117188\n"
     ]
    }
   ],
   "source": [
    "model = Model(x, params, prior_params)\n",
    "model.optimize_z()\n",
    "\n",
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "model.kl_free(free_par)\n",
    "\n",
    "grad = obj.kl_free_grad(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = model.kl_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "grad = model.kl_free_global_grad(global_free_par)\n",
    "hvp = model.kl_free_global_hvp(global_free_par, grad)\n",
    "\n",
    "# Not as slow!  You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = model.kl_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Hessian: ', sparse_hess_time)\n",
    "print('Hessian vector product', hvp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 100)\n",
      "(200, 100)\n",
      "8.881784197e-16\n"
     ]
    }
   ],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(obj.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = obj.params['global'].get_free()\n",
    "free_par_local = obj.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    obj.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    obj.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(obj.x.shape)\n",
    "print(likelihood_by_obs_free_local_jac_sparse.shape)\n",
    "print(loglik_vector_local_weight_hessian_sparse.shape)\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAECCAYAAAAGmJmkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJLWUC4WkIhcKSUUuFJKKXCgkFf0fIcoTuuJBQgMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0169d2c198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    # Compare the full and sparse Hessians\n",
    "    kl_vector_hessian = autograd.hessian(model.kl_vector)\n",
    "    hessian = model.kl_free_hessian(free_par) # Slow\n",
    "    vector_hessian = kl_vector_hessian(vec_par)  # Slow\n",
    "\n",
    "    # The slow full Hessian and sparse Hessian agree.\n",
    "    plt.matshow(sparse_hessian != 0)\n",
    "    assert np.max(np.abs(hessian - sparse_hessian)) < 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kl: 65.5216754166816\t\tkl_diff = -531.1452016537255\t\tdiff = 2.9603594160961695\n",
      " kl: 65.46763493014835\t\tkl_diff = -0.05404048653325333\t\tdiff = 0.029041830790111245\n",
      " kl: 65.33602121578957\t\tkl_diff = -0.1316137143587781\t\tdiff = 0.05166484062981569\n",
      " kl: 64.92132540583609\t\tkl_diff = -0.41469580995348565\t\tdiff = 0.09813716572296105\n",
      " kl: 63.40216845957899\t\tkl_diff = -1.5191569462570982\t\tdiff = 0.187735203424237\n",
      " kl: 57.77525546736068\t\tkl_diff = -5.626912992218308\t\tdiff = 0.3346201069196626\n",
      " kl: 45.73827134027532\t\tkl_diff = -12.036984127085361\t\tdiff = 0.7453020334874485\n",
      " kl: 41.01192831660232\t\tkl_diff = -4.726343023672996\t\tdiff = 0.6178020387956238\n",
      " kl: 40.05448100916955\t\tkl_diff = -0.9574473074327727\t\tdiff = 0.21655497078016595\n",
      " kl: 39.809925123142705\t\tkl_diff = -0.24455588602684486\t\tdiff = 0.0787575294840751\n",
      " kl: 39.72194765120244\t\tkl_diff = -0.08797747194026329\t\tdiff = 0.04939892878594093\n",
      " kl: 39.66905155915344\t\tkl_diff = -0.052896092049003585\t\tdiff = 0.03713977051509243\n",
      " kl: 39.625582828948765\t\tkl_diff = -0.04346873020467257\t\tdiff = 0.04571166248710501\n",
      " kl: 39.58580656162353\t\tkl_diff = -0.03977626732523731\t\tdiff = 0.04731819556622874\n",
      " kl: 39.547979405563495\t\tkl_diff = -0.03782715606003251\t\tdiff = 0.046560508113409504\n",
      " kl: 39.5110835075084\t\tkl_diff = -0.03689589805509286\t\tdiff = 0.04465166802192133\n",
      " kl: 39.47409225230771\t\tkl_diff = -0.036991255200689466\t\tdiff = 0.04314217811945387\n",
      " kl: 39.43587858416479\t\tkl_diff = -0.03821366814292304\t\tdiff = 0.041885660278735415\n",
      " kl: 39.39509103067532\t\tkl_diff = -0.04078755348947283\t\tdiff = 0.040908135562018444\n",
      " kl: 39.35012016075825\t\tkl_diff = -0.04497086991706567\t\tdiff = 0.0402774009791762\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Perform EM.\n",
    "\n",
    "model.params.set_free(init_par_vec)\n",
    "model.optimize_z()\n",
    "global_param_vec = model.params['global'].get_vector()\n",
    "kl = model.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = model.params['global'].get_free()\n",
    "\n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: model.kl_free_global(par, verbose=False),\n",
    "       x0=global_free_par, jac=model.kl_free_global_grad, hessp=model.kl_free_global_hvp,\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #    lambda par: model.global_kl_wrapper(par, verbose=False),\n",
    "    #    x0=global_free_par, jac=model.global_kl_grad, hess=model.global_kl_hessian,\n",
    "    #    method='trust-ncg', options={'maxiter': 50})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: model.global_kl_wrapper(par, verbose=False),\n",
    "    #   x0=global_free_par, method='nelder-mead', options={'maxiter': 500})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: model.global_kl_wrapper(par, verbose=False),\n",
    "    #  x0=global_free_par, method='bfgs', options={'maxiter': 50})\n",
    "    model.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    model.optimize_z()\n",
    "\n",
    "    new_global_param_vec = model.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = model.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = model.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.3501201608\n",
      "39.330611485\n",
      "39.3241650965\n",
      "39.2587946368\n",
      "39.2391470818\n",
      "39.1238272875\n",
      "39.0585393562\n",
      "39.0558843488\n",
      "38.8068466782\n",
      "38.7268642099\n",
      "38.7153348132\n",
      "38.5329977187\n",
      "37.7810221208\n",
      "37.5194120653\n",
      "37.4947279562\n",
      "37.4850944341\n",
      "35.9052393808\n",
      "35.2993122141\n",
      "35.204735649\n",
      "35.1855735601\n",
      "50.5103709927\n",
      "34.3124814265\n",
      "33.8708849342\n",
      "33.8662323466\n",
      "33.8633616531\n",
      "33.2635544809\n",
      "32.7721457319\n",
      "32.6204678923\n",
      "32.5606235305\n",
      "32.745120953\n",
      "32.4527199113\n",
      "32.4443104288\n",
      "32.4344778331\n",
      "32.2958869377\n",
      "32.2879146489\n",
      "32.2877750485\n",
      "32.2279252988\n",
      "32.227667879\n",
      "32.2060679487\n",
      "32.2060349267\n",
      "38.1941094041\n",
      "32.2027951112\n",
      "32.2014717615\n",
      "32.2012474319\n",
      "32.2020908859\n",
      "32.1998064906\n",
      "32.199582017\n",
      "32.1976308354\n",
      "32.1974003764\n",
      "32.1952537704\n",
      "32.1952403798\n",
      "done\n",
      "32.1952403798\n"
     ]
    }
   ],
   "source": [
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "# vb_opt = optimize.minimize(\n",
    "#     lambda par: model.kl_free(par, verbose=True),\n",
    "#     x0=em_free_par, jac=model.kl_free_grad, hessp=model.kl_free_hvp,\n",
    "#     method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "# print('Done')\n",
    "# model.params.set_free(vb_opt.x)\n",
    "\n",
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    lambda par: model.kl_free(par, verbose=True),\n",
    "    x0=em_free_par, jac=model.kl_free_grad, hess=model.kl_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(model.kl_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0169d000f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = model.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = model.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = model.kl_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n"
     ]
    }
   ],
   "source": [
    "weight_jac = model.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(model.x.shape[0]), rm_row)\n",
    "model.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(model.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(model.params['global'])\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_model = Model(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    }
   ],
   "source": [
    "init_par = rm_model.params.get_free()\n",
    "\n",
    "rm_model.kl_free(init_par)\n",
    "rm_model.kl_free_hessian_sparse(init_par)\n",
    "\n",
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "# rm_vb_opt = optimize.minimize(\n",
    "#     lambda par: rm_model.kl_free(par, verbose=True),\n",
    "#     x0=init_par, jac=rm_model.kl_free_grad, hessp=rm_model.kl_free_hvp,\n",
    "#     method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    lambda par: rm_model.kl_free(par, verbose=True),\n",
    "    x0=init_par, jac=rm_model.kl_free_grad, hess=rm_model.kl_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_model.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_model.get_interesting_moments(rm_vb_opt.x) - model.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
