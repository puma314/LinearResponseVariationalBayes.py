{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "\n",
    "# from VariationalBayes.ParameterDictionary import ModelParamsDict\n",
    "# from VariationalBayes import PosDefMatrixParam, PosDefMatrixParamVector\n",
    "# from VariationalBayes import SimplexParam\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 100\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng5TcRUQy6P8BW8aV6PctmoQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f577ab2d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.97103813  0.22991905]\n",
      " [ 0.74290491  0.59620491]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VariationalBayes.Parameters import convert_vector_to_free_hessian\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        # Autograd derivatives\n",
    "        self.kl_free_grad = autograd.grad(self.kl_free)\n",
    "        self.kl_free_hessian = autograd.hessian(self.kl_free) # This will be slow.\n",
    "        self.kl_free_hvp = autograd.hessian_vector_product(self.kl_free)\n",
    "\n",
    "        self.kl_free_global_grad = autograd.grad(self.kl_free_global)\n",
    "        self.kl_free_global_hessian = autograd.hessian(self.kl_free_global)\n",
    "        self.kl_free_global_hvp = autograd.hessian_vector_product(self.kl_free_global)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "        self.kl_vector_global_jac = autograd.jacobian(self.kl_vector_global_local, argnum=0)\n",
    "        self.kl_vector_global_hessian = autograd.hessian(self.kl_vector_global_local, argnum=0)\n",
    "        self.kl_vector_global_local_hessian = autograd.jacobian(self.kl_vector_global_jac, argnum=1)\n",
    "\n",
    "        self.kl_vector_jac = autograd.jacobian(self.kl_vector)\n",
    "        self.kl_vector_hessian = autograd.hessian(self.kl_vector)\n",
    "\n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def kl_vector_hessian_sparse(self):\n",
    "        global_vec = self.params['global'].get_vector()\n",
    "        local_vec = self.params['local'].get_vector()\n",
    "    \n",
    "        global_hess = self.kl_vector_global_hessian(global_vec, local_vec)\n",
    "        global_local_hess = self.kl_vector_global_local_hessian(global_vec, local_vec)\n",
    "        local_hess_sparse = self.kl_vector_local_hessian_sparse()\n",
    "        sp_hess =  sp.sparse.bmat([ [global_hess,         global_local_hess],\n",
    "                                    [global_local_hess.T, local_hess_sparse]])\n",
    "        return np.array(sp_hess.toarray())\n",
    "    \n",
    "\n",
    "    # This takes free_params as an argument so it can be used in optimization.\n",
    "    def kl_free_hessian_sparse(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        kl_vector_hessian_sparse = self.kl_vector_hessian_sparse()\n",
    "        kl_vector_jac = self.kl_vector_jac(self.params.get_vector())\n",
    "        kl_hessian_sparse = convert_vector_to_free_hessian(\n",
    "            self.params, free_params, kl_vector_jac, kl_vector_hessian_sparse)\n",
    "\n",
    "        # If you don't convert to an array, it returns a matrix type, which\n",
    "        # seems to cause mysterious problems with scipy.optimize.minimize.\n",
    "        return np.array(kl_hessian_sparse)\n",
    "\n",
    "    # Wrappers for autodiff follow. The nomeclature is\n",
    "    # {function}_{free | vector}_{|global|local}_{|sparse}\n",
    "\n",
    "    def kl_free(self, free_params, verbose=False):\n",
    "        self.params.set_free(free_params)\n",
    "        kl = self.kl()\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_free_global(self, global_free_params, verbose=False):\n",
    "        self.params['global'].set_free(global_free_params)\n",
    "        kl = self.kl(include_local_entropy=False)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_vector_global_local(self, global_vec_params, local_vec_params,\n",
    "                               verbose=False, include_local_entropy=True):\n",
    "        self.params['global'].set_vector(global_vec_params)\n",
    "        self.params['local'].set_vector(local_vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def kl_vector(self, vec_params, \n",
    "                  verbose=False, include_local_entropy=True):\n",
    "        self.params.set_vector(vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian:  0.6311047077178955\n",
      "Hessian vector product 0.019204139709472656\n"
     ]
    }
   ],
   "source": [
    "obj = Objective(x, params, prior_params)\n",
    "obj.optimize_z()\n",
    "\n",
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "obj.kl_free(free_par)\n",
    "\n",
    "grad = obj.kl_free_grad(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = obj.kl_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "grad = obj.kl_free_global_grad(global_free_par)\n",
    "hvp = obj.kl_free_global_hvp(global_free_par, grad)\n",
    "\n",
    "# Not as slow!  You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = obj.kl_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Hessian: ', sparse_hess_time)\n",
    "print('Hessian vector product', hvp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'e_z'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-126b4f2b8680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mloglik_obs_free_local_jac\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mget_loglik_obs_free_local_jac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfree_par_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfree_par_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloglik_vector_local_weight_hessian_sparse\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglik_vector_local_weight_hessian_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlikelihood_by_obs_free_local_jac_sparse\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglik_free_local_weight_hessian_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7b4ff1aeaf84>\u001b[0m in \u001b[0;36mloglik_vector_local_weight_hessian_sparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# This is the Hessian of the negative entropy, which enters the KL divergence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mz_row_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'e_z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mhess_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_lik_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rgiordan/Documents/git_repos/LinearResponseVariationalBayes.py/VariationalBayes/ParameterDictionary.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpush_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'e_z'"
     ]
    }
   ],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(obj.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = obj.params['global'].get_free()\n",
    "free_par_local = obj.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    obj.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    obj.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(obj.x.shape)\n",
    "print(likelihood_by_obs_free_local_jac_sparse.shape)\n",
    "print(loglik_vector_local_weight_hessian_sparse.shape)\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Compare the full and sparse Hessians\n",
    "    kl_vector_hessian = autograd.hessian(obj.kl_vector)\n",
    "    hessian = obj.kl_free_hessian(free_par) # Slow\n",
    "    vector_hessian = kl_vector_hessian(vec_par)  # Slow\n",
    "\n",
    "    # The slow full Hessian and sparse Hessian agree.\n",
    "    plt.matshow(sparse_hessian != 0)\n",
    "    assert np.max(np.abs(hessian - sparse_hessian)) < 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "obj.params.set_free(init_par_vec)\n",
    "obj.optimize_z()\n",
    "global_param_vec = obj.params['global'].get_vector()\n",
    "kl = obj.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = obj.params['global'].get_free()\n",
    "\n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: obj.kl_free_global(par, verbose=False),\n",
    "       x0=global_free_par, jac=obj.kl_free_global_grad, hessp=obj.kl_free_global_hvp,\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #    lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #    x0=global_free_par, jac=obj.global_kl_grad, hess=obj.global_kl_hessian,\n",
    "    #    method='trust-ncg', options={'maxiter': 50})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #   x0=global_free_par, method='nelder-mead', options={'maxiter': 500})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #  x0=global_free_par, method='bfgs', options={'maxiter': 50})\n",
    "    obj.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    obj.optimize_z()\n",
    "\n",
    "    new_global_param_vec = obj.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = obj.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = obj.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "# vb_opt = optimize.minimize(\n",
    "#     lambda par: obj.kl_free(par, verbose=True),\n",
    "#     x0=em_free_par, jac=obj.kl_free_grad, hessp=obj.kl_free_hvp,\n",
    "#     method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "# print('Done')\n",
    "# obj.params.set_free(vb_opt.x)\n",
    "\n",
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    lambda par: obj.kl_free(par, verbose=True),\n",
    "    x0=em_free_par, jac=obj.kl_free_grad, hess=obj.kl_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(obj.kl_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = obj.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = obj.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = obj.kl_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_jac = obj.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(obj.x.shape[0]), rm_row)\n",
    "obj.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(obj.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(obj.params['global'])\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_obj = Objective(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_par = rm_obj.params.get_free()\n",
    "\n",
    "rm_obj.kl_free(init_par)\n",
    "rm_obj.kl_free_hessian_sparse(init_par)\n",
    "\n",
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "# rm_vb_opt = optimize.minimize(\n",
    "#     lambda par: rm_obj.kl_free(par, verbose=True),\n",
    "#     x0=init_par, jac=rm_obj.kl_free_grad, hessp=rm_obj.kl_free_hvp,\n",
    "#     method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    lambda par: rm_obj.kl_free(par, verbose=True),\n",
    "    x0=init_par, jac=rm_obj.kl_free_grad, hess=rm_obj.kl_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_obj.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_obj.get_interesting_moments(rm_vb_opt.x) - obj.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
