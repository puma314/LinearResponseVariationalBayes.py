{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "from VariationalBayes.SparseObjectives import SparseObjective, Objective\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "#from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 1000\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbe2993f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.18971769  0.59204327]\n",
      " [ 0.90351638  0.29244379]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self, global_vec, local_vec):\n",
    "        self.params['global'].set_vector(global_vec)\n",
    "        self.params['local'].set_vector(local_vec)\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    # The SparseObjectives module still needs to support sparse Jacobians. \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, params, prior_params)\n",
    "model.optimize_z()\n",
    "\n",
    "kl_obj = SparseObjective(\n",
    "    model.params, model.kl,\n",
    "    fun_vector_local_hessian=model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "kl_obj_dense = Objective(model.params, model.kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun_vector_hessian_split: fun_vector_global_hessian:  0.048452138900756836\n",
      "fun_vector_hessian_split: fun_vector_cross_hessian:  0.03882789611816406\n",
      "fun_vector_hessian_split: bmat:  0.01848006248474121\n",
      "Parameters: free_to_vector_jac:  0.7521979808807373\n",
      "**  global <\n",
      "**  info <\n",
      "**  info calculating hessian:  0.038747549057006836\n",
      "**  info appending hessian:  0.0030438899993896484\n",
      "**  info >\n",
      "Dict  global free_to_vector_hess  info :  0.04302692413330078\n",
      "**  mu <\n",
      "**  mu calculating hessian:  0.00174713134765625\n",
      "**  mu appending hessian:  0.002146005630493164\n",
      "**  mu >\n",
      "Dict  global free_to_vector_hess  mu :  0.005010843276977539\n",
      "**  pi <\n",
      "**  pi calculating hessian:  0.0005741119384765625\n",
      "**  pi appending hessian:  0.0011796951293945312\n",
      "**  pi >\n",
      "Dict  global free_to_vector_hess  pi :  0.002803802490234375\n",
      "**  global calculating hessian:  0.05216836929321289\n",
      "**  global appending hessian:  0.004657268524169922\n",
      "**  global >\n",
      "Dict  mixture model free_to_vector_hess  global :  0.057880401611328125\n",
      "**  local <\n",
      "**  e_z <\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  e_z calculating hessian:  0.5093986988067627\n",
      "**  e_z appending hessian:  0.7651276588439941\n",
      "**  e_z >\n",
      "Dict  local free_to_vector_hess  e_z :  1.2775297164916992\n",
      "**  local calculating hessian:  1.2910704612731934\n",
      "**  local appending hessian:  0.8745269775390625\n",
      "**  local >\n",
      "Dict  mixture model free_to_vector_hess  local :  2.167734384536743\n",
      "Parameters: free_to_vector_hess:  2.242424726486206\n",
      "Parameters: accumulate:  0.26574063301086426\n",
      "Parameters: jac multiply:  0.031199932098388672\n",
      "convert_vector_to_free_hessian:  3.2938735485076904\n",
      "Sparse Hessian time: \t\t 3.4176313877105713\n",
      "Hessian vector product time:\t 0.016699790954589844\n"
     ]
    }
   ],
   "source": [
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "kl_obj.fun_free(free_par)\n",
    "grad = kl_obj.fun_free_grad_sparse(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = kl_obj.fun_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "local_free_par = params['local'].get_free()\n",
    "grad = kl_obj.fun_free_global_grad(global_free_par, local_free_par)\n",
    "hess = kl_obj.fun_free_global_hessian(global_free_par, local_free_par)\n",
    "\n",
    "# You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Sparse Hessian time: \\t\\t', sparse_hess_time)\n",
    "print('Hessian vector product time:\\t', hvp_time)\n",
    "\n",
    "if True:\n",
    "    dense_hess_time = time.time()\n",
    "    dense_hessian = kl_obj_dense.fun_free_hessian(free_par)\n",
    "    dense_hess_time = time.time() - dense_hess_time\n",
    "\n",
    "    print('Dense Hessian time: \\t\\t', dense_hess_time)\n",
    "    print('Difference: ', np.max(np.abs(dense_hessian - sparse_hessian)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "cProfile.run('sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)', 'hessian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats('hessian')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(model.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = model.params['global'].get_free()\n",
    "free_par_local = model.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    model.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    model.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "model.params.set_free(init_par_vec)\n",
    "model.optimize_z()\n",
    "global_param_vec = model.params['global'].get_vector()\n",
    "kl = model.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = model.params['global'].get_free()\n",
    "    local_free_par = model.params['local'].get_free()\n",
    "    \n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: kl_obj.fun_free_split(par, local_free_par),\n",
    "       x0=global_free_par,\n",
    "       jac=lambda par: kl_obj.fun_free_global_grad(par, local_free_par),\n",
    "       hess=lambda par: kl_obj.fun_free_global_hessian(par, local_free_par),\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    model.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    model.optimize_z()\n",
    "\n",
    "    new_global_param_vec = model.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = model.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = model.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    kl_obj.fun_free,\n",
    "    x0=em_free_par,\n",
    "    jac=kl_obj.fun_free_grad_sparse,\n",
    "    hess=kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(kl_obj.fun_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = model.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = model.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = kl_obj.fun_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_jac = model.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(model.x.shape[0]), rm_row)\n",
    "model.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(model.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(deepcopy(model.params['global']))\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_model = Model(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm_model.kl_free(init_par)\n",
    "# rm_model.kl_free_hessian_sparse(init_par)\n",
    "rm_kl_obj = SparseObjective(\n",
    "    rm_model.params, rm_model.kl,\n",
    "    fun_vector_local_hessian=rm_model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "init_par = rm_model.params.get_free()\n",
    "global_vec = rm_model.params['global'].get_vector()\n",
    "local_vec = rm_model.params['local'].get_vector()\n",
    "\n",
    "?print(rm_model.kl_vector_local_hessian_sparse(global_vec, local_vec))\n",
    "#print(rm_kl_obj.fun_vector_local_hessian(global_vec, local_vec))\n",
    "\n",
    "#print(rm_kl_obj.fun_free_hessian_sparse(init_par))\n",
    "rm_model.optimize_z()\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    rm_kl_obj.fun_free,\n",
    "    x0=init_par,\n",
    "    jac=rm_kl_obj.fun_free_grad_sparse,\n",
    "    hess=rm_kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_model.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_model.get_interesting_moments(rm_vb_opt.x) - model.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
