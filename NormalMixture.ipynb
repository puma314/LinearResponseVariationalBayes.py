{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "from VariationalBayes.SparseObjectives import SparseObjective, Objective\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "#from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 1000\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ba0a27e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.54974488  0.90890827]\n",
      " [ 0.90411055  0.19010624]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self, global_vec, local_vec):\n",
    "        self.params['global'].set_vector(global_vec)\n",
    "        self.params['local'].set_vector(local_vec)\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    # The SparseObjectives module still needs to support sparse Jacobians. \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, params, prior_params)\n",
    "model.optimize_z()\n",
    "\n",
    "kl_obj = SparseObjective(\n",
    "    model.params, model.kl,\n",
    "    fun_vector_local_hessian=model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "kl_obj_dense = Objective(model.params, model.kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun_vector_hessian_split: fun_vector_global_hessian:  0.05748152732849121\n",
      "fun_vector_hessian_split: fun_vector_cross_hessian:  4.476523160934448\n",
      "fun_vector_hessian_split: bmat:  0.020542383193969727\n",
      "fun_vector_hessian_split:  4.564345598220825\n",
      "fun_vector_grad_split:  0.00842595100402832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert_vector_to_free_hessian:  4.151432991027832\n",
      "Sparse Hessian time: \t\t 8.726191997528076\n",
      "Hessian vector product time:\t 0.016910791397094727\n",
      "Dense Hessian time: \t\t 9.654400110244751\n",
      "Difference:  1.7763568394e-15\n"
     ]
    }
   ],
   "source": [
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "kl_obj.fun_free(free_par)\n",
    "grad = kl_obj.fun_free_grad_sparse(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = kl_obj.fun_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "local_free_par = params['local'].get_free()\n",
    "grad = kl_obj.fun_free_global_grad(global_free_par, local_free_par)\n",
    "hess = kl_obj.fun_free_global_hessian(global_free_par, local_free_par)\n",
    "\n",
    "# You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Sparse Hessian time: \\t\\t', sparse_hess_time)\n",
    "print('Hessian vector product time:\\t', hvp_time)\n",
    "\n",
    "if True:\n",
    "    dense_hess_time = time.time()\n",
    "    dense_hessian = kl_obj_dense.fun_free_hessian(free_par)\n",
    "    dense_hess_time = time.time() - dense_hess_time\n",
    "\n",
    "    print('Dense Hessian time: \\t\\t', dense_hess_time)\n",
    "    print('Difference: ', np.max(np.abs(dense_hessian - sparse_hessian)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "cProfile.run('sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)', 'hessian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats('hessian')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(model.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = model.params['global'].get_free()\n",
    "free_par_local = model.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    model.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    model.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "model.params.set_free(init_par_vec)\n",
    "model.optimize_z()\n",
    "global_param_vec = model.params['global'].get_vector()\n",
    "kl = model.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = model.params['global'].get_free()\n",
    "    local_free_par = model.params['local'].get_free()\n",
    "    \n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: kl_obj.fun_free_split(par, local_free_par),\n",
    "       x0=global_free_par,\n",
    "       jac=lambda par: kl_obj.fun_free_global_grad(par, local_free_par),\n",
    "       hess=lambda par: kl_obj.fun_free_global_hessian(par, local_free_par),\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    model.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    model.optimize_z()\n",
    "\n",
    "    new_global_param_vec = model.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = model.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = model.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    kl_obj.fun_free,\n",
    "    x0=em_free_par,\n",
    "    jac=kl_obj.fun_free_grad_sparse,\n",
    "    hess=kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(kl_obj.fun_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = model.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = model.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = kl_obj.fun_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_jac = model.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(model.x.shape[0]), rm_row)\n",
    "model.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(model.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(deepcopy(model.params['global']))\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_model = Model(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm_model.kl_free(init_par)\n",
    "# rm_model.kl_free_hessian_sparse(init_par)\n",
    "rm_kl_obj = SparseObjective(\n",
    "    rm_model.params, rm_model.kl,\n",
    "    fun_vector_local_hessian=rm_model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "init_par = rm_model.params.get_free()\n",
    "global_vec = rm_model.params['global'].get_vector()\n",
    "local_vec = rm_model.params['local'].get_vector()\n",
    "\n",
    "?print(rm_model.kl_vector_local_hessian_sparse(global_vec, local_vec))\n",
    "#print(rm_kl_obj.fun_vector_local_hessian(global_vec, local_vec))\n",
    "\n",
    "#print(rm_kl_obj.fun_free_hessian_sparse(init_par))\n",
    "rm_model.optimize_z()\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    rm_kl_obj.fun_free,\n",
    "    x0=init_par,\n",
    "    jac=rm_kl_obj.fun_free_grad_sparse,\n",
    "    hess=rm_kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_model.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_model.get_interesting_moments(rm_vb_opt.x) - model.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
