{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "from VariationalBayes.SparseObjectives import SparseObjective, Objective\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "#from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 100\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAWyo+XP12k76AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ff3832da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.75746343  0.72390054]\n",
      " [ 0.7599709   0.42600357]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self, global_vec, local_vec):\n",
    "        self.params['global'].set_vector(global_vec)\n",
    "        self.params['local'].set_vector(local_vec)\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    # The SparseObjectives module still needs to support sparse Jacobians. \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, params, prior_params)\n",
    "model.optimize_z()\n",
    "\n",
    "kl_obj = SparseObjective(\n",
    "    model.params, model.kl,\n",
    "    fun_vector_local_hessian=model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "kl_obj_dense = Objective(model.params, model.kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7763568394e-15\n",
      "Hessian: \t\t 0.8536655902862549\n",
      "Dense Hessian: \t\t 0.880500316619873\n",
      "Hessian vector product:\t 0.019779205322265625\n"
     ]
    }
   ],
   "source": [
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "kl_obj.fun_free(free_par)\n",
    "grad = kl_obj.fun_free_grad_sparse(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = kl_obj.fun_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "local_free_par = params['local'].get_free()\n",
    "grad = kl_obj.fun_free_global_grad(global_free_par, local_free_par)\n",
    "hess = kl_obj.fun_free_global_hessian(global_free_par, local_free_par)\n",
    "\n",
    "# # Not as slow!  You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "dense_hess_time = time.time()\n",
    "dense_hessian = kl_obj_dense.fun_free_hessian(free_par)\n",
    "dense_hess_time = time.time() - dense_hess_time\n",
    "\n",
    "print(np.max(np.abs(dense_hessian - sparse_hessian)))\n",
    "\n",
    "print('Hessian: \\t\\t', sparse_hess_time)\n",
    "print('Dense Hessian: \\t\\t', dense_hess_time)\n",
    "print('Hessian vector product:\\t', hvp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.881784197e-16\n"
     ]
    }
   ],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(model.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = model.params['global'].get_free()\n",
    "free_par_local = model.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    model.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    model.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kl: 177.73975577648523\t\tkl_diff = -923.888522289046\t\tdiff = 3.5860713818052052\n",
      " kl: 96.50635192333931\t\tkl_diff = -81.23340385314592\t\tdiff = 1.1825714402582777\n",
      " kl: 66.87296400583382\t\tkl_diff = -29.633387917505488\t\tdiff = 1.9079951860544688\n",
      " kl: 55.8553176231289\t\tkl_diff = -11.017646382704925\t\tdiff = 1.3684021587515238\n",
      " kl: 30.4058812359336\t\tkl_diff = -25.4494363871953\t\tdiff = 2.5964883300952137\n",
      " kl: 26.246229828987744\t\tkl_diff = -4.159651406945855\t\tdiff = 4.604194059301868\n",
      " kl: 26.246229814678824\t\tkl_diff = -1.4308920270877934e-08\t\tdiff = 3.6200606771785715e-05\n",
      " kl: 26.246229814646057\t\tkl_diff = -3.276667825957702e-11\t\tdiff = 8.401816919967331e-06\n",
      " kl: 26.24622981464606\t\tkl_diff = 3.552713678800501e-15\t\tdiff = 1.7763568394002505e-15\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Perform EM.\n",
    "\n",
    "model.params.set_free(init_par_vec)\n",
    "model.optimize_z()\n",
    "global_param_vec = model.params['global'].get_vector()\n",
    "kl = model.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = model.params['global'].get_free()\n",
    "    local_free_par = model.params['local'].get_free()\n",
    "    \n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: kl_obj.fun_free_split(par, local_free_par),\n",
    "       x0=global_free_par,\n",
    "       jac=lambda par: kl_obj.fun_free_global_grad(par, local_free_par),\n",
    "       hess=lambda par: kl_obj.fun_free_global_hessian(par, local_free_par),\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    model.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    model.optimize_z()\n",
    "\n",
    "    new_global_param_vec = model.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = model.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = model.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "26.2462298146\n"
     ]
    }
   ],
   "source": [
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    kl_obj.fun_free,\n",
    "    x0=em_free_par,\n",
    "    jac=kl_obj.fun_free_grad_sparse,\n",
    "    hess=kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(kl_obj.fun_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntZwyoemqZ/QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ff15289e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = model.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = model.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = kl_obj.fun_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n"
     ]
    }
   ],
   "source": [
    "weight_jac = model.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(model.x.shape[0]), rm_row)\n",
    "model.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(model.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(deepcopy(model.params['global']))\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_model = Model(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# rm_model.kl_free(init_par)\n",
    "# rm_model.kl_free_hessian_sparse(init_par)\n",
    "rm_kl_obj = SparseObjective(\n",
    "    rm_model.params, rm_model.kl,\n",
    "    fun_vector_local_hessian=rm_model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "init_par = rm_model.params.get_free()\n",
    "global_vec = rm_model.params['global'].get_vector()\n",
    "local_vec = rm_model.params['local'].get_vector()\n",
    "\n",
    "?print(rm_model.kl_vector_local_hessian_sparse(global_vec, local_vec))\n",
    "#print(rm_kl_obj.fun_vector_local_hessian(global_vec, local_vec))\n",
    "\n",
    "#print(rm_kl_obj.fun_free_hessian_sparse(init_par))\n",
    "rm_model.optimize_z()\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    rm_kl_obj.fun_free,\n",
    "    x0=init_par,\n",
    "    jac=rm_kl_obj.fun_free_grad_sparse,\n",
    "    hess=rm_kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_model.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual sensitivity:\t [  1.33226763e-13   2.19380070e-13  -1.27068778e-03  -5.32078208e-03]\n",
      "Predicted sensitivity:\t [  1.11076640e-15   3.60559133e-14  -1.25451430e-03  -5.25425152e-03]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_model.get_interesting_moments(rm_vb_opt.x) - model.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
