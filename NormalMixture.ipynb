{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "from VariationalBayes.SparseObjectives import SparseObjective, Objective\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "#from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 1000\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36a94c7f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.5417693   0.39904969]\n",
      " [ 0.12209153  0.63869533]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self, global_vec, local_vec):\n",
    "        self.params['global'].set_vector(global_vec)\n",
    "        self.params['local'].set_vector(local_vec)\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    # The SparseObjectives module still needs to support sparse Jacobians. \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, params, prior_params)\n",
    "model.optimize_z()\n",
    "\n",
    "kl_obj = SparseObjective(\n",
    "    model.params, model.kl,\n",
    "    fun_vector_local_hessian=model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "kl_obj_dense = Objective(model.params, model.kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun_vector_hessian_split: fun_vector_global_hessian:  0.057195186614990234\n",
      "fun_vector_hessian_split: fun_vector_cross_hessian:  0.05001330375671387\n",
      "fun_vector_hessian_split: bmat:  0.01990365982055664\n",
      "Parameters: free_to_vector_jac:  0.8660507202148438\n",
      "**  global <\n",
      "**  info <\n",
      "**  info calculating hessian:  0.03485298156738281\n",
      "**  info appending hessian:  0.0003688335418701172\n",
      "**  info >\n",
      "Dict  global free_to_vector_hess  info :  0.03624773025512695\n",
      "**  mu <\n",
      "**  mu calculating hessian:  0.0012073516845703125\n",
      "**  mu appending hessian:  0.0001800060272216797\n",
      "**  mu >\n",
      "Dict  global free_to_vector_hess  mu :  0.002434968948364258\n",
      "**  pi <\n",
      "**  pi calculating hessian:  0.00031948089599609375\n",
      "**  pi appending hessian:  0.00010418891906738281\n",
      "**  pi >\n",
      "Dict  global free_to_vector_hess  pi :  0.001550912857055664\n",
      "**  global calculating hessian:  0.0415952205657959\n",
      "**  global appending hessian:  0.0005054473876953125\n",
      "**  global >\n",
      "Dict  mixture model free_to_vector_hess  global :  0.043088436126708984\n",
      "**  local "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "**  e_z <\n",
      "**  e_z calculating hessian:  0.2661135196685791\n",
      "**  e_z appending hessian:  0.09410858154296875\n",
      "**  e_z >\n",
      "Dict  local free_to_vector_hess  e_z :  0.36284971237182617\n",
      "**  local calculating hessian:  0.3777282238006592\n",
      "**  local appending hessian:  0.08742499351501465\n",
      "**  local >\n",
      "Dict  mixture model free_to_vector_hess  local :  0.4668242931365967\n",
      "Parameters: free_to_vector_hess:  0.5221564769744873\n",
      "Parameters: jacobian multiply:  0.03608131408691406\n",
      "convert_vector_to_free_hessian:  1.4434843063354492\n",
      "Sparse Hessian time: \t\t 1.5918221473693848\n",
      "Hessian vector product time:\t 0.017699480056762695\n"
     ]
    }
   ],
   "source": [
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "kl_obj.fun_free(free_par)\n",
    "grad = kl_obj.fun_free_grad_sparse(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = kl_obj.fun_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "local_free_par = params['local'].get_free()\n",
    "grad = kl_obj.fun_free_global_grad(global_free_par, local_free_par)\n",
    "hess = kl_obj.fun_free_global_hessian(global_free_par, local_free_par)\n",
    "\n",
    "# You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Sparse Hessian time: \\t\\t', sparse_hess_time)\n",
    "print('Hessian vector product time:\\t', hvp_time)\n",
    "\n",
    "if True:\n",
    "    dense_hess_time = time.time()\n",
    "    dense_hessian = kl_obj_dense.fun_free_hessian(free_par)\n",
    "    dense_hess_time = time.time() - dense_hess_time\n",
    "\n",
    "    print('Dense Hessian time: \\t\\t', dense_hess_time)\n",
    "    print('Difference: ', np.max(np.abs(dense_hessian - sparse_hessian)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "cProfile.run('sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)', 'hessian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats('hessian')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(model.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = model.params['global'].get_free()\n",
    "free_par_local = model.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    model.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    model.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "model.params.set_free(init_par_vec)\n",
    "model.optimize_z()\n",
    "global_param_vec = model.params['global'].get_vector()\n",
    "kl = model.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = model.params['global'].get_free()\n",
    "    local_free_par = model.params['local'].get_free()\n",
    "    \n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: kl_obj.fun_free_split(par, local_free_par),\n",
    "       x0=global_free_par,\n",
    "       jac=lambda par: kl_obj.fun_free_global_grad(par, local_free_par),\n",
    "       hess=lambda par: kl_obj.fun_free_global_hessian(par, local_free_par),\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    model.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    model.optimize_z()\n",
    "\n",
    "    new_global_param_vec = model.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = model.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = model.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    kl_obj.fun_free,\n",
    "    x0=em_free_par,\n",
    "    jac=kl_obj.fun_free_grad_sparse,\n",
    "    hess=kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(kl_obj.fun_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = model.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = model.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = kl_obj.fun_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_jac = model.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(model.x.shape[0]), rm_row)\n",
    "model.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(model.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(deepcopy(model.params['global']))\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_model = Model(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm_model.kl_free(init_par)\n",
    "# rm_model.kl_free_hessian_sparse(init_par)\n",
    "rm_kl_obj = SparseObjective(\n",
    "    rm_model.params, rm_model.kl,\n",
    "    fun_vector_local_hessian=rm_model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "init_par = rm_model.params.get_free()\n",
    "global_vec = rm_model.params['global'].get_vector()\n",
    "local_vec = rm_model.params['local'].get_vector()\n",
    "\n",
    "?print(rm_model.kl_vector_local_hessian_sparse(global_vec, local_vec))\n",
    "#print(rm_kl_obj.fun_vector_local_hessian(global_vec, local_vec))\n",
    "\n",
    "#print(rm_kl_obj.fun_free_hessian_sparse(init_par))\n",
    "rm_model.optimize_z()\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    rm_kl_obj.fun_free,\n",
    "    x0=init_par,\n",
    "    jac=rm_kl_obj.fun_free_grad_sparse,\n",
    "    hess=rm_kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_model.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_model.get_interesting_moments(rm_vb_opt.x) - model.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
