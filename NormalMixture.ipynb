{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "from VariationalBayes.SparseObjectives import SparseObjective, Objective\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "#from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 1000\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb6b06131d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.21489497  0.07101435]\n",
      " [ 0.22934328  0.38614283]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self, global_vec, local_vec):\n",
    "        self.params['global'].set_vector(global_vec)\n",
    "        self.params['local'].set_vector(local_vec)\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    # The SparseObjectives module still needs to support sparse Jacobians. \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, params, prior_params)\n",
    "model.optimize_z()\n",
    "\n",
    "kl_obj = SparseObjective(\n",
    "    model.params, model.kl,\n",
    "    fun_vector_local_hessian=model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "kl_obj_dense = Objective(model.params, model.kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: jacobian multiply:  0.03838515281677246\n",
      "convert_vector_to_free_hessian:  0.6146409511566162\n",
      "Sparse Hessian time: \t\t 0.7495052814483643\n",
      "Hessian vector product time:\t 0.024695396423339844\n",
      "Dense Hessian time: \t\t 9.836380243301392\n",
      "Difference:  2.27373675443e-13\n"
     ]
    }
   ],
   "source": [
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "kl_obj.fun_free(free_par)\n",
    "grad = kl_obj.fun_free_grad_sparse(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = kl_obj.fun_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "local_free_par = params['local'].get_free()\n",
    "grad = kl_obj.fun_free_global_grad(global_free_par, local_free_par)\n",
    "hess = kl_obj.fun_free_global_hessian(global_free_par, local_free_par)\n",
    "\n",
    "# You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Sparse Hessian time: \\t\\t', sparse_hess_time)\n",
    "print('Hessian vector product time:\\t', hvp_time)\n",
    "\n",
    "if True:\n",
    "    dense_hess_time = time.time()\n",
    "    dense_hessian = kl_obj_dense.fun_free_hessian(free_par)\n",
    "    dense_hess_time = time.time() - dense_hess_time\n",
    "\n",
    "    print('Dense Hessian time: \\t\\t', dense_hess_time)\n",
    "    print('Difference: ', np.max(np.abs(dense_hessian - sparse_hessian)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4a2885c34120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcProfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcProfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hessian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "cProfile.run('sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)', 'hessian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats('hessian')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(model.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = model.params['global'].get_free()\n",
    "free_par_local = model.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    model.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    model.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "model.params.set_free(init_par_vec)\n",
    "model.optimize_z()\n",
    "global_param_vec = model.params['global'].get_vector()\n",
    "kl = model.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = model.params['global'].get_free()\n",
    "    local_free_par = model.params['local'].get_free()\n",
    "    \n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: kl_obj.fun_free_split(par, local_free_par),\n",
    "       x0=global_free_par,\n",
    "       jac=lambda par: kl_obj.fun_free_global_grad(par, local_free_par),\n",
    "       hess=lambda par: kl_obj.fun_free_global_hessian(par, local_free_par),\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    model.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    model.optimize_z()\n",
    "\n",
    "    new_global_param_vec = model.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = model.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = model.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    kl_obj.fun_free,\n",
    "    x0=em_free_par,\n",
    "    jac=kl_obj.fun_free_grad_sparse,\n",
    "    hess=kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(kl_obj.fun_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = model.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = model.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = kl_obj.fun_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_jac = model.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(model.x.shape[0]), rm_row)\n",
    "model.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(model.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(deepcopy(model.params['global']))\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_model = Model(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm_model.kl_free(init_par)\n",
    "# rm_model.kl_free_hessian_sparse(init_par)\n",
    "rm_kl_obj = SparseObjective(\n",
    "    rm_model.params, rm_model.kl,\n",
    "    fun_vector_local_hessian=rm_model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "init_par = rm_model.params.get_free()\n",
    "global_vec = rm_model.params['global'].get_vector()\n",
    "local_vec = rm_model.params['local'].get_vector()\n",
    "\n",
    "?print(rm_model.kl_vector_local_hessian_sparse(global_vec, local_vec))\n",
    "#print(rm_kl_obj.fun_vector_local_hessian(global_vec, local_vec))\n",
    "\n",
    "#print(rm_kl_obj.fun_free_hessian_sparse(init_par))\n",
    "rm_model.optimize_z()\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    rm_kl_obj.fun_free,\n",
    "    x0=init_par,\n",
    "    jac=rm_kl_obj.fun_free_grad_sparse,\n",
    "    hess=rm_kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_model.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_model.get_interesting_moments(rm_vb_opt.x) - model.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
