{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "from VariationalBayes.SparseObjectives import SparseObjective, Objective\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "#from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 1000\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c35d78cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.56809315  0.10512912]\n",
      " [ 0.05843651  0.11259431]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self, global_vec, local_vec):\n",
    "        self.params['global'].set_vector(global_vec)\n",
    "        self.params['local'].set_vector(local_vec)\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    # The SparseObjectives module still needs to support sparse Jacobians. \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, params, prior_params)\n",
    "model.optimize_z()\n",
    "\n",
    "kl_obj = SparseObjective(\n",
    "    model.params, model.kl,\n",
    "    fun_vector_local_hessian=model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "kl_obj_dense = Objective(model.params, model.kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun_vector_hessian_split: fun_vector_global_hessian:  0.0492093563079834\n",
      "fun_vector_hessian_split: fun_vector_cross_hessian:  0.039598703384399414\n",
      "fun_vector_hessian_split: bmat:  0.01796269416809082\n",
      "Parameters: free_to_vector_jac:  0.6035351753234863\n",
      "Dict  global free_to_vector_hess  info :  0.04134535789489746\n",
      "Dict  global free_to_vector_hess  mu :  0.0013186931610107422\n",
      "Dict  global free_to_vector_hess  pi :  0.0004367828369140625\n",
      "Dict  mixture model free_to_vector_hess  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global :  0.04466819763183594\n",
      "Dict  local free_to_vector_hess  e_z :  0.33257436752319336\n",
      "Dict  mixture model free_to_vector_hess  local :  0.42754483222961426\n",
      "Parameters: free_to_vector_hess:  0.48346948623657227\n",
      "Parameters: jacobian multiply:  0.03094196319580078\n",
      "convert_vector_to_free_hessian:  1.1340582370758057\n",
      "Sparse Hessian time: \t\t 1.2578444480895996\n",
      "Hessian vector product time:\t 0.01674365997314453\n",
      "Dense Hessian time: \t\t 10.893482685089111\n",
      "Difference:  1.7763568394e-15\n"
     ]
    }
   ],
   "source": [
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "kl_obj.fun_free(free_par)\n",
    "grad = kl_obj.fun_free_grad_sparse(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = kl_obj.fun_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "local_free_par = params['local'].get_free()\n",
    "grad = kl_obj.fun_free_global_grad(global_free_par, local_free_par)\n",
    "hess = kl_obj.fun_free_global_hessian(global_free_par, local_free_par)\n",
    "\n",
    "# You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Sparse Hessian time: \\t\\t', sparse_hess_time)\n",
    "print('Hessian vector product time:\\t', hvp_time)\n",
    "\n",
    "if True:\n",
    "    dense_hess_time = time.time()\n",
    "    dense_hessian = kl_obj_dense.fun_free_hessian(free_par)\n",
    "    dense_hess_time = time.time() - dense_hess_time\n",
    "\n",
    "    print('Dense Hessian time: \\t\\t', dense_hess_time)\n",
    "    print('Difference: ', np.max(np.abs(dense_hessian - sparse_hessian)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4a2885c34120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcProfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcProfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hessian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "cProfile.run('sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)', 'hessian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats('hessian')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(model.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = model.params['global'].get_free()\n",
    "free_par_local = model.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    model.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    model.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "model.params.set_free(init_par_vec)\n",
    "model.optimize_z()\n",
    "global_param_vec = model.params['global'].get_vector()\n",
    "kl = model.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = model.params['global'].get_free()\n",
    "    local_free_par = model.params['local'].get_free()\n",
    "    \n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: kl_obj.fun_free_split(par, local_free_par),\n",
    "       x0=global_free_par,\n",
    "       jac=lambda par: kl_obj.fun_free_global_grad(par, local_free_par),\n",
    "       hess=lambda par: kl_obj.fun_free_global_hessian(par, local_free_par),\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    model.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    model.optimize_z()\n",
    "\n",
    "    new_global_param_vec = model.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = model.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = model.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    kl_obj.fun_free,\n",
    "    x0=em_free_par,\n",
    "    jac=kl_obj.fun_free_grad_sparse,\n",
    "    hess=kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(kl_obj.fun_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = model.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = model.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = kl_obj.fun_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_jac = model.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(model.x.shape[0]), rm_row)\n",
    "model.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(model.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(deepcopy(model.params['global']))\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_model = Model(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm_model.kl_free(init_par)\n",
    "# rm_model.kl_free_hessian_sparse(init_par)\n",
    "rm_kl_obj = SparseObjective(\n",
    "    rm_model.params, rm_model.kl,\n",
    "    fun_vector_local_hessian=rm_model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "init_par = rm_model.params.get_free()\n",
    "global_vec = rm_model.params['global'].get_vector()\n",
    "local_vec = rm_model.params['local'].get_vector()\n",
    "\n",
    "?print(rm_model.kl_vector_local_hessian_sparse(global_vec, local_vec))\n",
    "#print(rm_kl_obj.fun_vector_local_hessian(global_vec, local_vec))\n",
    "\n",
    "#print(rm_kl_obj.fun_free_hessian_sparse(init_par))\n",
    "rm_model.optimize_z()\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    rm_kl_obj.fun_free,\n",
    "    x0=init_par,\n",
    "    jac=rm_kl_obj.fun_free_grad_sparse,\n",
    "    hess=rm_kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_model.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_model.get_interesting_moments(rm_vb_opt.x) - model.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
