{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "from VariationalBayes.SparseObjectives import SparseObjective, Objective\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "#from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 1000\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f93766fef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.65507477  0.9948544 ]\n",
      " [ 0.88666462  0.22661001]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = vb.ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num)))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def loglik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "# TODO: put this in a library\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['local']['e_z'].get()\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.loglik_e_z)\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "    def loglik_obs_by_k(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        return loglik_obs_by_k(mu, info, pi, self.x)\n",
    "\n",
    "    # This needs to be defined so we can differentiate it for CAVI.\n",
    "    def loglik_e_z(self, e_z):\n",
    "        return np.sum(e_z * self.loglik_obs_by_k())\n",
    "\n",
    "    def loglik(self):\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return self.loglik_e_z(e_z)\n",
    "\n",
    "    def loglik_obs(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        return np.sum(log_lik_array * e_z, axis=1)    \n",
    "\n",
    "    def prior(self):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()        \n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        prior = 0.\n",
    "        prior += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        prior += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        prior += info_prior(info, self.prior_params['dof'].get())\n",
    "        return prior\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "\n",
    "        natural_parameters = self.get_z_nat_params(e_z)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['local']['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        elbo = self.prior() + self.loglik()\n",
    "\n",
    "        if include_local_entropy:\n",
    "            e_z = self.params['local']['e_z'].get()\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    # Moments for sensitivity\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    ######################################\n",
    "    # Compute sparse hessians by hand.\n",
    "\n",
    "    # Log likelihood by data point.\n",
    "    \n",
    "    # The rows are the z vector indices and the columns are the data points.\n",
    "    def loglik_vector_local_weight_hessian_sparse(self):\n",
    "        log_lik_array = self.loglik_obs_by_k()\n",
    "\n",
    "        hess_vals = [] # These will be the entries of dkl / dz dweight^T\n",
    "        hess_rows = [] # These will be the z indices\n",
    "        hess_cols = [] # These will be the data indices\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            z_row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(log_lik_array[row, col])\n",
    "                hess_rows.append(z_row_inds[col])\n",
    "                hess_cols.append(row)\n",
    "\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_cols)),\n",
    "                                     (local_size, self.x.shape[0]))\n",
    "\n",
    "    # KL\n",
    "    def kl_vector_local_hessian_sparse(self, global_vec, local_vec):\n",
    "        self.params['global'].set_vector(global_vec)\n",
    "        self.params['local'].set_vector(local_vec)\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        e_z = self.params['local']['e_z'].get()\n",
    "        for row in range(e_z.shape[0]):\n",
    "            # Note that we are relying on the fact that the local parameters\n",
    "            # only contain e_z, so the vector index in e_z is the vector index\n",
    "            # in the local parameters.\n",
    "            row_inds = self.params['local']['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        local_size = self.params['local']['e_z'].vector_size()\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (local_size, local_size))\n",
    "\n",
    "    ######################\n",
    "    # Everything below here should be boilerplate.\n",
    "    \n",
    "    # The SparseObjectives module still needs to support sparse Jacobians. \n",
    "    def loglik_free_local_weight_hessian_sparse(self):\n",
    "        free_par_local = self.params['local'].get_free()\n",
    "        free_to_vec_jac = self.params['local'].free_to_vector_jac(free_par_local) \n",
    "        return free_to_vec_jac .T * \\\n",
    "               self.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "    def loglik_free_weight_hessian_sparse(self):\n",
    "        get_loglik_obs_free_global_jac = \\\n",
    "            autograd.jacobian(self.loglik_obs_free_global_local, argnum=0)\n",
    "        loglik_obs_free_global_jac = \\\n",
    "            get_loglik_obs_free_global_jac(self.params['global'].get_free(),\n",
    "                                           self.params['local'].get_free()).T\n",
    "        loglik_obs_free_local_jac = \\\n",
    "            self.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "        return sp.sparse.vstack([ loglik_obs_free_global_jac, loglik_obs_free_local_jac ])\n",
    "    \n",
    "    def loglik_obs_free_global_local(self, free_params_global, free_params_local):\n",
    "        self.params['global'].set_free(free_params_global)\n",
    "        self.params['local'].set_free(free_params_local)\n",
    "        return self.loglik_obs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, params, prior_params)\n",
    "model.optimize_z()\n",
    "\n",
    "kl_obj = SparseObjective(\n",
    "    model.params, model.kl,\n",
    "    fun_vector_local_hessian=model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "kl_obj_dense = Objective(model.params, model.kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun_vector_hessian_split: fun_vector_global_hessian:  0.06303071975708008\n",
      "fun_vector_hessian_split: fun_vector_cross_hessian:  0.05479788780212402\n",
      "fun_vector_hessian_split: bmat:  0.016865015029907227\n",
      "fun_vector_hessian_split:  0.1438286304473877\n",
      "fun_vector_grad_split:  0.006960391998291016\n",
      "Parameters: free_to_vector_jac:  0.7495548725128174\n",
      "** calculating hessian:  0.060358285903930664\n",
      "** appending hessian:  0.006798267364501953\n",
      "Dict free_to_vector_hess  info :  0.06759023666381836\n",
      "** calculating hessian:  0.0031616687774658203\n",
      "** appending hessian:  0.003952503204345703\n",
      "Dict free_to_vector_hess  mu :  0.007658243179321289\n",
      "** calculating hessian:  0.0007843971252441406\n",
      "** appending hessian:  0.0019216537475585938\n",
      "Dict free_to_vector_hess  pi :  0.003215312957763672\n",
      "** calculating hessian:  0.07952570915222168\n",
      "** appending hessian:  0.013369083404541016\n",
      "Dict free_to_vector_hess  global :  0.09330201148986816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** calculating hessian:  0.6455967426300049\n",
      "** appending hessian:  1.4650421142578125\n",
      "Dict free_to_vector_hess  e_z :  2.1128995418548584\n",
      "** calculating hessian:  2.125199794769287\n",
      "** appending hessian:  1.3184161186218262\n",
      "Dict free_to_vector_hess  local :  3.444711446762085\n",
      "Parameters: free_to_vector_hess:  3.549910545349121\n"
     ]
    }
   ],
   "source": [
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "kl_obj.fun_free(free_par)\n",
    "grad = kl_obj.fun_free_grad_sparse(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = kl_obj.fun_free_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "local_free_par = params['local'].get_free()\n",
    "grad = kl_obj.fun_free_global_grad(global_free_par, local_free_par)\n",
    "hess = kl_obj.fun_free_global_hessian(global_free_par, local_free_par)\n",
    "\n",
    "# You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Sparse Hessian time: \\t\\t', sparse_hess_time)\n",
    "print('Hessian vector product time:\\t', hvp_time)\n",
    "\n",
    "if True:\n",
    "    dense_hess_time = time.time()\n",
    "    dense_hessian = kl_obj_dense.fun_free_hessian(free_par)\n",
    "    dense_hess_time = time.time() - dense_hess_time\n",
    "\n",
    "    print('Dense Hessian time: \\t\\t', dense_hess_time)\n",
    "    print('Difference: ', np.max(np.abs(dense_hessian - sparse_hessian)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "cProfile.run('sparse_hessian = kl_obj.fun_free_hessian_sparse(free_par)', 'hessian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats('hessian')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weight Jacobians.\n",
    "get_loglik_obs_free_local_jac = \\\n",
    "    autograd.jacobian(model.loglik_obs_free_global_local, argnum=1)\n",
    "\n",
    "free_par_global = model.params['global'].get_free()\n",
    "free_par_local = model.params['local'].get_free()\n",
    "\n",
    "\n",
    "loglik_obs_free_local_jac = \\\n",
    "    get_loglik_obs_free_local_jac(free_par_global, free_par_local)\n",
    "\n",
    "loglik_vector_local_weight_hessian_sparse = \\\n",
    "    model.loglik_vector_local_weight_hessian_sparse()\n",
    "\n",
    "likelihood_by_obs_free_local_jac_sparse = \\\n",
    "    model.loglik_free_local_weight_hessian_sparse()\n",
    "\n",
    "print(np.max(np.abs(loglik_obs_free_local_jac - likelihood_by_obs_free_local_jac_sparse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "model.params.set_free(init_par_vec)\n",
    "model.optimize_z()\n",
    "global_param_vec = model.params['global'].get_vector()\n",
    "kl = model.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = model.params['global'].get_free()\n",
    "    local_free_par = model.params['local'].get_free()\n",
    "    \n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: kl_obj.fun_free_split(par, local_free_par),\n",
    "       x0=global_free_par,\n",
    "       jac=lambda par: kl_obj.fun_free_global_grad(par, local_free_par),\n",
    "       hess=lambda par: kl_obj.fun_free_global_hessian(par, local_free_par),\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    model.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    model.optimize_z()\n",
    "\n",
    "    new_global_param_vec = model.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = model.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = model.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton is faster than CG if you go to high-quality optimum.\n",
    "vb_opt = optimize.minimize(\n",
    "    kl_obj.fun_free,\n",
    "    x0=em_free_par,\n",
    "    jac=kl_obj.fun_free_grad_sparse,\n",
    "    hess=kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('done')\n",
    "print(kl_obj.fun_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = model.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "moment_jac = model.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "kl_free_hessian_sparse = kl_obj.fun_free_hessian_sparse(vb_opt.x)\n",
    "sensitivity_operator = \\\n",
    "    sp.sparse.linalg.spsolve(csc_matrix(kl_free_hessian_sparse),\n",
    "                             csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_jac = model.loglik_free_weight_hessian_sparse()\n",
    "data_sens = (weight_jac.T * sensitivity_operator).toarray()\n",
    "print(data_sens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_row = 3\n",
    "keep_rows = np.setdiff1d(np.arange(model.x.shape[0]), rm_row)\n",
    "model.params.set_free(vb_opt.x)\n",
    "\n",
    "e_z_rm = vb.SimplexParam(name='e_z', shape=(n_num - 1, k_num))\n",
    "e_z_rm.set(model.params['local']['e_z'].get()[keep_rows, :])\n",
    "rm_local = vb.ModelParamsDict('local')\n",
    "rm_local.push_param(e_z_rm)\n",
    "\n",
    "rm_params = vb.ModelParamsDict('mixture model deleted row')\n",
    "rm_params.push_param(deepcopy(model.params['global']))\n",
    "rm_params.push_param(rm_local)\n",
    "\n",
    "rm_model = Model(x[keep_rows, :], rm_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm_model.kl_free(init_par)\n",
    "# rm_model.kl_free_hessian_sparse(init_par)\n",
    "rm_kl_obj = SparseObjective(\n",
    "    rm_model.params, rm_model.kl,\n",
    "    fun_vector_local_hessian=rm_model.kl_vector_local_hessian_sparse)\n",
    "\n",
    "init_par = rm_model.params.get_free()\n",
    "global_vec = rm_model.params['global'].get_vector()\n",
    "local_vec = rm_model.params['local'].get_vector()\n",
    "\n",
    "?print(rm_model.kl_vector_local_hessian_sparse(global_vec, local_vec))\n",
    "#print(rm_kl_obj.fun_vector_local_hessian(global_vec, local_vec))\n",
    "\n",
    "#print(rm_kl_obj.fun_free_hessian_sparse(init_par))\n",
    "rm_model.optimize_z()\n",
    "\n",
    "rm_vb_opt = optimize.minimize(\n",
    "    rm_kl_obj.fun_free,\n",
    "    x0=init_par,\n",
    "    jac=rm_kl_obj.fun_free_grad_sparse,\n",
    "    hess=rm_kl_obj.fun_free_hessian_sparse,\n",
    "    method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-8})\n",
    "\n",
    "print('Done')\n",
    "rm_model.params.set_free(rm_vb_opt.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Actual sensitivity:\\t', \n",
    "      rm_model.get_interesting_moments(rm_vb_opt.x) - model.get_interesting_moments(vb_opt.x))\n",
    "print('Predicted sensitivity:\\t', -1 * data_sens[rm_row, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
