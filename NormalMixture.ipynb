{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "# from VariationalBayes.ParameterDictionary import ModelParamsDict\n",
    "\n",
    "# from VariationalBayes import PosDefMatrixParam, PosDefMatrixParamVector\n",
    "# from VariationalBayes import SimplexParam\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 100\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4DM/B969Tpb7jaORQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f226a04f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.94498578  0.34365639]\n",
      " [ 0.4136796   0.04557085]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = \\\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num))\n",
    "\n",
    "single_local_params = \\\n",
    "    vb.SimplexParam(name='e_z', shape=(1, k_num), val=np.full((1, k_num), 1. / k_num))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "\n",
    "single_obs_params = vb.ModelParamsDict('mixture model single obs')\n",
    "single_obs_params.push_param(params['global'])\n",
    "single_obs_params.push_param(single_local_params)\n",
    "\n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def data_log_likelihood(mu, info, e_z, x, weights):\n",
    "    k_num = e_z.shape[1]\n",
    "    assert k_num == mu.shape[0]\n",
    "    assert k_num == info.shape[0]\n",
    "    log_lik = 0.0\n",
    "    # I would be interested to see how this could work without a loop.\n",
    "    e_z_weighted = weights * e_z\n",
    "    for k in range(k_num):\n",
    "        x_centered = x - np.expand_dims(mu[k, :], axis=0)\n",
    "        log_lik = log_lik - \\\n",
    "            0.5 * np.einsum('ni, ij, nj, n', \\\n",
    "                            x_centered, info[k, :, :], x_centered, e_z_weighted[:, k])\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        assert sign > 0\n",
    "        log_lik = log_lik + 0.5 * logdet * np.sum(e_z_weighted[:, k])\n",
    "    return log_lik\n",
    "\n",
    "def indicator_log_likelihood(e_z, pi):\n",
    "    return np.sum(np.matmul(e_z, np.log(pi.T)))\n",
    "\n",
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['e_z'].get()\n",
    "data_log_likelihood(true_mu, true_info, e_z, x, weights)\n",
    "indicator_log_likelihood(e_z, true_pi)\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        # Autograd derivatives\n",
    "        self.kl_grad = autograd.grad(self.kl_wrapper)\n",
    "        self.kl_hessian = autograd.hessian(self.kl_wrapper)\n",
    "        self.kl_hvp = autograd.hessian_vector_product(self.kl_wrapper)\n",
    "\n",
    "        self.global_kl_grad = autograd.grad(self.global_kl_wrapper)\n",
    "        self.global_kl_hvp = autograd.hessian_vector_product(self.global_kl_wrapper)\n",
    "        self.global_kl_hessian = autograd.hessian(self.global_kl_wrapper)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.expected_log_likelihood, argnum=0)\n",
    "\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "        self.get_global_vector_jacobian = autograd.jacobian(self.vector_kl_wrapper, argnum=0)\n",
    "        self.get_global_vector_hessian = autograd.hessian(self.vector_kl_wrapper, argnum=0)\n",
    "        self.get_global_local_vector_hessian = \\\n",
    "            autograd.jacobian(self.get_global_vector_jacobian, argnum=1)\n",
    "\n",
    "        self.get_vector_jacobian = autograd.jacobian(self.full_vector_kl_wrapper)\n",
    "        self.get_vector_hessian = autograd.hessian(self.full_vector_kl_wrapper)\n",
    "\n",
    "            \n",
    "    def expected_log_likelihood(self, e_z, mu, info, pi, weights):\n",
    "        elbo = 0.0\n",
    "\n",
    "        # Data:\n",
    "        elbo += data_log_likelihood(mu, info, e_z, self.x, weights)\n",
    "        elbo += indicator_log_likelihood(e_z, pi)\n",
    "        \n",
    "        # Priors:\n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        elbo += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        elbo += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        elbo += info_prior(info, self.prior_params['dof'].get())\n",
    "\n",
    "        return elbo\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['e_z'].get()\n",
    "\n",
    "        natural_parameters = obj.get_z_nat_params(e_z, mu, info, pi, self.weights)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['e_z'].get()\n",
    "\n",
    "        elbo = self.expected_log_likelihood(e_z, mu, info, pi, self.weights)\n",
    "        \n",
    "        if include_local_entropy:\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "\n",
    "    def kl_wrapper(self, free_params, verbose=False):\n",
    "        self.params.set_free(free_params)\n",
    "        kl = self.kl()\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def global_kl_wrapper(self, global_free_params, verbose=False):\n",
    "        self.params['global'].set_free(global_free_params)\n",
    "        kl = self.kl(include_local_entropy=False)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def vector_kl_wrapper(self, global_vec_params, local_vec_params,\n",
    "                          verbose=False, include_local_entropy=True):\n",
    "        self.params['global'].set_vector(global_vec_params)\n",
    "        self.params['e_z'].set_vector(local_vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def full_vector_kl_wrapper(self, vec_params, \n",
    "                               verbose=False, include_local_entropy=True):\n",
    "        self.params.set_vector(vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def get_sparse_local_vector_hessian(self, local_vec):\n",
    "        self.params['e_z'].set_vector(local_vec)\n",
    "        e_z = self.params['e_z'].get()\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            row_inds = self.params['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (len(local_vec), len(local_vec)))\n",
    "                \n",
    "    def get_sparse_vector_hessian(self, vec_params):\n",
    "        self.params.set_vector(vec_params)\n",
    "        global_vec = obj.params['global'].get_vector()\n",
    "        local_vec = obj.params['e_z'].get_vector()\n",
    "        global_vec_hess = obj.get_global_vector_hessian(global_vec, local_vec)\n",
    "        global_local_vec_hess = obj.get_global_local_vector_hessian(global_vec, local_vec)\n",
    "        local_vector_hessian = self.get_sparse_local_vector_hessian(local_vec)\n",
    "        return sp.sparse.bmat([ [global_vec_hess,         global_local_vec_hess],\n",
    "                                [global_local_vec_hess.T, local_vector_hessian]])\n",
    "\n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VariationalBayes.Parameters import convert_vector_to_free_hessian\n",
    "\n",
    "obj = Objective(x, params, prior_params)\n",
    "obj.optimize_z()\n",
    "\n",
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "obj.kl_wrapper(free_par)\n",
    "\n",
    "grad = obj.kl_grad(free_par)\n",
    "hvp = obj.kl_hvp(free_par, grad)\n",
    "\n",
    "grad = obj.global_kl_grad(global_free_par)\n",
    "hvp = obj.global_kl_hvp(global_free_par, grad)\n",
    "\n",
    "get_vector_hessian = autograd.hessian(obj.full_vector_kl_wrapper)\n",
    "\n",
    "hessian = obj.kl_hessian(free_par) # Slow\n",
    "vector_hessian = get_vector_hessian(vec_par)  # Slow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_vector_hessian = obj.get_sparse_vector_hessian(vec_par)\n",
    "vector_jac = obj.get_vector_jacobian(vec_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "(212, 212)\n",
      "(212, 212)\n",
      "(212,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f226803b710>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAECCAYAAAAGmJmkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJLWUC4WkIhcKSUUuFJKKXCgkFf0fIcoTuuJBQgMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2267b4fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.max(np.abs(vector_hessian - sparse_vector_hessian)))\n",
    "print(sparse_vector_hessian.shape)\n",
    "print(vector_hessian.shape)\n",
    "print(vector_jac.shape)\n",
    "\n",
    "free_hessian_sparse = convert_vector_to_free_hessian(\n",
    "    obj.params, free_par, vector_jac, vector_hessian)\n",
    "\n",
    "# The slow full Hessian and sparse Hessian agree.\n",
    "np.max(np.abs(hessian - free_hessian_sparse))\n",
    "\n",
    "plt.matshow(free_hessian_sparse != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_ind = params.free_indices_dict['e_z']\n",
    "z_free_indices = np.reshape(list(z_ind), params['e_z'].free_shape())\n",
    "\n",
    "global_ind = params.free_indices_dict['global']\n",
    "def log_likelihood(free_param):\n",
    "    params.set_free(free_param)\n",
    "    mu = params['global']['mu'].get()\n",
    "    info = params['global']['info'].get()\n",
    "    e_z = params['e_z'].get()\n",
    "    x = obj.x\n",
    "    weights = obj.weights\n",
    "    return data_log_likelihood(mu, info, e_z, x, weights)\n",
    "\n",
    "get_ll_hessian = autograd.hessian(log_likelihood)\n",
    "hess = get_ll_hessian(init_par_vec)\n",
    "ll = log_likelihood(init_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sparse_hess[np.ix_(global_ind, global_ind)] = hess[np.ix_(global_ind, global_ind)]\n",
    "# sparse_hess[np.ix_(global_ind, z_ind)] = hess[np.ix_(global_ind, z_ind)]\n",
    "# sparse_hess[np.ix_(z_ind, global_ind)] = hess[np.ix_(z_ind, global_ind)]\n",
    "sparse_hess = np.full((params.free_size(), params.free_size()), 0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "free_par = params.get_free()\n",
    "\n",
    "def single_row_log_likelihood(local_free_param, row):\n",
    "    single_obs_params.set_free(local_free_param)\n",
    "    mu = single_obs_params['global']['mu'].get()\n",
    "    info = single_obs_params['global']['info'].get()\n",
    "    e_z = single_obs_params['e_z'].get()\n",
    "    x = obj.x[row, :]\n",
    "    #return data_log_likelihood(mu, info, e_z, x[row, :], weights[row, :]) # slow\n",
    "\n",
    "    k_num = e_z.shape[1]\n",
    "    log_lik = 0.0\n",
    "\n",
    "    x_term = 0.\n",
    "    logdet = 0.\n",
    "    # I would be interested to see how this could work without a loop.\n",
    "    for k in range(k_num):\n",
    "        x_centered = x - mu[k, :]\n",
    "        x_term = np.matmul(x_centered, np.matmul(info[k, :, :], x_centered))\n",
    "        log_lik = log_lik - 0.5 * e_z[0, k] * x_term\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        log_lik = log_lik + 0.5 * logdet * e_z[0, k]\n",
    "    return log_lik\n",
    "\n",
    "\n",
    "single_row_log_likelihood(local_free_params, 3)\n",
    "single_row_log_likelihood_hess = autograd.hessian(single_row_log_likelihood)\n",
    "\n",
    "total_ll = 0.0\n",
    "sparse_hess_time = time.time()\n",
    "for row in range(n_num):\n",
    "#     z_row_ind = z_free_indices[row]\n",
    "#     row_ind = np.hstack([ global_ind, z_row_ind ])\n",
    "#     local_free_params = free_par[row_ind]\n",
    "#     total_ll += single_row_log_likelihood(local_free_params, row)\n",
    "    local_hess = single_row_log_likelihood_hess(local_free_params, row)\n",
    "\n",
    "#     single_obs_global_ind = single_obs_params.free_indices_dict['global']\n",
    "#     single_obs_z_ind = single_obs_params.free_indices_dict['e_z']\n",
    "\n",
    "#     sparse_hess[np.ix_(global_ind, global_ind)] += \\\n",
    "#         local_hess[np.ix_(single_obs_global_ind, single_obs_global_ind)]\n",
    "#     sparse_hess[np.ix_(global_ind, z_row_ind)] += \\\n",
    "#         local_hess[np.ix_(single_obs_global_ind, single_obs_z_ind)]\n",
    "#     sparse_hess[np.ix_(z_row_ind, global_ind)] += \\\n",
    "#         local_hess[np.ix_(single_obs_z_ind, single_obs_global_ind)]\n",
    "#     sparse_hess[np.ix_(z_row_ind, z_row_ind)] += \\\n",
    "#         local_hess[np.ix_(single_obs_z_ind, single_obs_z_ind)]\n",
    "\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "print(sparse_hess_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(sparse_hess != 0)\n",
    "plt.matshow(hess != 0)\n",
    "plt.matshow(np.abs(sparse_hess - hess) > 1e-6)\n",
    "\n",
    "print(ll)\n",
    "print(total_ll)\n",
    "print(np.max(np.abs(sparse_hess - hess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "obj.params.set_free(init_par_vec)\n",
    "obj.optimize_z()\n",
    "global_param_vec = obj.params['global'].get_vector()\n",
    "kl = obj.kl()\n",
    "\n",
    "for step in range(50):\n",
    "    global_free_par = obj.params['global'].get_free()\n",
    "\n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "       x0=global_free_par, jac=obj.global_kl_grad, hessp=obj.global_kl_hvp,\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #    lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #    x0=global_free_par, jac=obj.global_kl_grad, hess=obj.global_kl_hessian,\n",
    "    #    method='trust-ncg', options={'maxiter': 50})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #   x0=global_free_par, method='nelder-mead', options={'maxiter': 500})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #  x0=global_free_par, method='bfgs', options={'maxiter': 50})\n",
    "    obj.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    obj.optimize_z()\n",
    "\n",
    "    new_global_param_vec = obj.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = obj.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-8:\n",
    "        break\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "vb_opt = optimize.minimize(\n",
    "    lambda par: obj.kl_wrapper(par, verbose=True),\n",
    "    x0=obj.params.get_free(), jac=obj.kl_grad, hessp=obj.kl_hvp,\n",
    "    method='trust-ncg', options={'maxiter': 50})\n",
    "obj.params.set_free(vb_opt.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = obj.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is really slow for even problems of moderate size.  You'd want to build this up by hand if\n",
    "# you actualy needed it.\n",
    "\n",
    "# hess_time = time.time()\n",
    "# kl_hessian = obj.kl_hessian(obj.params.get_free())\n",
    "# hess_time = time.time() - hess_time\n",
    "# print(hess_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the sensitivity operator with conjugate gradient to avoid constructing the Hessian.\n",
    "\n",
    "moment_jac = obj.get_moment_jacobian(vb_opt.x)\n",
    "sensitivity_operator = np.full_like(moment_jac, float('nan'))\n",
    "free_param_size = len(vb_opt.x)\n",
    "\n",
    "KLHessVecProdLO = LinearOperator((free_param_size, free_param_size),\n",
    "                                 lambda vec: obj.kl_hvp(vb_opt.x, vec))\n",
    "for ind in range(sensitivity_operator.shape[0]):\n",
    "    cg_res, info = sp.sparse.linalg.cg(KLHessVecProdLO, moment_jac[ind, :].T)\n",
    "    sensitivity_operator[ind, :] = cg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj.params.set_free(vb_opt.x)\n",
    "def kl_weight_fun(weights):\n",
    "    obj.weights = weights\n",
    "    return obj.kl()\n",
    "\n",
    "default_weights = np.full((n_num, 1), 1.0)\n",
    "get_kl_weight_grad = autograd.grad(kl_weight_fun)\n",
    "kl_weight_grad = get_kl_weight_grad(default_weights)\n",
    "mu_weight_sens = np.matmul(sensitivity_operator, kl_weight_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
