{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "\n",
    "# from VariationalBayes.ParameterDictionary import ModelParamsDict\n",
    "# from VariationalBayes import PosDefMatrixParam, PosDefMatrixParamVector\n",
    "# from VariationalBayes import SimplexParam\n",
    "\n",
    "import math\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "#import copy\n",
    "from copy import deepcopy\n",
    "import scipy as sp\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points:\n",
    "n_num = 100\n",
    "\n",
    "# Dimension of observations:\n",
    "d_num = 2\n",
    "\n",
    "# Number of clusters:\n",
    "k_num = 2\n",
    "\n",
    "mu_scale = 3\n",
    "noise_scale = 0.5\n",
    "\n",
    "true_pi = np.linspace(0.2, 0.8, k_num)\n",
    "true_pi = true_pi / np.sum(true_pi)\n",
    "\n",
    "true_z = np.random.multinomial(1, true_pi, n_num)\n",
    "true_z_ind = np.full(n_num, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\n",
    "mu_prior_mean = np.full(d_num, 0.)\n",
    "mu_prior_cov = np.diag(np.full(d_num, mu_scale ** 2))\n",
    "mu_prior_info = np.linalg.inv(mu_prior_cov)\n",
    "true_mu = np.random.multivariate_normal(mu_prior_mean, mu_prior_cov, k_num)\n",
    "\n",
    "true_sigma = np.array([ np.diag(np.full(d_num, noise_scale ** 2)) + np.full((d_num, d_num), 0.1) \\\n",
    "                        for k in range(k_num) ])\n",
    "true_info = np.array([ np.linalg.inv(true_sigma[k, :, :]) for k in range(k_num) ])\n",
    "\n",
    "x = np.array([ np.random.multivariate_normal(true_mu[true_z_ind[n]], true_sigma[true_z_ind[n]]) \\\n",
    "               for n in range(n_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1db51bad68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Never a bad idea to visualize the dataz\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global:\n",
      "\tinfo:\n",
      "[[[ 1.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  1.]]]\n",
      "\tmu:\n",
      "[[ 0.24313251  0.20012172]\n",
      " [ 0.72277832  0.11224076]]\n",
      "\tpi: [[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "global_params = vb.ModelParamsDict('global')\n",
    "\n",
    "global_params.push_param(\n",
    "    vb.PosDefMatrixParamVector(name='info', length=k_num, matrix_size=d_num))\n",
    "global_params.push_param(\n",
    "    vb.ArrayParam(name='mu', shape=(k_num, d_num)))\n",
    "global_params.push_param(\n",
    "    vb.SimplexParam(name='pi', shape=(1, k_num)))\n",
    "\n",
    "local_params = \\\n",
    "    vb.SimplexParam(name='e_z', shape=(n_num, k_num),\n",
    "                    val=np.full(true_z.shape, 1. / k_num))\n",
    "\n",
    "single_local_params = \\\n",
    "    vb.SimplexParam(name='e_z', shape=(1, k_num), val=np.full((1, k_num), 1. / k_num))\n",
    "\n",
    "params = vb.ModelParamsDict('mixture model')\n",
    "params.push_param(global_params)\n",
    "params.push_param(local_params)\n",
    "\n",
    "true_init = False\n",
    "if true_init:\n",
    "    params['global']['info'].set(true_info)\n",
    "    params['global']['mu'].set(true_mu)\n",
    "    params['global']['pi'].set(true_pi)\n",
    "else:\n",
    "    params['global']['mu'].set(np.random.random(params['global']['mu'].shape()))\n",
    "    \n",
    "\n",
    "single_obs_params = vb.ModelParamsDict('mixture model single obs')\n",
    "single_obs_params.push_param(params['global'])\n",
    "single_obs_params.push_param(single_local_params)\n",
    "\n",
    "init_par_vec = params.get_free()\n",
    "\n",
    "print(params['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = vb.ModelParamsDict()\n",
    "prior_params.push_param(vb.VectorParam(name='mu_prior_mean', size=d_num, val=mu_prior_mean))\n",
    "prior_params.push_param(vb.PosDefMatrixParam(name='mu_prior_info', size=d_num, val=mu_prior_info))\n",
    "prior_params.push_param(vb.ScalarParam(name='alpha', val=2.0))\n",
    "prior_params.push_param(vb.ScalarParam(name='dof', val=d_num + 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_logdet_array(info):\n",
    "    return np.array([ np.linalg.slogdet(info[k, :, :])[1] for k in range(info.shape[0]) ])\n",
    "\n",
    "# This is the log probability of each observation for each component.\n",
    "def data_log_lik_obs_by_k(mu, info, pi, x):\n",
    "    log_lik = \\\n",
    "        -0.5 * np.einsum('ni, kij, nj -> nk', x, info, x) + \\\n",
    "               np.einsum('ni, kij, kj -> nk', x, info, mu) + \\\n",
    "        -0.5 * np.expand_dims(np.einsum('ki, kij, kj -> k', mu, info, mu), axis=0)\n",
    "\n",
    "    logdet_array = np.expand_dims(get_info_logdet_array(info), axis=0)\n",
    "    log_pi = np.log(pi)\n",
    "\n",
    "    log_lik += 0.5 * logdet_array + log_pi\n",
    "    \n",
    "    return log_lik\n",
    "\n",
    "mu = global_params['mu'].get()\n",
    "info = global_params['info'].get()\n",
    "pi = global_params['pi'].get()\n",
    "\n",
    "log_lik_array = data_log_lik_obs_by_k(mu, info, pi, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def data_log_likelihood(mu, info, e_z, pi, x):\n",
    "    k_num = e_z.shape[1]\n",
    "    assert k_num == mu.shape[0]\n",
    "    assert k_num == info.shape[0]\n",
    "\n",
    "    log_lik_array = data_log_lik_obs_by_k(mu, info, pi, x)\n",
    "    return np.sum(e_z * log_lik_array)\n",
    "\n",
    "def mu_prior(mu, mu_prior_mean, mu_prior_info):\n",
    "    k_num = mu.shape[0]\n",
    "    d_num = len(mu_prior_mean)\n",
    "    assert mu.shape[1] == d_num\n",
    "    assert mu_prior_info.shape[0] == d_num\n",
    "    assert mu_prior_info.shape[1] == d_num\n",
    "    mu_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        mu_centered = mu[k, :] - mu_prior_mean\n",
    "        mu_prior_val += -0.5 * np.matmul(np.matmul(mu_centered, mu_prior_info), mu_centered)\n",
    "    return mu_prior_val\n",
    "    \n",
    "def pi_prior(pi, alpha):\n",
    "    return np.sum(alpha * np.log(pi))\n",
    "\n",
    "def info_prior(info, dof):\n",
    "    k_num = info.shape[0]\n",
    "    d_num = info.shape[1]\n",
    "    assert d_num == info.shape[2]\n",
    "    assert dof > d_num - 1\n",
    "    # Not a complete Wishart prior\n",
    "    # TODO: cache the log determinants.\n",
    "    info_prior_val = 0.0\n",
    "    for k in range(k_num):\n",
    "        sign, logdet = np.linalg.slogdet(info[k, :, :])\n",
    "        info_prior_val += 0.5 * (dof - d_num - 1) * logdet\n",
    "    return info_prior_val\n",
    "\n",
    "def multinoulli_entropy(e_z):\n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n",
    "\n",
    "def get_sparse_multinoulli_entropy_hessian(e_z_vec):\n",
    "    k = len(e_z_vec)\n",
    "    vals = -1. / e_z_vec\n",
    "    return sp.sparse.csr_matrix((vals, ((range(k)), (range(k)))), (k, k))\n",
    "\n",
    "weights = np.full((n_num, 1), 1.0)\n",
    "e_z = params['e_z'].get()\n",
    "data_log_likelihood(true_mu, true_info, e_z, pi, x)\n",
    "mu_prior(true_mu, mu_prior_mean, mu_prior_info)\n",
    "pi_prior(true_pi, 2.0)\n",
    "info_prior(true_info, d_num + 2)\n",
    "multinoulli_entropy(e_z)\n",
    "\n",
    "get_multinoulli_entropy_hessian = autograd.hessian(multinoulli_entropy)\n",
    "e_z0 = e_z[0, :]\n",
    "\n",
    "print(np.max(np.abs(\n",
    "    get_multinoulli_entropy_hessian(e_z0) - get_sparse_multinoulli_entropy_hessian(e_z0).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from VariationalBayes.Parameters import convert_vector_to_free_hessian\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, x, params, prior_params):\n",
    "        self.x = x\n",
    "        self.params = deepcopy(params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "\n",
    "        # Autograd derivatives\n",
    "        self.kl_grad = autograd.grad(self.kl_wrapper)\n",
    "        self.kl_hessian = autograd.hessian(self.kl_wrapper)\n",
    "        self.kl_hvp = autograd.hessian_vector_product(self.kl_wrapper)\n",
    "\n",
    "        self.global_kl_grad = autograd.grad(self.global_kl_wrapper)\n",
    "        self.global_kl_hvp = autograd.hessian_vector_product(self.global_kl_wrapper)\n",
    "        self.global_kl_hessian = autograd.hessian(self.global_kl_wrapper)\n",
    "\n",
    "        self.get_z_nat_params = autograd.grad(self.expected_log_likelihood, argnum=0)\n",
    "\n",
    "        self.get_moment_jacobian = autograd.jacobian(self.get_interesting_moments)\n",
    "        \n",
    "        self.get_global_vector_jacobian = autograd.jacobian(self.vector_kl_wrapper, argnum=0)\n",
    "        self.get_global_vector_hessian = autograd.hessian(self.vector_kl_wrapper, argnum=0)\n",
    "        self.get_global_local_vector_hessian = \\\n",
    "            autograd.jacobian(self.get_global_vector_jacobian, argnum=1)\n",
    "\n",
    "        self.get_vector_jacobian = autograd.jacobian(self.full_vector_kl_wrapper)\n",
    "        self.get_vector_hessian = autograd.hessian(self.full_vector_kl_wrapper)\n",
    "\n",
    "            \n",
    "    def expected_log_likelihood(self, e_z, mu, info, pi, weights):\n",
    "        elbo = 0.0\n",
    "\n",
    "        # Data:\n",
    "        elbo += data_log_likelihood(mu, info, e_z, pi, self.x)\n",
    "        \n",
    "        # Priors:\n",
    "        mu_prior_mean = self.prior_params['mu_prior_mean'].get()\n",
    "        mu_prior_info = self.prior_params['mu_prior_info'].get()\n",
    "        elbo += mu_prior(mu, mu_prior_mean, mu_prior_info)\n",
    "        elbo += pi_prior(pi, self.prior_params['alpha'].get())\n",
    "        elbo += info_prior(info, self.prior_params['dof'].get())\n",
    "\n",
    "        return elbo\n",
    "    \n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['e_z'].get()\n",
    "\n",
    "        natural_parameters = obj.get_z_nat_params(e_z, mu, info, pi, self.weights)\n",
    "        z_logsumexp = np.expand_dims(sp.misc.logsumexp(natural_parameters, 1), axis=1)\n",
    "        e_z = np.exp(natural_parameters - z_logsumexp)\n",
    "        self.params['e_z'].set(e_z)\n",
    "    \n",
    "    def kl(self, include_local_entropy=True):\n",
    "        info = self.params['global']['info'].get()\n",
    "        mu = self.params['global']['mu'].get()\n",
    "        pi = self.params['global']['pi'].get()\n",
    "        e_z = self.params['e_z'].get()\n",
    "\n",
    "        elbo = self.expected_log_likelihood(e_z, mu, info, pi, self.weights)\n",
    "        \n",
    "        if include_local_entropy:\n",
    "            elbo += multinoulli_entropy(e_z)\n",
    "        \n",
    "        return -1 * elbo\n",
    "    \n",
    "    def get_interesting_moments(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        return self.params['global']['mu'].get_vector()\n",
    "\n",
    "    # Wrappers for autodiff follow.\n",
    "    def kl_wrapper(self, free_params, verbose=False):\n",
    "        self.params.set_free(free_params)\n",
    "        kl = self.kl()\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def global_kl_wrapper(self, global_free_params, verbose=False):\n",
    "        self.params['global'].set_free(global_free_params)\n",
    "        kl = self.kl(include_local_entropy=False)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def vector_kl_wrapper(self, global_vec_params, local_vec_params,\n",
    "                          verbose=False, include_local_entropy=True):\n",
    "        self.params['global'].set_vector(global_vec_params)\n",
    "        self.params['e_z'].set_vector(local_vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def full_vector_kl_wrapper(self, vec_params, \n",
    "                               verbose=False, include_local_entropy=True):\n",
    "        self.params.set_vector(vec_params)\n",
    "        kl = self.kl(include_local_entropy=include_local_entropy)\n",
    "        if verbose:\n",
    "            print(kl)\n",
    "        return kl\n",
    "\n",
    "    def get_sparse_local_vector_hessian(self, local_vec):\n",
    "        self.params['e_z'].set_vector(local_vec)\n",
    "        e_z = self.params['e_z'].get()\n",
    "        hess_vals = []\n",
    "        hess_rows = []\n",
    "        # This is the Hessian of the negative entropy, which enters the KL divergence.\n",
    "        for row in range(e_z.shape[0]):\n",
    "            row_inds = self.params['e_z'].get_vector_indices(row)\n",
    "            for col in range(e_z.shape[1]):\n",
    "                hess_vals.append(1. / e_z[row, col])\n",
    "                hess_rows.append(row_inds[col])\n",
    "        return sp.sparse.csr_matrix((hess_vals, (hess_rows, hess_rows)),\n",
    "                                    (len(local_vec), len(local_vec)))\n",
    "                \n",
    "    def get_sparse_vector_hessian(self, vec_params):\n",
    "        self.params.set_vector(vec_params)\n",
    "        global_vec = obj.params['global'].get_vector()\n",
    "        local_vec = obj.params['e_z'].get_vector()\n",
    "        global_vec_hess = obj.get_global_vector_hessian(global_vec, local_vec)\n",
    "        global_local_vec_hess = obj.get_global_local_vector_hessian(global_vec, local_vec)\n",
    "        local_vector_hessian = self.get_sparse_local_vector_hessian(local_vec)\n",
    "        sp_hess =  sp.sparse.bmat([ [global_vec_hess,         global_local_vec_hess],\n",
    "                                    [global_local_vec_hess.T, local_vector_hessian]])\n",
    "        return np.array(sp_hess.toarray())\n",
    "    \n",
    "    def get_sparse_hessian(self, free_params):\n",
    "        self.params.set_free(free_params)\n",
    "        vec_par = self.params.get_vector()\n",
    "        sparse_vector_hessian = self.get_sparse_vector_hessian(vec_par)\n",
    "        vector_jac = self.get_vector_jacobian(vec_par)\n",
    "        # If you don't convert to an array, it returns a matrix type, which\n",
    "        # seems to cause mysterious problems with scipy.optimize.minimize.\n",
    "\n",
    "        sparse_hessian = convert_vector_to_free_hessian(\n",
    "            self.params, free_params, vector_jac, sparse_vector_hessian)\n",
    "        return np.array(sparse_hessian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/autograd/autograd/core.py:16: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian:  0.45551490783691406\n",
      "Hessian vector product 0.01858973503112793\n"
     ]
    }
   ],
   "source": [
    "obj = Objective(x, params, prior_params)\n",
    "obj.optimize_z()\n",
    "\n",
    "free_par = params.get_free()\n",
    "vec_par = params.get_vector()\n",
    "\n",
    "global_free_par = params['global'].get_free()\n",
    "obj.kl_wrapper(free_par)\n",
    "\n",
    "grad = obj.kl_grad(free_par)\n",
    "\n",
    "hvp_time = time.time()\n",
    "hvp = obj.kl_hvp(free_par, grad)\n",
    "hvp_time = time.time() - hvp_time\n",
    "\n",
    "grad = obj.global_kl_grad(global_free_par)\n",
    "hvp = obj.global_kl_hvp(global_free_par, grad)\n",
    "\n",
    "# Not as slow!  You can ignore the autograd warning.\n",
    "sparse_hess_time = time.time()\n",
    "sparse_hessian = obj.get_sparse_hessian(free_par)\n",
    "sparse_hess_time = time.time() - sparse_hess_time\n",
    "\n",
    "print('Hessian: ', sparse_hess_time)\n",
    "print('Hessian vector product', hvp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4408920985e-16\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAECCAYAAAAGmJmkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJLWUC4WkIhcKSUUuFJKKXCgkFf0fIcoTuuJBQgMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1de3fe2588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if False:\n",
    "    # Compare the full and sparse Hessians\n",
    "    get_vector_hessian = autograd.hessian(obj.full_vector_kl_wrapper)\n",
    "    hessian = obj.kl_hessian(free_par) # Slow\n",
    "    vector_hessian = get_vector_hessian(vec_par)  # Slow\n",
    "\n",
    "    # The slow full Hessian and sparse Hessian agree.\n",
    "    plt.matshow(sparse_hessian != 0)\n",
    "    print(np.max(np.abs(hessian - sparse_hessian)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import cProfile, pstats\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "    param = obj.params\n",
    "    free_val = free_par\n",
    "\n",
    "    #cProfile.run('obj.get_sparse_hessian(free_par)', 'prof_hess')\n",
    "    cProfile.run('param.free_to_vector_hess(free_val)', 'prof_hess')\n",
    "\n",
    "    param.set_free(free_val)\n",
    "    vec_par = param.get_vector()\n",
    "\n",
    "    print('0')\n",
    "    tic = time.time()\n",
    "    vector_hess = obj.get_sparse_vector_hessian(vec_par)\n",
    "    print(time.time() - tic)\n",
    "\n",
    "    print('1')\n",
    "    tic = time.time()\n",
    "    vector_jac = obj.get_vector_jacobian(vec_par)\n",
    "    print(time.time() - tic)\n",
    "\n",
    "    print('2')\n",
    "    tic = time.time()\n",
    "    vector_jac = obj.get_vector_jacobian(vec_par)\n",
    "    print(time.time() - tic)\n",
    "\n",
    "    print('3')\n",
    "    tic = time.time()\n",
    "    free_hess = csr_matrix((param.free_size(), param.free_size()))\n",
    "    print(time.time() - tic)\n",
    "\n",
    "    print('4')\n",
    "    tic = time.time()\n",
    "    free_to_vec_jacobian = param.free_to_vector_jac(free_val)\n",
    "    print(time.time() - tic)\n",
    "\n",
    "    print('5')\n",
    "    tic = time.time()\n",
    "    free_to_vec_hessian = param.free_to_vector_hess(free_val)\n",
    "    print(time.time() - tic)\n",
    "\n",
    "    # Accumulate the third order terms, which are sparse.\n",
    "    print('6')\n",
    "    tic = time.time()\n",
    "    for vec_ind in range(param.vector_size()):\n",
    "        free_hess += free_to_vec_hessian[vec_ind] * vector_jac[vec_ind]\n",
    "    print(time.time() - tic)\n",
    "\n",
    "    # Then add the second-order terms, which may be dense depending on the\n",
    "    # vec_hess_target.\n",
    "    print('7')\n",
    "    tic = time.time()\n",
    "    free_hess += \\\n",
    "        free_to_vec_jacobian.T * vector_hess * free_to_vec_jacobian\n",
    "    print(time.time() - tic)\n",
    "\n",
    "\n",
    "    import pstats\n",
    "    prof_hess = pstats.Stats('prof_hess')\n",
    "    prof_hess.strip_dirs().sort_stats('cumulative').print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EM.\n",
    "\n",
    "obj.params.set_free(init_par_vec)\n",
    "obj.optimize_z()\n",
    "global_param_vec = obj.params['global'].get_vector()\n",
    "kl = obj.kl()\n",
    "\n",
    "for step in range(20):\n",
    "    global_free_par = obj.params['global'].get_free()\n",
    "\n",
    "    # Different choices for the M step:\n",
    "    global_vb_opt = optimize.minimize(\n",
    "       lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "       x0=global_free_par, jac=obj.global_kl_grad, hessp=obj.global_kl_hvp,\n",
    "       method='trust-ncg', options={'maxiter': 50, 'gtol': 1e-2})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #    lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #    x0=global_free_par, jac=obj.global_kl_grad, hess=obj.global_kl_hessian,\n",
    "    #    method='trust-ncg', options={'maxiter': 50})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #   x0=global_free_par, method='nelder-mead', options={'maxiter': 500})\n",
    "    #global_vb_opt = optimize.minimize(\n",
    "    #   lambda par: obj.global_kl_wrapper(par, verbose=False),\n",
    "    #  x0=global_free_par, method='bfgs', options={'maxiter': 50})\n",
    "    obj.params['global'].set_free(global_vb_opt.x)\n",
    "\n",
    "    # E-step:\n",
    "    obj.optimize_z()\n",
    "\n",
    "    new_global_param_vec = obj.params['global'].get_vector()\n",
    "    diff = np.max(np.abs(new_global_param_vec - global_param_vec))\n",
    "    global_param_vec = deepcopy(new_global_param_vec)\n",
    "    \n",
    "    new_kl = obj.kl()\n",
    "    kl_diff = new_kl - kl\n",
    "    kl = new_kl\n",
    "    print(' kl: {}\\t\\tkl_diff = {}\\t\\tdiff = {}'.format(kl, kl_diff, diff))\n",
    "    if diff < 1e-6:\n",
    "        break\n",
    "\n",
    "em_free_par = obj.params.get_free()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish with one joint Newton optimization to ensure global optimality.\n",
    "vb_opt = optimize.minimize(\n",
    "    lambda par: obj.kl_wrapper(par, verbose=True),\n",
    "    x0=em_free_par, jac=obj.kl_grad, hessp=obj.kl_hvp,\n",
    "    method='trust-ncg', options={'maxiter': 50})\n",
    "\n",
    "print('Done')\n",
    "obj.params.set_free(vb_opt.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # This isn't any faster, though with the sparse Hessian it's at least now comparable.\n",
    "    vb_opt_hess = optimize.minimize(\n",
    "        lambda par: obj.kl_wrapper(par, verbose=True),\n",
    "        x0=em_free_par, jac=obj.kl_grad, hess=obj.get_sparse_hessian,\n",
    "        method='trust-ncg', options={'maxiter': 50})\n",
    "\n",
    "    print('done')\n",
    "    print(obj.kl_wrapper(vb_opt_hess.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the solution looks sensible.\n",
    "mu_fit = obj.params['global']['mu'].get()\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_num):\n",
    "    plt.plot(mu_fit[k, 0], mu_fit[k, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "moment_jac = obj.get_moment_jacobian(vb_opt.x)\n",
    "\n",
    "use_cg = False\n",
    "if use_cg:\n",
    "    # Get the sensitivity operator with conjugate gradient to avoid constructing the Hessian.\n",
    "    # This was necessary before we had the sparse Hessian.\n",
    "    sensitivity_operator = np.full_like(moment_jac, float('nan'))\n",
    "    free_param_size = len(vb_opt.x)\n",
    "\n",
    "    KLHessVecProdLO = LinearOperator((free_param_size, free_param_size),\n",
    "                                     lambda vec: obj.kl_hvp(vb_opt.x, vec))\n",
    "    for ind in range(sensitivity_operator.shape[0]):\n",
    "        cg_res, info = sp.sparse.linalg.cg(KLHessVecProdLO, moment_jac[ind, :].T)\n",
    "        sensitivity_operator[ind, :] = cg_res\n",
    "else:\n",
    "    kl_hess = obj.get_sparse_hessian(vb_opt.x)\n",
    "    sensitivity_operator = sp.sparse.linalg.spsolve(csc_matrix(kl_hess), csr_matrix(moment_jac).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(sensitivity_operator.toarray())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.params.set_free(vb_opt.x)\n",
    "def kl_weight_fun(weights):\n",
    "    obj.weights = weights\n",
    "    return obj.kl()\n",
    "\n",
    "default_weights = np.full((n_num, 1), 1.0)\n",
    "get_kl_weight_grad = autograd.grad(kl_weight_fun)\n",
    "kl_weight_grad = get_kl_weight_grad(default_weights)\n",
    "mu_weight_sens = np.matmul(sensitivity_operator, kl_weight_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
