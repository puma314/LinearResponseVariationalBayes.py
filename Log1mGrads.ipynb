{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.core import primitive\n",
    "from autograd import grad, jacobian, hessian\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LogitTermClosure(beta, beta_cov, x_mat, std_vec):\n",
    "    class DataState:\n",
    "        pass\n",
    "    \n",
    "    class ParamState:\n",
    "        pass\n",
    "    \n",
    "    def HashData(x_mat, std_vec):\n",
    "        # Better than nothing I guess.\n",
    "        return hash(str(x_mat) + str(std_vec))\n",
    "    \n",
    "    data = DataState()\n",
    "    par = ParamState()\n",
    "    def SetDataState(x_mat, std_vec):\n",
    "        data.x_mat = x_mat\n",
    "        data.std_vec = std_vec\n",
    "        data.x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "        data.data_hash = HashData(x_mat, std_vec)\n",
    "        \n",
    "    def SetParamState(beta, beta_cov):\n",
    "        par.beta = beta\n",
    "        par.K = beta.size\n",
    "        assert beta_cov.shape == (par.K, par.K)\n",
    "        par.beta_cov = beta_cov\n",
    "        par.sigma = np.einsum('ijk,jk->i', data.x_outer, beta_cov)\n",
    "        par.mu = np.einsum('ij,j->i', data.x_mat, beta)\n",
    "        par.z = np.einsum('i,j->ij', par.sigma, data.std_vec) + \\\n",
    "                np.expand_dims(par.mu, 1)\n",
    "        par.p = np.exp(par.z) / (1 + np.exp(par.z))\n",
    "        \n",
    "    def CheckParCache(beta, beta_cov):\n",
    "        if (beta != par.beta).any() or (beta_cov != par.beta_cov).any():\n",
    "            print('Refreshing parameter cache.  (So refreshing.)')\n",
    "            SetParamState(beta, beta_cov)\n",
    "\n",
    "    def CheckDataCache(x_mat, std_vec):\n",
    "        if HashData(x_mat, std_vec) != data.data_hash:\n",
    "            print('Refreshing data cache.  (So refreshing.)')\n",
    "            SetDataState(x_mat, std_vec)\n",
    "            SetParamState(par.beta, par.beta_cov)\n",
    "            \n",
    "    SetDataState(x_mat, std_vec)\n",
    "    SetParamState(beta, beta_cov)\n",
    "    \n",
    "    # Define the functions.\n",
    "    \n",
    "    # Only this will be accessible on the outside, so only here do we need the\n",
    "    # cache check.\n",
    "    @primitive\n",
    "    def LogitTerm(beta, beta_cov, x_mat, std_vec):\n",
    "        CheckDataCache(x_mat, std_vec)\n",
    "        CheckParCache(beta, beta_cov)\n",
    "        return np.sum(np.log(1 + np.exp(par.z)))\n",
    "\n",
    "    # We will simplify the formulas by expressing gradients as functions of\n",
    "    # weighted sums over p, since only p depends on the parameters.\n",
    "    @primitive\n",
    "    def WeightedPSum(beta, beta_cov, weights):\n",
    "        return np.sum(par.p * weights)\n",
    "\n",
    "    @primitive\n",
    "    def LogitTerm_grad_beta_term(beta, beta_cov, a):\n",
    "        weights = np.expand_dims(data.x_mat[:, a], 1)\n",
    "        return WeightedPSum(par.beta, par.beta_cov, weights)\n",
    "\n",
    "    @primitive\n",
    "    def LogitTerm_grad_beta(beta, beta_cov):\n",
    "        return np.array([ LogitTerm_grad_beta_term(par.beta, par.beta_cov, a) for a in range(par.K)])\n",
    "\n",
    "    @primitive\n",
    "    def LogitTerm_vjp_beta(g, ans, vs, gvs, beta, beta_cov, x_mat, std_vec):\n",
    "        return g * LogitTerm_grad_beta(par.beta, par.beta_cov)\n",
    "    LogitTerm.defvjp(LogitTerm_vjp_beta, argnum=0)\n",
    "    \n",
    "    @primitive\n",
    "    def LogitTerm_grad_beta_cov_term(beta, beta_cov, a, b):\n",
    "        z_grad = np.einsum('j,i->ij', data.std_vec, data.x_outer[:, a, b])\n",
    "        return WeightedPSum(par.beta, par.beta_cov, z_grad)\n",
    "\n",
    "    @primitive\n",
    "    def LogitTerm_grad_beta_cov(beta, beta_cov):\n",
    "        GradTerm = lambda a, b: LogitTerm_grad_beta_cov_term(par.beta, par.beta_cov, a, b)\n",
    "        return np.array([[ GradTerm(a, b) for a in range(par.K) ] for b in range(par.K) ])\n",
    "\n",
    "    @primitive\n",
    "    def LogitTerm_vjp_beta_cov(g, ans, vs, gvs, beta, beta_cov, x_mat, std_vec):\n",
    "        return g * LogitTerm_grad_beta_cov(beta, beta_cov)\n",
    "    LogitTerm.defvjp(LogitTerm_vjp_beta_cov, argnum=1)\n",
    "\n",
    "\n",
    "    return LogitTerm, SetDataState, SetParamState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LogitTermAD(beta, beta_cov, x_mat, std_vec):\n",
    "    x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "    sigma = np.einsum('ijk,jk->i', x_outer, beta_cov)\n",
    "    mu = np.einsum('ij,j->i', x_mat, beta)\n",
    "    z = np.einsum('i,j->ij', sigma, std_vec) + np.expand_dims(mu, 1)\n",
    "    return np.sum(np.log(1 + np.exp(z)))\n",
    "\n",
    "# @primitive\n",
    "# def LogitTerm(beta, beta_cov, x_mat, std_vec):\n",
    "#     return LogitTermAD(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "# Every gradient is of the form of weighted sums of p, so writing everything\n",
    "# in terms of this makes it easy to differentiate.\n",
    "# @primitive\n",
    "# def WeightedPSum(beta, beta_cov, p, x_mat, std_vec, weights):\n",
    "#     return np.sum(p * weights)\n",
    "\n",
    "@primitive\n",
    "def WeightedPSum_grad_beta(beta, beta_cov, p, x_mat, std_vec, weights):\n",
    "    return np.einsum('ij,ij,ik->k', p * (1 - p), weights, x_mat)\n",
    "\n",
    "@primitive\n",
    "def WeightedPSum_grad_beta_cov(beta, beta_cov, p, x_mat, std_vec, weights):\n",
    "    x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "    return np.einsum('ij,ij,j,iab->ab', p * (1 - p), weights, std_vec, x_outer)\n",
    "\n",
    "# def LogitTerm_grad_beta_term(beta, p, x_mat, std_vec, a):\n",
    "#     # sigma = np.einsum('ijk,jk->i', x_outer, beta_cov)\n",
    "#     # mu = np.einsum('ij,j->i', x_mat, beta)\n",
    "#     # z = np.einsum('i,j->ij', sigma, std_vec) + np.expand_dims(mu, 1)\n",
    "#     # p = np.exp(z) / (1 + np.exp(z))\n",
    "#     return WeightedPSum(beta, beta_cov, p, x_mat, std_vec, np.expand_dims(x_mat[:, a], 1))\n",
    "#     # return np.sum(p * np.expand_dims(x_mat[:, a], 1))\n",
    "\n",
    "# def LogitTerm_grad_beta(beta, beta_cov, p, x_mat, std_vec):\n",
    "#     K = beta.size\n",
    "#     return np.array([ LogitTerm_grad_beta_term(beta, beta_cov, p, x_mat, std_vec, a) for a in range(K)])\n",
    "\n",
    "# def LogitTerm_vjp_beta(g, ans, vs, gvs, beta, beta_cov, x_mat, std_vec):\n",
    "# #     sigma = np.einsum('ijk,jk->i', x_outer, beta_cov)\n",
    "# #     mu = np.einsum('ij,j->i', x_mat, beta)\n",
    "# #     z = np.einsum('i,j->ij', sigma, std_vec) + np.expand_dims(mu, 1)\n",
    "# #     p = np.exp(z) / (1 + np.exp(z))\n",
    "#     return g * LogitTerm_grad_beta(beta, beta_cov, x_mat, std_vec)\n",
    "# LogitTerm.defvjp(LogitTerm_vjp_beta, argnum=0)\n",
    "\n",
    "# def LogitTerm_grad_beta_cov_term(beta, beta_cov, x_mat, std_vec, a, b):\n",
    "#     sigma = np.einsum('ijk,jk->i', x_outer, beta_cov)\n",
    "#     mu = np.einsum('ij,j->i', x_mat, beta)\n",
    "#     z = np.einsum('i,j->ij', sigma, std_vec) + np.expand_dims(mu, 1)\n",
    "#     p = np.exp(z) / (1 + np.exp(z))\n",
    "#     x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "#     z_grad = np.einsum('j,i->ij', std_vec, x_outer[:, a, b])\n",
    "#     return WeightedPSum(beta, beta_cov, p, x_mat, std_vec, z_grad)\n",
    "#     # return np.einsum('ji,i,j', p, std_vec, x_outer[:, a, b])\n",
    "\n",
    "# def LogitTerm_grad_beta_cov(beta, beta_cov, x_mat, std_vec):\n",
    "#     K = beta.size\n",
    "#     GradTerm = lambda a, b: LogitTerm_grad_beta_cov_term(beta, beta_cov, x_mat, std_vec, a, b)\n",
    "#     return np.array([[ GradTerm(a, b) for a in range(K) ] for b in range(K) ])\n",
    "\n",
    "# def LogitTerm_vjp_beta_cov(g, ans, vs, gvs, beta, beta_cov, x_mat, std_vec):\n",
    "#     return g * LogitTerm_grad_beta_cov(beta, beta_cov, x_mat, std_vec)\n",
    "# LogitTerm.defvjp(LogitTerm_vjp_beta_cov, argnum=1)\n",
    "\n",
    "\n",
    "# Define Hessians\n",
    "\n",
    "\n",
    "# Wrapping functions\n",
    "def UnWrap(par_vec, K):\n",
    "    beta = par_vec[0:K]\n",
    "    beta_cov = par_vec[K:].reshape(K, K)\n",
    "    return beta, beta_cov\n",
    "\n",
    "def Wrap(beta, beta_cov):\n",
    "    K = beta.size\n",
    "    return np.concatenate((beta, beta_cov.ravel()))\n",
    "\n",
    "def LogitTermWrap(par_vec, x_mat, std_vec):\n",
    "    beta, beta_cov = UnWrap(par_vec, 2)\n",
    "    return LogitTerm(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "def LogitTermWrapAD(par_vec, x_mat, std_vec):\n",
    "    beta, beta_cov = UnWrap(par_vec, 2)\n",
    "    return LogitTermAD(beta, beta_cov, x_mat, std_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.1203404731\n",
      "36.1203404731\n",
      "Refreshing parameter cache.  (So refreshing.)\n",
      "69.6750717025\n",
      "69.6750717025\n",
      "Refreshing data cache.  (So refreshing.)\n",
      "30.7931641682\n",
      "30.7931641682\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "beta = np.array([1., -0.5])\n",
    "beta_cov = np.full((2, 2), 0.1) + np.eye(2)\n",
    "x_mat = np.random.rand(10 * 2).reshape(N, 2)\n",
    "std_vec = np.array([-0.8, -0.3, 0.3, 0.8])\n",
    "\n",
    "LogitTerm, SetDataState, SetParamState = LogitTermClosure(beta, beta_cov, x_mat, std_vec)\n",
    "print LogitTerm(beta, beta_cov, x_mat, std_vec)\n",
    "print LogitTermAD(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "print LogitTerm(beta + 1, beta_cov, x_mat, std_vec)\n",
    "print LogitTermAD(beta + 1, beta_cov, x_mat, std_vec)\n",
    "\n",
    "print LogitTerm(beta + 1, beta_cov, x_mat * 0.1, std_vec)\n",
    "print LogitTermAD(beta + 1, beta_cov, x_mat * 0.1, std_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5527136788e-15\n",
      "3.5527136788e-15\n"
     ]
    }
   ],
   "source": [
    "par_vec = Wrap(beta, beta_cov)\n",
    "UnWrap(par_vec, 2)\n",
    "\n",
    "LogitTermWrapADGrad = grad(LogitTermWrapAD, argnum=0)\n",
    "LogitTermWrapGrad = grad(LogitTermWrap, argnum=0)\n",
    "print np.max(np.abs(LogitTermWrapADGrad(par_vec, x_mat, std_vec) -\n",
    "                    LogitTermWrapGrad(par_vec, x_mat, std_vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 20, 1, 11, 21]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
