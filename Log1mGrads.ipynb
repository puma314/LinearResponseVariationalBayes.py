{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.core import primitive\n",
    "from autograd import grad, jacobian, hessian\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LogitTermClosure(beta, beta_cov, x_mat, std_vec):\n",
    "    class DataState:\n",
    "        pass\n",
    "    \n",
    "    class ParamState:\n",
    "        pass\n",
    "    \n",
    "    def HashData(x_mat, std_vec):\n",
    "        # Better than nothing I guess.\n",
    "        return hash(str(x_mat) + str(std_vec))\n",
    "    \n",
    "    data = DataState()\n",
    "    par = ParamState()\n",
    "    def SetDataState(x_mat, std_vec):\n",
    "        data.x_mat = x_mat\n",
    "        data.std_vec = std_vec\n",
    "        data.x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "        data.data_hash = HashData(x_mat, std_vec)\n",
    "        \n",
    "    def SetParamState(beta, beta_cov):\n",
    "        par.beta = beta\n",
    "        par.K = beta.size\n",
    "        assert beta_cov.shape == (par.K, par.K)\n",
    "        par.beta_cov = beta_cov\n",
    "        par.sigma = np.einsum('ijk,jk->i', data.x_outer, beta_cov)\n",
    "        par.mu = np.einsum('ij,j->i', data.x_mat, beta)\n",
    "        par.z = np.einsum('i,j->ij', par.sigma, data.std_vec) + \\\n",
    "                np.expand_dims(par.mu, 1)\n",
    "        par.p = np.exp(par.z) / (1 + np.exp(par.z))\n",
    "        par.p_1m_p = par.p * (1 - par.p)\n",
    "        \n",
    "    def CheckParCache(beta, beta_cov):\n",
    "        if (beta != par.beta).any() or (beta_cov != par.beta_cov).any():\n",
    "            print('Refreshing parameter cache.  (So refreshing.)')\n",
    "            SetParamState(beta, beta_cov)\n",
    "\n",
    "    def CheckDataCache(x_mat, std_vec):\n",
    "        if HashData(x_mat, std_vec) != data.data_hash:\n",
    "            print('Refreshing data cache.  (So refreshing.)')\n",
    "            SetDataState(x_mat, std_vec)\n",
    "            SetParamState(par.beta, par.beta_cov)\n",
    "            \n",
    "    SetDataState(x_mat, std_vec)\n",
    "    SetParamState(beta, beta_cov)\n",
    "    \n",
    "    # Define the functions.\n",
    "    \n",
    "    # Only this will be accessible on the outside, so only here do we need the\n",
    "    # cache check.\n",
    "    @primitive\n",
    "    def LogitTerm(beta, beta_cov, x_mat, std_vec):\n",
    "        CheckDataCache(x_mat, std_vec)\n",
    "        CheckParCache(beta, beta_cov)\n",
    "        return np.sum(np.log(1 + np.exp(par.z)))\n",
    "\n",
    "    # We will simplify the formulas by expressing gradients as functions of\n",
    "    # weighted sums over p, since only p depends on the parameters.\n",
    "    @primitive\n",
    "    def WeightedPSum(beta, beta_cov, weights):\n",
    "        return np.sum(par.p * weights)\n",
    "\n",
    "    @primitive\n",
    "    def WeightedPSum_grad_beta(weights):\n",
    "        return np.einsum('ij,ij,ik->k', par.p_1m_p, weights, data.x_mat)\n",
    "    @primitive\n",
    "    def WeightedPSum_vjp_beta(g, ans, vs, gvs, beta, beta_cov, weights):\n",
    "        return g * WeightedPSum_grad_beta(weights)\n",
    "    WeightedPSum.defvjp(WeightedPSum_vjp_beta, argnum=0)\n",
    "\n",
    "    @primitive\n",
    "    def WeightedPSum_grad_beta_cov(weights):\n",
    "        return np.einsum('ij,ij,j,iab->ab', par.p_1m_p, weights, data.std_vec, data.x_outer)\n",
    "    @primitive\n",
    "    def WeightedPSum_vjp_beta_cov(g, ans, vs, gvs, beta, beta_cov, weights):\n",
    "        return g * WeightedPSum_grad_beta_cov(weights)\n",
    "    WeightedPSum.defvjp(WeightedPSum_vjp_beta_cov, argnum=1)\n",
    "\n",
    "    # Here and below, this is not primitive to inherit derivtatives from WeightedPSum\n",
    "    #@primitive\n",
    "    def LogitTerm_grad_beta_term(beta, beta_cov, a):\n",
    "        weights = np.expand_dims(data.x_mat[:, a], 1)\n",
    "        return WeightedPSum(beta, beta_cov, weights)\n",
    "\n",
    "    #@primitive\n",
    "    def LogitTerm_grad_beta(beta, beta_cov):\n",
    "        return np.array([ LogitTerm_grad_beta_term(beta, beta_cov, a) for a in range(par.K)])\n",
    "\n",
    "    #@primitive\n",
    "    def LogitTerm_vjp_beta(g, ans, vs, gvs, beta, beta_cov, x_mat, std_vec):\n",
    "        return g * LogitTerm_grad_beta(beta, beta_cov)\n",
    "    LogitTerm.defvjp(LogitTerm_vjp_beta, argnum=0)\n",
    "    \n",
    "    #@primitive\n",
    "    def LogitTerm_grad_beta_cov_term(beta, beta_cov, a, b):\n",
    "        z_grad = np.einsum('j,i->ij', data.std_vec, data.x_outer[:, a, b])\n",
    "        return WeightedPSum(beta, beta_cov, z_grad)\n",
    "\n",
    "    #@primitive\n",
    "    def LogitTerm_grad_beta_cov(beta, beta_cov):\n",
    "        GradTerm = lambda a, b: LogitTerm_grad_beta_cov_term(beta, beta_cov, a, b)\n",
    "        return np.array([[ GradTerm(a, b) for a in range(par.K) ] for b in range(par.K) ])\n",
    "\n",
    "    #@primitive\n",
    "    def LogitTerm_vjp_beta_cov(g, ans, vs, gvs, beta, beta_cov, x_mat, std_vec):\n",
    "        return g * LogitTerm_grad_beta_cov(beta, beta_cov)\n",
    "    LogitTerm.defvjp(LogitTerm_vjp_beta_cov, argnum=1)\n",
    "\n",
    "\n",
    "    return LogitTerm, SetDataState, SetParamState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LogitTermAD(beta, beta_cov, x_mat, std_vec):\n",
    "    x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "    sigma = np.einsum('ijk,jk->i', x_outer, beta_cov)\n",
    "    mu = np.einsum('ij,j->i', x_mat, beta)\n",
    "    z = np.einsum('i,j->ij', sigma, std_vec) + np.expand_dims(mu, 1)\n",
    "    return np.sum(np.log(1 + np.exp(z)))\n",
    "\n",
    "# Wrapping functions\n",
    "def UnWrap(par_vec, K):\n",
    "    beta = par_vec[0:K]\n",
    "    beta_cov = par_vec[K:].reshape(K, K)\n",
    "    return beta, beta_cov\n",
    "\n",
    "def Wrap(beta, beta_cov):\n",
    "    K = beta.size\n",
    "    return np.concatenate((beta, beta_cov.ravel()))\n",
    "\n",
    "def LogitTermWrap(par_vec, x_mat, std_vec):\n",
    "    K = x_mat.shape[1]\n",
    "    beta, beta_cov = UnWrap(par_vec, K)\n",
    "    return LogitTerm(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "def LogitTermWrapAD(par_vec, x_mat, std_vec):\n",
    "    K = x_mat.shape[1]\n",
    "    beta, beta_cov = UnWrap(par_vec, K)\n",
    "    return LogitTermAD(beta, beta_cov, x_mat, std_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6380.39646398\n",
      "6380.39646398\n",
      "Refreshing parameter cache.  (So refreshing.)\n",
      "14690.1153357\n",
      "14690.1153357\n",
      "Refreshing data cache.  (So refreshing.)\n",
      "3562.45966451\n",
      "3562.45966451\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "K = 5\n",
    "beta = np.random.rand(K)\n",
    "beta_cov = np.full((K, K), 0.1) + np.eye(K)\n",
    "x_mat = np.random.rand(N * K).reshape(N, K)\n",
    "std_vec = np.array([-0.8, -0.3, 0.3, 0.8])\n",
    "\n",
    "LogitTerm, SetDataState, SetParamState = LogitTermClosure(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "print LogitTerm(beta, beta_cov, x_mat, std_vec)\n",
    "print LogitTermAD(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "print LogitTerm(beta + 1, beta_cov, x_mat, std_vec)\n",
    "print LogitTermAD(beta + 1, beta_cov, x_mat, std_vec)\n",
    "\n",
    "print LogitTerm(beta + 1, beta_cov, x_mat * 0.1, std_vec)\n",
    "print LogitTermAD(beta + 1, beta_cov, x_mat * 0.1, std_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing data cache.  (So refreshing.)\n",
      "Refreshing parameter cache.  (So refreshing.)\n",
      "2.50111042988e-12\n",
      "2.84217094304e-14\n"
     ]
    }
   ],
   "source": [
    "par_vec = Wrap(beta, beta_cov)\n",
    "UnWrap(par_vec, K)\n",
    "\n",
    "print 'Grads:'\n",
    "LogitTermWrapADGrad = grad(LogitTermWrapAD)\n",
    "LogitTermWrapGrad = grad(LogitTermWrap)\n",
    "print np.max(np.abs(LogitTermWrapADGrad(par_vec, x_mat, std_vec) -\n",
    "                    LogitTermWrapGrad(par_vec, x_mat, std_vec)))\n",
    "\n",
    "\n",
    "print 'Hessians:'\n",
    "LogitTermWrapADHess = hessian(LogitTermWrapAD)\n",
    "LogitTermWrapHess = hessian(LogitTermWrap)\n",
    "print np.max(np.abs(LogitTermWrapADHess(par_vec, x_mat, std_vec) -\n",
    "                    LogitTermWrapHess(par_vec, x_mat, std_vec)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessians:\n",
      "\n",
      "0.191372203827\n",
      "0.0229320049286\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "time_num = 5\n",
    "print 'Hessians:\\n'\n",
    "print timeit.timeit(lambda: LogitTermWrapHess(par_vec, x_mat, std_vec), number=time_num) / time_num\n",
    "print timeit.timeit(lambda: LogitTermWrapADHess(par_vec, x_mat, std_vec), number=time_num) / time_num\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
