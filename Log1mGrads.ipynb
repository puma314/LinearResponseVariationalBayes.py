{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.core import primitive\n",
    "from autograd import grad, jacobian, hessian\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LogitTermAD(beta, beta_cov, x_mat, std_vec):\n",
    "    x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "    sigma = np.einsum('ijk,jk->i', x_outer, beta_cov)\n",
    "    mu = np.einsum('ij,j->i', x_mat, beta)\n",
    "    z = np.einsum('i,j->ij', sigma, std_vec) + np.expand_dims(mu, 1)\n",
    "    return np.sum(np.log(1 + np.exp(z)))\n",
    "\n",
    "@primitive\n",
    "def LogitTerm(beta, beta_cov, x_mat, std_vec):\n",
    "    return LogitTermAD(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "# Every gradient is of the form of weighted sums of p, so writing everything\n",
    "# in terms of this makes it easy to differentiate.\n",
    "@primitive\n",
    "def WeightedPSum(beta, beta_cov, p, x_mat, std_vec, weights):\n",
    "    return np.sum(p * weights)\n",
    "\n",
    "\n",
    "@primitive\n",
    "def LogitTerm_grad_beta_term(beta, beta_cov, x_mat, std_vec, a):\n",
    "    x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "    sigma = np.einsum('ijk,jk->i', x_outer, beta_cov)\n",
    "    mu = np.einsum('ij,j->i', x_mat, beta)\n",
    "    z = np.einsum('i,j->ij', sigma, std_vec) + np.expand_dims(mu, 1)\n",
    "    p = np.exp(z) / (1 + np.exp(z))\n",
    "    return WeightedPSum(beta, beta_cov, p, x_mat, std_vec, np.expand_dims(x_mat[:, a], 1))\n",
    "    # return np.sum(p * np.expand_dims(x_mat[:, a], 1))\n",
    "\n",
    "def LogitTerm_grad_beta(beta, beta_cov, x_mat, std_vec):\n",
    "    K = beta.size\n",
    "    return np.array([ LogitTerm_grad_beta_term(beta, beta_cov, x_mat, std_vec, a) for a in range(K)])\n",
    "\n",
    "def LogitTerm_vjp_beta(g, ans, vs, gvs, beta, beta_cov, x_mat, std_vec):\n",
    "    return g * LogitTerm_grad_beta(beta, beta_cov, x_mat, std_vec)\n",
    "LogitTerm.defvjp(LogitTerm_vjp_beta, argnum=0)\n",
    "\n",
    "\n",
    "@primitive\n",
    "def LogitTerm_grad_beta_cov_term(beta, beta_cov, x_mat, std_vec, a, b):\n",
    "    x_outer = np.einsum('ij,ik->ijk', x_mat, x_mat)\n",
    "    sigma = np.einsum('ijk,jk->i', x_outer, beta_cov)\n",
    "    mu = np.einsum('ij,j->i', x_mat, beta)\n",
    "    z = np.einsum('i,j->ij', sigma, std_vec) + np.expand_dims(mu, 1)\n",
    "    p = np.exp(z) / (1 + np.exp(z))\n",
    "    weights = np.einsum('j,i->ij', std_vec, x_outer[:, a, b])\n",
    "    return WeightedPSum(beta, beta_cov, p, x_mat, std_vec, weights)\n",
    "    # return np.einsum('ji,i,j', p, std_vec, x_outer[:, a, b])\n",
    "\n",
    "def LogitTerm_grad_beta_cov(beta, beta_cov, x_mat, std_vec):\n",
    "    K = beta.size\n",
    "    GradTerm = lambda a, b: LogitTerm_grad_beta_cov_term(beta, beta_cov, x_mat, std_vec, a, b)\n",
    "    return np.array([[ GradTerm(a, b) for a in range(K) ] for b in range(K) ])\n",
    "\n",
    "def LogitTerm_vjp_beta_cov(g, ans, vs, gvs, beta, beta_cov, x_mat, std_vec):\n",
    "    return g * LogitTerm_grad_beta_cov(beta, beta_cov, x_mat, std_vec)\n",
    "LogitTerm.defvjp(LogitTerm_vjp_beta_cov, argnum=1)\n",
    "\n",
    "\n",
    "# Define Hessians\n",
    "\n",
    "\n",
    "\n",
    "# Wrapping functions\n",
    "def UnWrap(par_vec, K):\n",
    "    beta = par_vec[0:K]\n",
    "    beta_cov = par_vec[K:].reshape(K, K)\n",
    "    return beta, beta_cov\n",
    "\n",
    "def Wrap(beta, beta_cov):\n",
    "    K = beta.size\n",
    "    return np.concatenate((beta, beta_cov.ravel()))\n",
    "\n",
    "def LogitTermWrap(par_vec, x_mat, std_vec):\n",
    "    beta, beta_cov = UnWrap(par_vec, 2)\n",
    "    return LogitTerm(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "def LogitTermWrapAD(par_vec, x_mat, std_vec):\n",
    "    beta, beta_cov = UnWrap(par_vec, 2)\n",
    "    return LogitTermAD(beta, beta_cov, x_mat, std_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0964436978\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "beta = np.array([1., -0.5])\n",
    "beta_cov = np.full((2, 2), 0.1) + np.eye(2)\n",
    "x_mat = np.random.rand(10 * 2).reshape(N, 2)\n",
    "std_vec = np.array([-0.8, -0.3, 0.3, 0.8])\n",
    "\n",
    "LogitTerm(beta, beta_cov, x_mat, std_vec)\n",
    "\n",
    "par_vec = Wrap(beta, beta_cov)\n",
    "UnWrap(par_vec, 2)\n",
    "\n",
    "LogitTermWrapADGrad = grad(LogitTermWrapAD, argnum=0)\n",
    "LogitTermWrapGrad = grad(LogitTermWrap, argnum=0)\n",
    "print np.max(np.abs(LogitTermWrapADGrad(par_vec, x_mat, std_vec) -\n",
    "                    LogitTermWrapGrad(par_vec, x_mat, std_vec)))\n",
    "\n",
    "# LogitTermWrapADHess = hessian(LogitTermWrapAD, argnum=0)\n",
    "# LogitTermWrapHess = hessian(LogitTermWrap, argnum=0)\n",
    "# print np.max(np.abs(LogitTermWrapADHess(par_vec, x_mat, std_vec) -\n",
    "#                     LogitTermWrapADHess(par_vec, x_mat, std_vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 20, 1, 11, 21]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
