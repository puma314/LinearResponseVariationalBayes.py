{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import LinearResponseVariationalBayes as vb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "import scipy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model with observations $y_n$ and $x_n$, for $n = 1,...,N$, and matrix parameters $\\beta$ and $\\Lambda$, where\n",
    "\n",
    "$$\n",
    "y_n \\vert x_n, \\beta, \\Lambda \\sim \\mathcal{N} \\left(\\beta x_n, \\Lambda^{-1} \\right)\n",
    "$$\n",
    "\n",
    "For fun, let's assume that all the entries of $\\beta$ are constrained to be positive. We will compute the maximum likelihood estimate for $\\beta$ and $\\Lambda$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multivariate normal model with a constrained mean.\n",
    "x_dim = 2\n",
    "n_obs = 10000\n",
    "\n",
    "true_beta = np.exp(np.random.random((x_dim, x_dim)))\n",
    "true_lambda = np.eye(x_dim) + np.full((x_dim, x_dim), 0.5)\n",
    "true_cov = np.linalg.inv(true_lambda)\n",
    "\n",
    "x = np.random.random((n_obs, x_dim))\n",
    "true_mean = np.matmul(x, true_beta)\n",
    "y = np.array([ np.random.multivariate_normal(true_mean[n], true_cov) for n in range(n_obs) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the model as a function of parameters.  A ```Parameter``` object called, say, ```par```, needs to be able to do a few things:\n",
    "\n",
    "Read and set in its standard form:\n",
    "* ```par.get()```\n",
    "* ```par.set(value)```\n",
    "\n",
    "These methods return or set a parameter value that you can use in a model.  For example, if the parameter is a matrix, ```get()``` returns a positive definite matrice, and ```set()``` must be passed a positive definite matrix.\n",
    "\n",
    "Convert to and from vectors:\n",
    "* ```par.get_vector()```\n",
    "* ```par.set_vector(vector_value)```\n",
    "* ```par.vector_size()```\n",
    "\n",
    "These methods unpack or pack the parameter into a one-dimensional numpy array without changing any of the values.  The length of this vector is given by ```vector_size()```.  If you run ```set_vector()```, the expectation is that you are setting the parameter to a legal value.\n",
    "\n",
    "Convert to and from unconstrained parameterizations:\n",
    "* ```par.get_free()```\n",
    "* ```par.set_free(free_vector_value)```\n",
    "* ```par.free_size()```\n",
    "\n",
    "These methods also use 1d representations of the parameter, only they are expressed in an unconstrained space.  Up to numerical issues, you should be able run ```set_free(free_vector_value)``` with any ```free_vector_value``` that is the correct length.  The length of the free vector is given by ```free_size()```.  Note that ```free_size()``` and ```vector_size()``` may not be the same.  For example, a simplex parameter requires fewer free values than are contained in the vectorized representation of the simplex.\n",
    "\n",
    "Importantly, all these methods must be differentiable with autograd.  See the test ```execute_required_methods``` for full details of what is required of a ```Parameter```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "lambda:\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "Unconstrained values:\n",
      "beta:  [ 0.  0.  0.  0.]\n",
      "lamb:  [ 0.  0.  0.]\n",
      "Vector values:\n",
      "beta:  [ 1.  1.  1.  1.]\n",
      "lamb:  [ 1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Define our parameters.\n",
    "\n",
    "# By setting lb = 0., we make all the entries of beta positive.\n",
    "beta = vb.ArrayParam(name='beta', shape=(x_dim, x_dim), lb=0.)\n",
    "lamb = vb.PosDefMatrixParam('lambda', size=x_dim)\n",
    "\n",
    "print(beta)\n",
    "print(lamb)\n",
    "\n",
    "print('Unconstrained values:')\n",
    "print('beta: ', beta.get_free())\n",
    "print('lamb: ', lamb.get_free())\n",
    "\n",
    "print('Vector values:')\n",
    "print('beta: ', beta.get_vector())\n",
    "print('lamb: ', lamb.get_vector())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be combined into parameter dictionaries using the ```ModelParamsDict``` type.  Importantly, a ```ModelParamsDict``` is also a ```Parameter``` with all the methods described above.  By combining parameter into a dictionary, you can easily get or set an unconstrained or vectorized representation of a parameter set all at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      "\tbeta:\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "\tlambda:\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "beta:\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "lambda:\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Combine the parameters into a dictionary:\n",
    "\n",
    "par = vb.ModelParamsDict('params')\n",
    "par.push_param(beta)\n",
    "par.push_param(lamb)\n",
    "\n",
    "initial_free_par = deepcopy(par.get_free())\n",
    "\n",
    "print(par)\n",
    "print(par['beta'])\n",
    "print(par['lambda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      "\tbeta:\n",
      "[[ 1.99935453  1.25489014]\n",
      " [ 1.37444099  2.31354934]]\n",
      "\tlambda:\n",
      "[[ 2.27497606  0.74887345]\n",
      " [ 0.74887345  2.73479722]]\n",
      "\n",
      "Set par back to initial values:\n",
      "params:\n",
      "\tbeta:\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "\tlambda:\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "\n",
      "Set par using the combined free vector:\n",
      "params:\n",
      "\tbeta:\n",
      "[[ 1.99935453  1.25489014]\n",
      " [ 1.37444099  2.31354934]]\n",
      "\tlambda:\n",
      "[[ 2.27497606  0.74887345]\n",
      " [ 0.74887345  2.73479722]]\n"
     ]
    }
   ],
   "source": [
    "# Generate some random parameters for demonstration purposes.\n",
    "beta_free_param = np.random.random(beta.free_size())\n",
    "lamb_free_param = np.random.random(lamb.free_size())\n",
    "\n",
    "par['beta'].set_free(beta_free_param)\n",
    "par['lambda'].set_free(lamb_free_param)\n",
    "\n",
    "print(par)\n",
    "par_free = par.get_free()\n",
    "\n",
    "print('\\nSet par back to initial values:')\n",
    "par.set_free(initial_free_par)\n",
    "print(par)\n",
    "\n",
    "print('\\nSet par using the combined free vector:')\n",
    "par.set_free(par_free)\n",
    "print(par)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build a model with parameters and data as attributes and a method that evaluates to an objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, y, x, par):\n",
    "        # You may want to deepcopy the parameters to avoid confusing things happening,\n",
    "        # especially with autograd.\n",
    "        self.par = deepcopy(par)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.weights = np.full(x.shape[0], 1.0)\n",
    "        \n",
    "    def loglik(self):\n",
    "        # Evaluate the objective at the current parameter value.\n",
    "        beta = self.par['beta'].get()\n",
    "        lamb = self.par['lambda'].get()\n",
    "\n",
    "        y_centered = self.y - np.matmul(self.x, beta)\n",
    "        y_term = -0.5 * np.einsum('ni,ij,nj,n', y_centered, lamb, y_centered, self.weights)\n",
    "        \n",
    "        s, logdet = np.linalg.slogdet(lamb)\n",
    "        assert s > 0\n",
    "        \n",
    "        return y_term + 0.5 * np.sum(self.weights) * logdet\n",
    "    \n",
    "    def eval_objective(self, free_par):\n",
    "        # scipy minimizes, so return the negative log likelihood.\n",
    "        self.par.set_free(free_par)\n",
    "        ll = self.loglik()\n",
    "        return -1. * ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's objective function can now be passed directly to autograd and optimization routines.\n",
    "\n",
    "Some of this boilerplate is wrapper in the module ```SparseObjectives.py```, which also tries to accommodate hand-coded sparse Hessians, though that is still in development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16721.6275444\n",
      "11598.8477658\n",
      "3071779.63632\n"
     ]
    }
   ],
   "source": [
    "model = Model(y, x, par)\n",
    "\n",
    "# Now the objective is a function of the free parameter:\n",
    "print(model.eval_objective(par_free - 1.0))\n",
    "print(model.eval_objective(par_free))\n",
    "print(model.eval_objective(par_free + 1.0))\n",
    "\n",
    "# And it is differentiable.\n",
    "eval_objective_grad = autograd.grad(model.eval_objective)\n",
    "eval_objective_hess = autograd.hessian(model.eval_objective)\n",
    "eval_objective_hvp = autograd.hessian_vector_product(model.eval_objective)\n",
    "\n",
    "grad = eval_objective_grad(par_free)\n",
    "hess = eval_objective_hess(par_free)\n",
    "hvp = eval_objective_hvp(par_free, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Drop into an optimization method\n",
    "\n",
    "print('Running Newton Trust Region')\n",
    "vb_opt = optimize.minimize(\n",
    "    model.eval_objective,\n",
    "    jac=eval_objective_grad,\n",
    "    hessp=eval_objective_hvp,\n",
    "    x0=initial_free_par,\n",
    "    method='trust-ncg')\n",
    "\n",
    "print('Done.')\n",
    "opt_free_par = deepcopy(vb_opt.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True beta:\n",
      " [[ 2.22639279  1.53736923]\n",
      " [ 1.22294226  1.22112609]]\n",
      "Fit beta:  beta:\n",
      "[[ 2.20862199  1.57028409]\n",
      " [ 1.23491386  1.21065658]]\n",
      "True lambda:\n",
      " [[ 1.5  0.5]\n",
      " [ 0.5  1.5]]\n",
      "Fit lambda:  lambda:\n",
      "[[ 1.51052283  0.49646839]\n",
      " [ 0.49646839  1.53826346]]\n"
     ]
    }
   ],
   "source": [
    "model.par.set_free(opt_free_par)\n",
    "\n",
    "print('True beta:\\n', true_beta)\n",
    "print('Fit beta: ', model.par['beta'])\n",
    "\n",
    "print('True lambda:\\n', true_lambda)\n",
    "print('Fit lambda: ', model.par['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining a vector of summarizing statistics (the things you want to know about the model), you can then do sensitivity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we're interested in the sensitivity of beta to the weights.  Define a\n",
    "# summary class that takes in the free parameters are returns the summary of\n",
    "# interest.\n",
    "\n",
    "class Summary(object):\n",
    "    def __init__(self, par):\n",
    "        self.par = deepcopy(par)\n",
    "        self.summary_jacobian = autograd.jacobian(self.eval_summary)\n",
    "        \n",
    "    def summary(self):\n",
    "        return self.par['beta'].get_vector()\n",
    "    \n",
    "    def eval_summary(self, par_free):\n",
    "        self.par.set_free(par_free)\n",
    "        return self.summary()\n",
    "\n",
    "summary = Summary(par)\n",
    "summary_jac = summary.summary_jacobian(opt_free_par)\n",
    "\n",
    "objective_hess = eval_objective_hess(opt_free_par)\n",
    "\n",
    "summary_sens_operator = -1. * np.linalg.solve(objective_hess, summary_jac.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new objective that's a function of the weights.\n",
    "def objective_weights(weights, free_par, model):\n",
    "    model.weights = weights\n",
    "    return model.eval_objective(free_par)\n",
    "\n",
    "# Get the cross Hessian between the weights and parameters by taking the\n",
    "# Jacobian of a gradient.\n",
    "\n",
    "# Autograd runs faster when you do the smaller dimension first.\n",
    "eval_objective_par_grad = autograd.grad(objective_weights, argnum=1)\n",
    "eval_objective_par_weight_hess = autograd.jacobian(eval_objective_par_grad, argnum=0)\n",
    "\n",
    "initial_weights = np.full(x.shape[0], 1.)\n",
    "par_weight_hess = eval_objective_par_weight_hess(initial_weights, opt_free_par, model)\n",
    "\n",
    "# Weight sens now contains the sensitivity of beta to individual points' weights\n",
    "# in its rows.\n",
    "weight_sens = np.matmul(par_weight_hess.T, summary_sens_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton Trust Region\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Let's check the accuracy of weight_sens by manually removing a data point and re-fitting.\n",
    "\n",
    "eval_objective_par_hvp = \\\n",
    "    autograd.hessian_vector_product(objective_weights, argnum=1)\n",
    "\n",
    "# In order to avoid some lambda function weirdness, wrap the hessian vector product.\n",
    "def eval_objective_par_hvp_wrapper(free_par, grad):\n",
    "    return eval_objective_par_hvp(weights, free_par, model, grad)\n",
    "\n",
    "# To verify this, perturb a weight and re-fit.\n",
    "perturb_row = 25\n",
    "weights = np.full(x.shape[0], 1.)\n",
    "weights[perturb_row] = 0.\n",
    "print('Running Newton Trust Region')\n",
    "vb_opt_perturbed = optimize.minimize(\n",
    "    lambda free_par: objective_weights(weights, free_par, model),\n",
    "    jac=lambda free_par: eval_objective_par_grad(weights, free_par, model),\n",
    "    hessp=eval_objective_par_hvp_wrapper,\n",
    "    x0=opt_free_par,\n",
    "    method='trust-ncg')\n",
    "print('Done.')\n",
    "\n",
    "opt_free_par_perturbed = deepcopy(vb_opt_perturbed.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preturbing row  25\n",
      "Before:\t [ 2.20862199  1.57028409  1.23491386  1.21065658]\n",
      "After:\t [ 2.20855098  1.57009578  1.23496579  1.21079429]\n",
      "Actual difference:\t [  7.10120342e-05   1.88308808e-04  -5.19255622e-05  -1.37705753e-04]\n",
      "Predicted difference:\t [  7.09962804e-05   1.88263485e-04  -5.19199793e-05  -1.37678146e-04]\n"
     ]
    }
   ],
   "source": [
    "# The perturbation matches the prediction.\n",
    "\n",
    "print('Preturbing row ', perturb_row)\n",
    "\n",
    "print('Before:\\t', summary.eval_summary(opt_free_par))\n",
    "print('After:\\t', summary.eval_summary(opt_free_par_perturbed))\n",
    "\n",
    "print('Actual difference:\\t',\n",
    "      summary.eval_summary(opt_free_par) -\n",
    "      summary.eval_summary(opt_free_par_perturbed))\n",
    "print('Predicted difference:\\t',\n",
    "      weight_sens[perturb_row])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
