#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Notes for presentation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\theta & =\textrm{Unobserved, relatively low-dimensional shared parameters}\\
z=\left(z_{1},z_{2},...,z_{n}\right) & =\textrm{Unobserved, relatively high-dimensional set of "latent" parameters or data}\\
X & =\textrm{Observed data}
\end{align*}

\end_inset


\end_layout

\begin_layout Section
The Neyman-Scott paradox
\end_layout

\begin_layout Standard
The Neyman-Scott paradox.
\begin_inset Formula 
\begin{align*}
\textrm{For }n & =1,...,N\\
X_{1n} & \sim\mathcal{N}\left(z_{n},\theta\right)\\
X_{2n} & \sim\mathcal{N}\left(z_{n},\theta\right)\\
\log P\left(X_{1n}\vert z_{n},\theta\right) & =-\frac{1}{2}\theta^{-1}\left(X_{1n}^{2}-2X_{1n}z_{n}+z_{n}^{2}\right)-\log\theta+C\\
\log P\left(X_{2n}\vert z_{n},\theta\right) & =-\frac{1}{2}\theta^{-1}\left(X_{2n}^{2}-2X_{2n}z_{n}+z_{n}^{2}\right)-\log\theta+C\\
\log P\left(X\vert z_{1},...,z_{n},\theta\right) & =\sum_{n=1}^{N}\left(\log P\left(X_{1n}\vert z_{n},\theta\right)+\log P\left(X_{2n}\vert z_{n},\theta\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Irrespective of 
\begin_inset Formula $\theta$
\end_inset

,
\begin_inset Formula 
\begin{align*}
\hat{z}_{n} & =\mathrm{argmax}_{z_{n}}P\left(X_{1n},X_{2n}\vert z_{n},\theta\right)\\
 & =\frac{X_{1n}+X_{2n}}{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\hat{z}_{n} & =\frac{X_{1n}+X_{2n}}{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and that consequently,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\hat{\theta} & =\mathrm{argmax}_{\theta}P\left(X_{1n},X_{2n}\vert\hat{z}_{n},\theta\right)\\
 & =\frac{1}{2}\left(\frac{1}{N}\sum_{n}\left(X_{1n}-\hat{z}_{n}\right)^{2}+\frac{1}{N}\sum_{n}\left(X_{2n}-\hat{z}_{n}\right)^{2}\right)\\
 & =\frac{1}{2}\left(\frac{1}{N}\sum_{n}\left(\frac{X_{1n}-X_{2n}}{2}\right)^{2}+\frac{1}{N}\sum_{n}\left(\frac{X_{2n}-X_{1n}}{2}\right)^{2}\right)\\
 & =\frac{1}{4N}\sum_{n}\left(X_{1n}-X_{2n}\right)^{2}
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
\hat{\theta} & =\mathrm{argmax}_{\theta}P\left(X_{1n},X_{2n}\vert\hat{z}_{n},\theta\right)\\
 & =\frac{1}{2}\left(\frac{1}{N}\sum_{n}\left(X_{1n}-\hat{z}_{n}\right)^{2}+\frac{1}{N}\sum_{n}\left(X_{2n}-\hat{z}_{n}\right)^{2}\right)\\
 & =\frac{1}{4N}\sum_{n}\left(X_{1n}-X_{2n}\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\hat{\theta} & =\frac{1}{4N}\sum_{n}\left(X_{1n}-X_{2n}\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
However,
\begin_inset Formula 
\begin{align*}
\mathbb{E}\left[\left(X_{1n}-X_{2n}\right)^{2}\right] & =\mathbb{E}\left[\mathbb{E}\left[\left(X_{1n}-X_{2n}\right)^{2}\vert z_{n}\right]\right]\\
 & =\mathbb{E}\left[\mathbb{E}\left[\left(X_{1n}-z_{n}+z_{n}-X_{2n}\right)^{2}\vert z_{n}\right]\right]\\
 & =\mathbb{E}\left[\mathbb{E}\left[\left(X_{1n}-z_{n}\right)^{2}+\left(X_{2n}-z_{n}\right)^{2}+2\left(X_{2n}-z_{n}\right)\left(X_{1n}-z_{n}\right)\vert z_{n}\right]\right]\\
 & =\mathbb{E}\left[\mathbb{E}\left[\theta+\theta+0\vert z_{n}\right]\right]\\
 & =2\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathbb{E}\left[\left(X_{1n}-X_{2n}\right)^{2}\right] & =\mathbb{E}\left[\mathbb{E}\left[\left(X_{1n}-X_{2n}\right)^{2}\vert z_{n}\right]\right]=2\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathbb{E}\left[\left(X_{1n}-X_{2n}\right)^{2}\right] & =2\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
So 
\begin_inset Formula 
\begin{align*}
\hat{\theta} & \xrightarrow[n\rightarrow\infty]{}\frac{1}{4}2\theta=\frac{\theta}{2}\ne\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note that we estimated 
\begin_inset Formula $\hat{\theta}$
\end_inset

 as if we knew that 
\begin_inset Formula $\hat{z}_{n}=z_{n}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Marginally,
\begin_inset Formula 
\begin{align*}
\mathrm{Var}\left(X_{1n}\right) & =\mathrm{Var}\left(\mathbb{E}\left[X_{1n}\vert z_{n}\right]\right)+\mathbb{E}\left[\mathrm{Var}\left(X_{1n}\vert z_{n}\right)\right]=\mathrm{Var}\left(z_{n}\right)+\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We are implicitly using
\begin_inset Formula 
\begin{align*}
\mathrm{Var}\left(X_{1n}\right) & \approx\frac{1}{2N}\sum_{n}\left(X_{1n}-\bar{X}_{1}\right)^{2}=\\
\mathrm{Var}\left(z_{n}\right) & \approx\mathrm{Var}\left(\hat{z}_{n}\right)=\mathrm{Var}\left(\frac{X_{1n}+X_{2n}}{2}\right)=\frac{1}{4}\left(\mathrm{Var}\left(X_{1n}\right)+\mathrm{Var}\left(X_{2n}\right)\right)=\frac{1}{2}\mathrm{Var}\left(X_{1n}\right)\\
\theta & \approx\mathrm{Var}\left(X_{1n}\right)-\mathrm{Var}\left(\hat{z}_{n}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Suppose we assume
\begin_inset Formula 
\begin{align*}
z_{n} & \sim\mathcal{N}\left(0,\theta_{z}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Then standard facts about the bivariate normal give
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(\left(\begin{array}{c}
X_{1n}\\
X_{2n}
\end{array}\right)\vert\theta,\theta_{z}\right) & =\int P\left(\left(\begin{array}{c}
X_{1n}\\
X_{2n}
\end{array}\right),z_{n}\vert\theta,\theta_{z}\right)dz_{n}=\mathcal{N}\left(\left(\begin{array}{c}
0\\
0
\end{array}\right),\left(\begin{array}{cc}
\theta_{z}+\theta & \theta_{z}\\
\theta_{z} & \theta_{z}+\theta
\end{array}\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The sample covariance of a bivariate normal is consistent, so
\begin_inset Formula 
\begin{align*}
X_{n} & =\left(\begin{array}{c}
X_{1n}\\
X_{2n}
\end{array}\right)\quad\bar{X}=\frac{1}{N}\sum_{n=1}^{N}X_{n}\quad\hat{\Sigma}=\frac{1}{N}\sum_{n=1}^{N}\left(X_{n}-\bar{X}\right)\left(X_{n}-\bar{X}\right)^{T}\\
\hat{\Sigma} & \xrightarrow[n\rightarrow\infty]{}\left(\begin{array}{cc}
\theta_{z}+\theta & \theta_{z}\\
\theta_{z} & \theta_{z}+\theta
\end{array}\right)\Rightarrow\hat{\Sigma}_{11}-\hat{\Sigma}_{12}\xrightarrow[n\rightarrow\infty]{}\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Integrate out
\end_layout

\begin_layout Standard
Formally, we want to 
\begin_inset Quotes eld
\end_inset

integrate out
\begin_inset Quotes erd
\end_inset

 the latent variables.
\begin_inset Formula 
\begin{align*}
P\left(X,Z\vert\theta\right) & =\textrm{Probability of the data and latent variables given the parameters}\\
P\left(X\vert\theta\right)=\int P\left(X,Z\vert\theta\right)dZ & =\textrm{Marginal probability of the data given the parameters}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Are latent variables data or parameters?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(X,Z\vert\theta\right) & \textrm{ means the latent variables are unobserved data}\\
P\left(X\vert Z,\theta\right) & \textrm{ means the latent variables are extra parameters}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
If the 
\begin_inset Formula $Z$
\end_inset

 are parameters, then in order to integrate then 
\begin_inset Formula $Z$
\end_inset

, we need to posit a probability distribution, 
\begin_inset Formula $P\left(Z\vert\theta\right)$
\end_inset

 (possibly involving extra parameters in 
\begin_inset Formula $\theta$
\end_inset

).
 Then 
\begin_inset Formula 
\begin{align*}
P\left(X\vert Z,\theta\right) & =P\left(X,Z\vert\theta\right)P\left(Z\vert\theta\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Often, the frequentist estimate is 
\begin_inset Formula 
\begin{align*}
\hat{\theta} & =\sup_{\theta}P\left(X\vert\theta\right)=\sup_{\theta}\int P\left(X,Z\vert\theta\right)=\sup_{\theta}\int P\left(X\vert Z,\theta\right)P\left(Z\vert\theta\right)dZ
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Bayesians want the posterior
\begin_inset Formula 
\begin{align*}
P\left(\theta\vert X\right) & =\frac{P\left(X\vert\theta\right)P\left(\theta\right)}{P\left(X\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Doing posterior inference on the marginal likelihood is equivalent to marginaliz
ing the posterior:
\begin_inset Formula 
\begin{align*}
P\left(\theta\vert X\right) & =\frac{\left(\int P\left(X\vert Z,\theta\right)P\left(Z\vert\theta\right)dZ\right)P\left(\theta\right)}{P\left(X\right)}\\
 & =\int\frac{P\left(X\vert Z,\theta\right)P\left(Z\vert\theta\right)P\left(\theta\right)}{P\left(X\right)}dZ\\
 & =\int P\left(Z,\theta\vert X\right)dZ
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In either case, we have a high-dimensional integral to do.
\end_layout

\begin_layout Section
The EM Algorithm
\end_layout

\begin_layout Standard
We want to maximize 
\begin_inset Formula $P\left(X\vert\theta\right)$
\end_inset

, (or, equivalently, 
\begin_inset Formula $\log P\left(X\vert\theta\right)$
\end_inset

) but it's hard to calculate 
\begin_inset Formula $\int P\left(X,Z\vert\theta\right)dZ$
\end_inset

.
 By Jensen's inequality and concavity of the 
\begin_inset Formula $\log$
\end_inset

 function, for any distribution 
\begin_inset Formula $q\left(Z\right)$
\end_inset

,
\begin_inset Formula 
\begin{align*}
\log\int P\left(X,Z\vert\theta\right)dZ & =\log\int P\left(X,Z\vert\theta\right)\frac{q\left(Z\right)}{q\left(Z\right)}dZ\ge\int q\left(Z\right)\log\frac{P\left(X,Z\vert\theta\right)}{q\left(Z\right)}dZ
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Furthermore, if for some 
\begin_inset Formula $\theta_{0}$
\end_inset

 
\begin_inset Formula $q\left(Z\right)=P\left(Z\vert X,\theta_{0}\right)$
\end_inset

 then the inequality is an equality:
\begin_inset Formula 
\begin{align*}
\int P\left(Z\vert X,\theta_{0}\right)\log\frac{P\left(X,Z\vert\theta_{0}\right)}{P\left(Z\vert X,\theta_{0}\right)}dZ= & \int P\left(Z\vert X,\theta_{0}\right)\log\frac{P\left(X,Z\vert\theta_{0}\right)P\left(X\vert\theta_{0}\right)}{P\left(X,Z\vert\theta_{0}\right)}dZ\\
 & =\log P\left(X\vert\theta_{0}\right)\int P\left(Z\vert X,\theta_{0}\right)dZ=\log P\left(X\vert\theta_{0}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\int P\left(Z\vert X,\theta_{0}\right)\log\frac{P\left(X,Z\vert\theta\right)}{P\left(Z\vert X,\theta_{0}\right)}dZ
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
E-step: Starting at 
\begin_inset Formula $\theta_{t}$
\end_inset

, calculate the expectation 
\begin_inset Formula $E\left(\theta\right)=\int P\left(Z\vert X,\theta_{t}\right)\log P\left(X,Z\vert\theta\right)dZ$
\end_inset


\end_layout

\begin_layout Enumerate
M-step: Optimize 
\begin_inset Formula $\theta_{t+1}=\mathrm{argsup}E\left(\theta\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Repeat.
\end_layout

\begin_layout Standard
This is guaranteed to increase the marginal likelihood 
\begin_inset Formula $\log P\left(\theta\vert X\right)$
\end_inset

 since
\begin_inset Formula 
\begin{align*}
\mathrm{sup}_{\theta}\int P\left(Z\vert X,\theta_{t}\right)\log P\left(X,Z\vert\theta\right)dZ & =\mathrm{sup}_{\theta}\int P\left(Z\vert X,\theta_{t}\right)\log\frac{P\left(X,Z\vert\theta\right)}{P\left(Z\vert X,\theta_{t}\right)}dZ\\
 & \ge\int P\left(Z\vert X,\theta_{t}\right)\log\frac{P\left(X,Z\vert\theta_{t}\right)}{P\left(Z\vert X,\theta_{t}\right)}dZ=\log P\left(\theta_{t}\vert X\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathrm{sup}_{\theta}\int P\left(Z\vert X,\theta_{t}\right)\log P\left(X,Z\vert\theta\right)dZ
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Neyman-Scott EM algorithm
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\log P\left(X_{1n}\vert z_{n},\theta_{t}\right) & =-\frac{1}{2}\theta_{t}^{-1}\left(X_{1n}^{2}-2X_{1n}z_{n}+z_{n}^{2}\right)-\frac{1}{2}\log\theta_{t}+C\\
\log P\left(X_{2n}\vert z_{n},\theta_{t}\right) & =-\frac{1}{2}\theta_{t}^{-1}\left(X_{2n}^{2}-2X_{2n}z_{n}+z_{n}^{2}\right)-\frac{1}{2}\log\theta_{t}+C\\
\log P\left(X_{1n},X_{2n}\vert z_{n},\theta_{t}\right) & =-\frac{1}{2}\theta_{t}^{-1}\left(X_{1n}^{2}+X_{2n}^{2}-2\left(X_{1n}+X_{2n}\right)z_{n}+2z_{n}^{2}\right)-\log\theta_{t}+C
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
It follows that 
\begin_inset Formula 
\begin{align*}
P\left(z_{n}\vert X_{1n},X_{2n},\theta_{t}\right) & =\mathcal{N}\left(\frac{X_{1n}+X_{2n}}{2},\frac{\theta_{t}}{2}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{align*}
 & \int P\left(z_{n}\vert X_{1n},X_{2n},\theta_{t}\right)\log P\left(X_{1n},X_{2n},z_{n}\vert\theta\right)dZ\\
 & =-\frac{1}{2}\theta^{-1}\left(X_{1n}^{2}+X_{2n}^{2}-2\left(X_{1n}+X_{2n}\right)\mathbb{E}_{q}\left[z_{n}\right]+2\mathbb{E}_{q}\left[z_{n}^{2}\right]\right)-\frac{1}{2}\log\theta\\
 & =-\frac{1}{2}\theta^{-1}\left(X_{1n}^{2}+X_{2n}^{2}-\left(X_{1n}+X_{2n}\right)^{2}+\frac{1}{2}\left(X_{1n}+X_{2n}\right)^{2}+\theta_{t}\right)-\frac{1}{2}\log\theta\\
 & =-\frac{1}{2}\theta^{-1}\left(X_{1n}^{2}+X_{2n}^{2}-\frac{1}{2}\left(X_{1n}+X_{2n}\right)^{2}+\theta_{t}\right)-\frac{1}{2}\log\theta\\
 & =-\frac{1}{2}\theta^{-1}\left(X_{1n}^{2}+X_{2n}^{2}-\frac{1}{2}\left(X_{1n}^{2}+X_{2n}^{2}+2X_{1n}X_{2n}\right)+\theta_{t}\right)-\frac{1}{2}\log\theta\\
 & =-\frac{1}{4}\theta^{-1}\left(X_{1n}^{2}+X_{2n}^{2}-2X_{1n}X_{2n}+\theta_{t}\right)-\frac{1}{2}\log\theta\\
 & =-\frac{1}{4}\theta^{-1}\left(\left(X_{1n}-X_{2n}\right)^{2}+\theta_{t}\right)-\frac{1}{2}\log\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The next step is given by
\begin_inset Formula 
\begin{align*}
\theta_{t+1} & =\mathrm{argsup}_{\theta}-\frac{1}{2}\theta^{-1}\sum_{n=1}^{N}\left(\left(X_{1n}-X_{2n}\right)^{2}+\theta_{t}\right)-N\frac{1}{2}\log\theta\\
0 & =\frac{1}{2}\theta_{t+1}^{-2}\sum_{n=1}^{N}\left(\left(X_{1n}-X_{2n}\right)^{2}+\theta_{t}\right)-N\frac{1}{2}\theta_{t+1}^{-1}\\
\theta_{t+1} & =\frac{1}{N}\sum_{n=1}^{N}\left(\left(X_{1n}-X_{2n}\right)^{2}+\theta_{t}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
A fixed point is
\begin_inset Formula 
\begin{align*}
\hat{\theta} & =\frac{1}{N}\sum_{n=1}^{N}\left(\left(X_{1n}-X_{2n}\right)^{2}+\hat{\theta}\right)\\
\hat{\theta} & =\frac{1}{2N}\sum_{n=1}^{N}\left(X_{1n}-X_{2n}\right)^{2}\xrightarrow[n\rightarrow\infty]{}\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Banana distribution
\end_layout

\begin_layout Standard
We will have
\begin_inset Formula 
\begin{align*}
\log P_{\theta,z}\left(\theta,z\right) & =-\frac{1}{2}\left(\theta^{2}\Lambda_{\theta}+z^{2}\Lambda_{z}+2\theta z\Lambda_{\theta z}\right)+\frac{1}{2}\log\left|\Lambda\right|+C
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
To define a 
\begin_inset Quotes eld
\end_inset

banana distribution
\begin_inset Quotes erd
\end_inset

, set
\begin_inset Formula 
\begin{align*}
\tilde{\theta} & =\exp\left(\theta\right)\\
P_{\tilde{\theta},z}\left(\tilde{\theta},z\right) & =P_{\theta,z}\left(\log\tilde{\theta},z\right)\frac{d\theta}{d\tilde{\theta}}=P_{\theta,z}\left(\log\tilde{\theta},z\right)\exp\left(-\theta\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note that including the Jacobian 
\begin_inset Formula $\frac{d\theta}{d\tilde{\theta}}$
\end_inset

 changes the MAP.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\log P_{\tilde{\theta},z}\left(\tilde{\theta},z\right) & =-\frac{1}{2}\left(\left(\log\left(\tilde{\theta}\right)\right)^{2}\Lambda_{\theta}+z^{2}\Lambda_{z}+2\log\left(\tilde{\theta}\right)z\Lambda_{\theta z}\right)+\frac{1}{2}\log\left|\Lambda\right|-\log\left(\tilde{\theta}\right)+C
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Define the variational distributions
\begin_inset Formula 
\begin{align*}
q\left(\tilde{\theta}\right) & =\mathrm{Lognormal}\left(\mu_{\theta},v_{\theta}^{2}\right)\\
\mathbb{E}\left[\log\tilde{\theta}\right] & =\mu_{\theta}\\
\mathbb{E}\left[\left(\log\tilde{\theta}\right)^{2}\right] & =\mu_{\theta}^{2}+v_{\theta}^{2}\\
-\mathbb{E}\left[\log q\left(\tilde{\theta}\right)\right] & =\frac{1}{2}\log v_{\theta}^{2}+\mu_{\theta}+C
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and
\begin_inset Formula 
\begin{align*}
q\left(z\right) & =\mathcal{N}\left(\mu_{z},v_{z}^{2}\right)\\
-\mathbb{E}\left[\log q\left(z\right)\right] & =\frac{1}{2}\log v_{z}^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Thus the ELBO is given by
\begin_inset Formula 
\begin{align*}
E\left(q\right) & =-\frac{1}{2}\left(\left(\mu_{\theta}^{2}+v_{\theta}^{2}\right)\Lambda_{\theta}+\left(\mu_{z}^{2}+v_{z}^{2}\right)\Lambda_{z}+2\mu_{\theta}\mu_{z}\Lambda_{\theta z}\right)-\mu_{\theta}+\frac{1}{2}\log v_{\theta}^{2}+\mu_{\theta}+\frac{1}{2}\log v_{z}^{2}\\
 & =-\frac{1}{2}\left(\left(\mu_{\theta}^{2}+v_{\theta}^{2}\right)\Lambda_{\theta}+\left(\mu_{z}^{2}+v_{z}^{2}\right)\Lambda_{z}+2\mu_{\theta}\mu_{z}\Lambda_{\theta z}\right)+\frac{1}{2}\log v_{\theta}^{2}+\frac{1}{2}\log v_{z}^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note that the 
\begin_inset Formula $\mu_{\theta}$
\end_inset

 cancels, since the KL divergence is invariance to changes of measure.
 (Though of course the approximating distribution is not.) Note also that
 we were able to choose the exact optimal 
\begin_inset Formula $q$
\end_inset

.
 That means we are solving the same problem as in the original normal space.
 The optimum will be
\begin_inset Formula 
\begin{align*}
\mu_{\theta}=\mu_{z} & =0\\
v_{\theta}^{2} & =\Lambda_{\theta}^{-1}\\
v_{z}^{2} & =\Lambda_{z}^{-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Neyman-Scott Variational
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbe}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
X_{i,}Y_{i} & \sim\mathcal{N}\left(\alpha_{i},\nu^{-1}\right)\\
\log P\left(X_{i},Y_{i}\vert\alpha_{i},\nu\right) & =-\frac{\nu}{2}\left(X_{i}-\alpha_{i}\right)^{2}-\frac{\nu}{2}\left(Y_{i}-\alpha_{i}\right)^{2}+\frac{2}{2}\log\nu\\
 & =-\frac{\nu}{2}\left(X_{i}^{2}+Y_{i}^{2}-2\left(X_{i}+Y_{i}\right)\alpha_{i}+2\alpha_{i}^{2}\right)+\log\nu\\
\log P\left(X,Y\vert\alpha,\nu\right) & =-\frac{\nu}{2}\left(\sum_{i=1}^{N}\left(X_{i}^{2}+Y_{i}^{2}\right)-2\sum_{i=1}^{N}\left(X_{i}+Y_{i}\right)\alpha_{i}+2\sum_{i=1}^{N}\alpha_{i}^{2}\right)+N\log\nu
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
When 
\begin_inset Formula $N=1$
\end_inset

, 
\begin_inset Formula $P\left(\nu\vert X,Y,\alpha_{1}\right)$
\end_inset

 depends quadratically on 
\begin_inset Formula $\alpha_{1}$
\end_inset

.
 Thus we expect the variational approximation to do badly.
 However, when 
\begin_inset Formula $N$
\end_inset

 gets large, it depends on the average of many 
\begin_inset Formula $\alpha_{i}$
\end_inset

, which will end up being quite concentrated.
\end_layout

\begin_layout Standard
Define the variational approximation
\begin_inset Formula 
\begin{align*}
q\left(\alpha,\nu\right) & =\left(\prod_{i}q\left(\alpha_{i}\right)\right)q\left(\nu\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The updates are then given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
q^{t}\left(\alpha_{i}\right) & \leftarrow\mathcal{N}\left(\frac{X_{i}+Y_{i}}{2},\frac{1}{2\mathbb{E}^{t}\left[\nu\right]}\right)\\
\mathbb{E}_{q^{t}\left(\alpha\right)}\left[\log P\left(X_{i},Y_{i}\vert\alpha_{i},\nu\right)\right] & =-\frac{\nu}{2}\left(\left(X_{i}^{2}+Y_{i}^{2}\right)-2\left(X_{i}+Y_{i}\right)\mbe\left[\alpha_{i}\right]+2\mbe\left[\alpha_{i}^{2}\right]\right)+\log\nu\\
 & =-\frac{\nu}{2}\left(\left(X_{i}^{2}+Y_{i}^{2}\right)-2\left(X_{i}+Y_{i}\right)\mbe\left[\alpha_{i}\right]+2\left(\mbe\left[\alpha_{i}\right]^{2}+\mathrm{Var}\left(\alpha_{i}\right)\right)\right)+\log\nu
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In terms of the data and the previous step, this is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{q^{t}\left(\alpha\right)}\left[\log P\left(X_{i},Y_{i}\vert\alpha_{i},\nu\right)\right] & =-\frac{\nu}{2}\left(\left(X_{i}^{2}+Y_{i}^{2}\right)-\left(X_{i}+Y_{i}\right)^{2}+2\left(\frac{1}{4}\left(X_{i}+Y_{i}\right)^{2}+\frac{1}{2\mathbb{E}^{t}\left[\nu\right]}\right)\right)+\log\nu\\
 & =-\frac{\nu}{2}\left(\left(X_{i}^{2}+Y_{i}^{2}\right)-\left(X_{i}+Y_{i}\right)^{2}+\frac{1}{2}\left(X_{i}+Y_{i}\right)^{2}+\frac{1}{\mathbb{E}^{t}\left[\nu\right]}\right)+\log\nu\\
 & =-\frac{\nu}{2}\left(X_{i}^{2}+Y_{i}^{2}-\frac{1}{2}X_{i}^{2}-\frac{1}{2}Y_{i}^{2}-X_{i}Y_{i}+\frac{1}{\mathbb{E}^{t}\left[\nu\right]}\right)+\log\nu\\
 & =-\frac{\nu}{2}\left(\frac{1}{2}\left(X_{i}^{2}+Y_{i}^{2}-2X_{i}Y_{i}\right)+\frac{1}{\mathbb{E}^{t}\left[\nu\right]}\right)+\log\nu\\
 & =-\frac{\nu}{2}\left(\frac{1}{2}\left(X_{i}-Y_{i}\right)^{2}+\frac{1}{\mathbb{E}^{t}\left[\nu\right]}\right)+\log\nu
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Define 
\begin_inset Formula $S=\sum_{i}\left(X_{i}-Y_{i}\right)^{2}$
\end_inset

.
 Then the update for 
\begin_inset Formula $q\left(\nu\right)$
\end_inset

 i
\begin_inset Formula 
\begin{align*}
q^{t+1}\left(\nu\right) & \leftarrow\mathrm{Gamma}\left(N+1,\frac{S}{4}+\frac{1}{2\mathbb{E}^{t}\left[\nu\right]}\right)\\
\mathbb{E}^{t+1}\left[\nu\right] & =\frac{N+1}{\frac{S}{4}+\frac{1}{2\mathbb{E}^{t}\left[\nu\right]}}\Rightarrow\textrm{the optimum satisfies}\\
\mathbb{E}\left[\nu\right] & =\frac{N+1}{\frac{S}{4}+\frac{1}{2\mathbb{E}\left[\nu\right]}}\Rightarrow\\
1 & =\frac{N+1}{\frac{S}{2}\mathbb{E}\left[\nu\right]+1}\Rightarrow\\
\mathbb{E}\left[\nu\right] & =\frac{2N}{S}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathbb{E}\left[S\right] & =\mathbb{E}\left[\sum_{i}\left(X_{i}-\alpha_{i}+\alpha_{i}-Y_{i}\right)^{2}\right]\\
 & =\mathbb{E}\left[\sum_{i}\left(X_{i}-\alpha_{i}\right)+\left(Y_{i}-\alpha_{i}\right)^{2}\right]\\
 & =2N\nu_{0}^{-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
this is a consistent estimate.
 Note that if we had 
\begin_inset Formula $\mathrm{Var}_{q}\left(\alpha_{i}\right)=0$
\end_inset

, then the optimum would have been
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
q^{MLE}\left(\nu\right) & \leftarrow\mathrm{Gamma}\left(N+1,\frac{S}{4}\right)\\
\mbe\left[\nu\right] & =4\frac{N+1}{S}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
which is too large (i.e., the variance is underestimated), as expected.
\end_layout

\begin_layout Section
Variational Bayes
\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

Kullback-Liebler divergence
\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\begin{align*}
KL\left(q\left(\theta\right)||P\left(\theta\vert X\right)\right) & =\int q\left(\theta\right)\log\frac{q\left(\theta\right)}{P\left(\theta\vert X\right)}d\theta\\
KL\left(q\left(\theta\right)||P\left(\theta\vert X\right)\right) & \ge0\\
KL\left(q\left(\theta\right)||P\left(\theta\vert X\right)\right)=0 & \Leftrightarrow q\left(\theta\right)=P\left(\theta\vert X\right)\\
\\
\textrm{but}\\
KL\left(q\left(\theta\right)||P\left(\theta\vert X\right)\right) & \ne KL\left(P\left(\theta\vert X\right)||q\left(\theta\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note:
\begin_inset Formula 
\begin{align*}
P\left(\theta\vert X\right) & =\mathrm{argmin_{q}}KL\left(q\left(\theta\right)||P\left(\theta\vert X\right)\right)\\
 & =\mathrm{argmin_{q}}\int q\left(\theta\right)\log\frac{q\left(\theta\right)}{P\left(\theta\vert X\right)}d\theta\\
 & =\mathrm{argmin_{q}}\left\{ \int q\left(\theta\right)\log q\left(\theta\right)d\theta-\int q\left(\theta\right)\log P\left(\theta,X\right)P\left(\theta\right)d\theta-P\left(X\right)\right\} \\
 & =\mathrm{argmax_{q}}\left\{ -\int q\left(\theta\right)\log q\left(\theta\right)d\theta+\int q\left(\theta\right)\log P\left(\theta,X\right)P\left(\theta\right)d\theta\right\} 
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note the connection to EM:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\log P\left(X\vert\theta\right) & =\mathrm{sup}_{q}\int q\left(Z\right)\log\frac{P\left(X,Z\vert\theta\right)}{q\left(Z\right)}dZ\\
q\left(Z\right) & =\mathrm{argsup}_{q}\int q\left(Z\right)\log\frac{P\left(X,Z\vert\theta\right)}{q\left(Z\right)}dZ\\
 & =\mathrm{argmin}_{q}\int q\left(Z\right)\log\frac{q\left(Z\right)}{P\left(X,Z\vert\theta\right)}dZ\\
 & =\mathrm{argmin}_{q}KL\left(q\left(Z\right)||P\left(X,Z\vert\theta\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Maybe this connection is best made later.
\end_layout

\begin_layout Standard
In general, the entropy term is hard.
 So choose a tractable family and find the closest approximating distribution.
\begin_inset Formula 
\begin{align*}
\mathcal{Q} & =\left\{ q\textrm{ that are tractable in some way}\right\} \\
q^{*}\left(\theta\right) & =\mathrm{argmin_{q\in\mathcal{Q}}}KL\left(q\left(\theta\right)||P\left(\theta\vert X\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathcal{Q} & =\left\{ q\left(\theta\right)=\prod_{k}q\left(\theta_{k}\right)\right\} \\
q^{*}\left(\theta\right) & =\mathrm{argmin_{q\in\mathcal{Q}}}KL\left(q\left(\theta\right)||P\left(\theta\vert X\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Suppose we assume that
\begin_inset Formula 
\begin{align*}
\mathcal{Q} & =\left\{ q\left(\theta,Z\right)=\delta\left(\theta-\theta_{0}\right)q\left(Z\right)\right\} \\
q\left(\theta,Z\right) & =\mathrm{argmin}_{q}KL\left(q||P\left(\theta,Z\vert X\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Then you recover the EM algorithm.
 Keeping 
\begin_inset Formula $q\left(\theta\right)$
\end_inset

 fixed,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
q^{*}\left(Z\right) & =\mathrm{argmin}_{q\left(Z\right)}KL\left(q\left(Z\right)\delta\left(\theta-\theta_{0}\right)||P\left(\theta,Z\vert X\right)\right)\\
 & =\mathrm{argmin}_{q\left(Z\right)}\int q\left(Z\right)\delta\left(\theta-\theta_{0}\right)\frac{q\left(Z\right)\delta\left(\theta-\theta_{0}\right)}{\log P\left(\theta,Z\vert X\right)}dZd\theta\\
 & =\mathrm{argmin}_{q\left(Z\right)}\int q\left(Z\right)\frac{q\left(Z\right)}{\log P\left(\theta_{0},Z\vert X\right)}dZ\\
 & =\mathrm{argmax}_{q\left(Z\right)}\int q\left(Z\right)\frac{\log P\left(\theta_{0},Z\vert X\right)}{q\left(Z\right)}dZ\\
 & =\mathrm{argmax}_{q\left(Z\right)}\int q\left(Z\right)\frac{\log P\left(Z,X\vert\theta_{0}\right)}{q\left(Z\right)}dZ\\
 & =P\left(Z\vert\theta_{0},X\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And keeping 
\begin_inset Formula $q\left(Z\right)$
\end_inset

 fixed,
\begin_inset Formula 
\begin{align*}
\theta_{0} & =\mathrm{argmin}_{\theta_{0}}KL\left(q\left(Z\right)\delta\left(\theta-\theta_{0}\right)||P\left(\theta,Z\vert X\right)\right)\\
 & =\mathrm{argmax}_{\theta_{0}}\int q\left(Z\right)\log P\left(\theta_{0},Z\vert X\right)dZ\\
 & =\mathrm{argmax}_{\theta_{0}}\int q\left(Z\right)\log P\left(X,Z\vert\theta_{0}\right)dZ
\end{align*}

\end_inset


\end_layout

\begin_layout Section
MLE version
\end_layout

\begin_layout Standard
It is well-known that the MLE is biased.
 Suppose we additionally posit
\begin_inset Formula 
\begin{align*}
\alpha_{i} & \sim\mathcal{N}\left(0,\nu_{\alpha}^{-1}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{align*}
\mathrm{Cov}\left(X_{i},Y_{i}\right) & =\mbe\left[X_{i}Y_{i}\right]\\
 & =\mbe\left[\left(X_{i}-\alpha_{i}+\alpha_{i}\right)\left(Y_{i}-\alpha_{i}+\alpha_{i}\right)\right]\\
 & =\mbe\left[\left(X_{i}-\alpha_{i}\right)\left(Y_{i}-\alpha_{i}\right)\right]+\mbe\left[\alpha_{i}\left(X_{i}-\alpha_{i}\right)\right]+\mbe\left[\alpha_{i}\left(Y_{i}-\alpha_{i}\right)\right]+\mbe\left[\alpha_{i}^{2}\right]\\
 & =\nu_{\alpha}^{-1}\\
X_{i},Y_{i}\vert\nu,\nu_{\alpha} & =\mathcal{N}\left(\left(\begin{array}{c}
0\\
0
\end{array}\right),\left(\begin{array}{cc}
\nu^{-1}+\nu_{\alpha}^{-1} & \nu_{\alpha}^{-1}\\
\nu_{\alpha}^{-1} & \nu^{-1}+\nu_{\alpha}^{-1}
\end{array}\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and the covariance matrix of 
\begin_inset Formula $\left(X_{i},Y_{i}\right)$
\end_inset

 can be estimated consistently by standard results.
\end_layout

\end_body
\end_document
