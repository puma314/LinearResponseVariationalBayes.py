{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.core import primitive\n",
    "from autograd import grad, jacobian, hessian\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@primitive\n",
    "def MySum(x):\n",
    "    return 2 * np.sum(x)\n",
    "\n",
    "def MySameSum(x):\n",
    "    return 2 * np.sum(x)\n",
    "\n",
    "def MySum_vjp(g, ans, vs, gvs, x):\n",
    "    return np.full(x.shape, g) * np.full(x.shape, 2)\n",
    "\n",
    "MySum.defvjp(MySum_vjp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: <function VJP_0_of_BinSum at 0x7f966fcc8578>, 1: <function VJP_1_of_BinSum at 0x7f966fcc82a8>}\n",
      "2.5\n",
      "1.0\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "@primitive\n",
    "def BinSum(x, y):\n",
    "    return y * x ** 2\n",
    "\n",
    "# It appears that the gradient is always with respect to the argument specified in argnum,\n",
    "# which defaults to zero (the first argument)\n",
    "\n",
    "def BinSum_vjp_x(g, ans, vs, gvs, x, y):\n",
    "    return unbroadcast(vs, gvs, g * 2 * x * y)\n",
    "\n",
    "global_vs = 0\n",
    "global_gvs = 0\n",
    "def BinSum_vjp_y(g, ans, vs, gvs, x, y):\n",
    "    global global_vs\n",
    "    global global_gvs\n",
    "    global_vs = vs\n",
    "    global_gvs = gvs\n",
    "    return unbroadcast(vs, gvs, g * 2 * x ** 2)\n",
    "\n",
    "BinSum.defvjp(BinSum_vjp_x, argnum=0)\n",
    "BinSum.defvjp(BinSum_vjp_y, argnum=1)\n",
    "\n",
    "BinSumGradX = grad(BinSum)\n",
    "BinSumGradY = grad(BinSum, argnum=1)\n",
    "print BinSum.vjps\n",
    "print BinSum(5., 0.1)\n",
    "print BinSumGradX(5., 0.1)\n",
    "print BinSumGradY(5., 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2000.  600000.]\n",
      "[[   1000.       0.]\n",
      " [      0.  200000.]]\n",
      "[   1000.  200000.]\n"
     ]
    }
   ],
   "source": [
    "# Looking at the derivative of diag, it appears tha the output is supposed\n",
    "# to have the same dimension as the input, but with g in the appropriate places?\n",
    "# anp.diag.defvjp(   lambda g, ans, vs, gvs, x, k=0          : anp.diag(g, k))\n",
    "\n",
    "# How does unbroadcast work?\n",
    "\n",
    "@primitive\n",
    "def ElementwiseProd(x, y):\n",
    "    return x * y\n",
    "\n",
    "def ElementwiseProd_vjp(g, ans, vs, gvs, x, y):\n",
    "#     return unbroadcast(vs, gvs, y) # Wrong.\n",
    "    return g * y\n",
    "\n",
    "ElementwiseProd.defvjp(ElementwiseProd_vjp)\n",
    "\n",
    "def AnotherProd(x, z, y):\n",
    "    return z * ElementwiseProd(x, y)\n",
    "\n",
    "AnotherProdJac = jacobian(AnotherProd)\n",
    "\n",
    "x = np.array([2., 3.])\n",
    "y = np.array([10., 200.])\n",
    "z = np.array([100., 1000.])\n",
    "print AnotherProd(x, y, z)\n",
    "print AnotherProdJac(x, z, y)\n",
    "print z * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 1)\n",
      "[25  1]\n",
      "[[ 0.  5.  0.  0.]\n",
      " [ 0.  0.  6.  0.]\n",
      " [ 0.  0.  0.  7.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([5, 1])\n",
    "y = np.expand_dims(x, 5)\n",
    "print x.shape\n",
    "print y.shape\n",
    "print x ** 2\n",
    "print np.diag(np.array([5., 6., 7.]), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.]\n",
      "[ 2.  2.]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "(13,)\n",
      "<function VJP_0_of_MySum at 0x7f966fcf07d0>\n"
     ]
    }
   ],
   "source": [
    "MySumGrad = grad(MySum)\n",
    "MySameSumGrad = grad(MySameSum)\n",
    "\n",
    "x = np.array([2., 4.])\n",
    "print MySumGrad(x)\n",
    "print MySameSumGrad(x)\n",
    "\n",
    "x_full = np.full(13, x.shape)\n",
    "print x_full\n",
    "print x_full.shape\n",
    "\n",
    "print MySum.vjps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stand alone:\n",
      "g: \n",
      "1.0\n",
      "ans: \n",
      "Autograd ArrayNode with value 20.0 and 1 progenitors(s)\n",
      "vs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (2,), 'scalartype': <type 'float'>, 'size': 2}\n",
      "gvs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (), 'scalartype': <type 'float'>, 'size': 1}\n",
      "x: \n",
      "Autograd ArrayNode with value [ 2.  4.] and 1 progenitors(s)\n",
      "Returning.\n",
      "[ 4.  8.]\n",
      "\n",
      "In function:\n",
      "g: \n",
      "3.0\n",
      "ans: \n",
      "Autograd ArrayNode with value 20.0 and 1 progenitors(s)\n",
      "vs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (2,), 'scalartype': <type 'float'>, 'size': 2}\n",
      "gvs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (), 'scalartype': <type 'float'>, 'size': 1}\n",
      "x: \n",
      "Autograd ArrayNode with value [ 2.  4.] and 1 progenitors(s)\n",
      "Returning.\n",
      "[ 12.  24.]\n",
      "\n",
      "In Hessian:\n",
      "g: \n",
      "3.0\n",
      "ans: \n",
      "Autograd ArrayNode with value 20.0 and 2 progenitors(s)\n",
      "vs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (2,), 'scalartype': <type 'float'>, 'size': 2}\n",
      "gvs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (), 'scalartype': <type 'float'>, 'size': 1}\n",
      "x: \n",
      "Autograd ArrayNode with value [ 2.  4.] and 2 progenitors(s)\n",
      "Returning.\n",
      "[[ 6.  0.]\n",
      " [ 0.  6.]]\n"
     ]
    }
   ],
   "source": [
    "@primitive\n",
    "def SumSq(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def SumSq_vjp(g, ans, vs, gvs, x):\n",
    "    print 'g: '\n",
    "    print g\n",
    "    print 'ans: '\n",
    "    print ans\n",
    "    print 'vs: '\n",
    "    print vs\n",
    "    print 'gvs: '\n",
    "    print gvs\n",
    "    print 'x: '\n",
    "    print x\n",
    "    print 'Returning.'\n",
    "    return np.full(x.shape, g) * 2 * x\n",
    "\n",
    "SumSq.defvjp(SumSq_vjp)\n",
    "\n",
    "def MyFun(x):\n",
    "    return 3 * SumSq(x)\n",
    "\n",
    "SumSqGrad = grad(SumSq)\n",
    "MyFunGrad = grad(MyFun)\n",
    "MyFunHess = hessian(MyFun)\n",
    "\n",
    "x = np.array([2., 4.])\n",
    "print 'Stand alone:'\n",
    "print SumSqGrad(x)\n",
    "\n",
    "print '\\nIn function:'\n",
    "print MyFunGrad(x)\n",
    "\n",
    "print '\\nIn Hessian:'\n",
    "print MyFunHess(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: \n",
      "0.0\n",
      "ans: \n",
      "Autograd ArrayNode with value 20.0 and 1 progenitors(s)\n",
      "vs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (2,), 'scalartype': <type 'float'>, 'size': 2}\n",
      "gvs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (), 'scalartype': <type 'float'>, 'size': 1}\n",
      "x: \n",
      "Autograd ArrayNode with value [ 2.  4.] and 1 progenitors(s)\n",
      "Returning.\n",
      "g: \n",
      "1.0\n",
      "ans: \n",
      "Autograd ArrayNode with value 20.0 and 1 progenitors(s)\n",
      "vs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (2,), 'scalartype': <type 'float'>, 'size': 2}\n",
      "gvs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (), 'scalartype': <type 'float'>, 'size': 1}\n",
      "x: \n",
      "Autograd ArrayNode with value [ 2.  4.] and 1 progenitors(s)\n",
      "Returning.\n",
      "g: \n",
      "5.0\n",
      "ans: \n",
      "Autograd ArrayNode with value 20.0 and 1 progenitors(s)\n",
      "vs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (2,), 'scalartype': <type 'float'>, 'size': 2}\n",
      "gvs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (), 'scalartype': <type 'float'>, 'size': 1}\n",
      "x: \n",
      "Autograd ArrayNode with value [ 2.  4.] and 1 progenitors(s)\n",
      "Returning.\n",
      "g: \n",
      "0.0\n",
      "ans: \n",
      "Autograd ArrayNode with value 20.0 and 1 progenitors(s)\n",
      "vs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (2,), 'scalartype': <type 'float'>, 'size': 2}\n",
      "gvs: \n",
      "ArrayVSpace_{'dtype': dtype('float64'), 'shape': (), 'scalartype': <type 'float'>, 'size': 1}\n",
      "x: \n",
      "Autograd ArrayNode with value [ 2.  4.] and 1 progenitors(s)\n",
      "Returning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  4.,   8.],\n",
       "       [ 20.,  40.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autograd import jacobian\n",
    "\n",
    "def MyVecFun(x):\n",
    "\n",
    "    return np.array([ SumSq(x), 5 * SumSq(x) ])\n",
    "\n",
    "MyVecJac = jacobian(MyVecFun)\n",
    "MyVecJac(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0\n",
      "[ 12.  27.]\n",
      "[[ 12.   0.]\n",
      " [  0.  18.]]\n",
      "35.0\n",
      "[ 12.  27.]\n",
      "[[ 12.   0.]\n",
      " [  0.  18.]]\n"
     ]
    }
   ],
   "source": [
    "# Define my own Hessian\n",
    "\n",
    "@primitive\n",
    "def Magnitude(x):\n",
    "    return np.sum(x ** 3)\n",
    "\n",
    "def MagnitudeRaw(x):\n",
    "    return np.sum(x ** 3)\n",
    "\n",
    "@primitive\n",
    "def MagnitudeGradPrimitive(x):\n",
    "    return 3 * (x ** 2)\n",
    "\n",
    "def MagnitudeGradJac(x):\n",
    "    return 6 * np.diag(x)\n",
    "\n",
    "def Magnitude_vjp(g, ans, vs, gvs, x):\n",
    "    return unbroadcast(vs, gvs, g * MagnitudeGradPrimitive(x))\n",
    "\n",
    "def MagnitudeGrad_vjp(g, ans, vs, gvs, x):\n",
    "    return np.matmul(MagnitudeGradJac(x), g)\n",
    "\n",
    "\n",
    "Magnitude.defvjp(Magnitude_vjp)\n",
    "MagnitudeGradPrimitive.defvjp(MagnitudeGrad_vjp)\n",
    "\n",
    "MagnitudeGrad = grad(Magnitude)\n",
    "MagnitudeHess = hessian(Magnitude)\n",
    "\n",
    "MagnitudeGradRaw = grad(MagnitudeRaw)\n",
    "MagnitudeHessRaw = hessian(MagnitudeRaw)\n",
    "\n",
    "x = np.array([2., 3.])\n",
    "print Magnitude(x)\n",
    "print MagnitudeGrad(x)\n",
    "print MagnitudeHess(x)\n",
    "\n",
    "print MagnitudeRaw(x)\n",
    "print MagnitudeGradRaw(x)\n",
    "print MagnitudeHessRaw(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.00613961607e-16\n",
      "-9.02056207508e-17\n"
     ]
    }
   ],
   "source": [
    "# Making Logistic operate on vectors may be a headache.\n",
    "\n",
    "# For testing\n",
    "def LogisticRaw(rho):\n",
    "    if rho <= 0:\n",
    "        exp_rho = np.exp(rho)\n",
    "        return exp_rho / (1 + exp_rho)\n",
    "    else:\n",
    "        mexp_rho = np.exp(-rho)\n",
    "        return 1 / (1 + mexp_rho)\n",
    "\n",
    "@primitive\n",
    "def Logistic(rho):\n",
    "    if rho <= 0:\n",
    "        exp_rho = np.exp(rho)\n",
    "        return exp_rho / (1 + exp_rho)\n",
    "    else:\n",
    "        mexp_rho = np.exp(-rho)\n",
    "        return 1 / (1 + mexp_rho)\n",
    "\n",
    "@primitive\n",
    "def LogisticGradient(logit_rho):\n",
    "    return logit_rho * (1 - logit_rho)\n",
    "\n",
    "def LogisticHessian(logit_rho):\n",
    "    return 1 - 2 * logit_rho\n",
    "\n",
    "def Logistic_vjp(g, ans, vs, gvs, x):\n",
    "    return unbroadcast(vs, gvs, g * LogisticGradient(ans))\n",
    "\n",
    "def LogisticGradient_vjp(g, ans, vs, gvs, x):\n",
    "    return unbroadcast(vs, gvs, g * LogisticHessian(x))\n",
    "\n",
    "Logistic.defvjp(Logistic_vjp)\n",
    "LogisticGradient.defvjp(LogisticGradient_vjp)\n",
    "\n",
    "LogisticADGrad = grad(Logistic)\n",
    "LogisticADHessian = hessian(Logistic)\n",
    "\n",
    "LogisticRawADGrad = grad(LogisticRaw)\n",
    "LogisticRawADHessian = hessian(LogisticRaw)\n",
    "\n",
    "x = 3.6\n",
    "print Logistic(x) - LogisticRaw(x)\n",
    "print LogisticADGrad(x) - LogisticRawADGrad(x)\n",
    "print LogisticADHessian(x) - LogisticRawADHessian(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.775416290764\n",
      "-1.10540049589\n",
      "-0.574442516812\n",
      "-0.244458311691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Log1mInvLogitRaw(u):\n",
    "    return -np.log1p(np.exp(u))\n",
    "\n",
    "@primitive\n",
    "def Log1mInvLogit(exp_u):\n",
    "    return -np.log1p(exp_u)\n",
    "  \n",
    "# @primitive\n",
    "def Log1mInvLogitDerivative(exp_u):\n",
    "    return -exp_u / (1 + exp_u)\n",
    "\n",
    "# @primitive\n",
    "# def Log1mInvLogitSecondDerivative(logit_u):\n",
    "#     return -logit_u * (1 - logit_u)\n",
    "\n",
    "def Log1mInvLogit_vjp(g, ans, vs, gvs, exp_u):\n",
    "    return unbroadcast(vs, gvs, g * Log1mInvLogitDerivative(exp_u))\n",
    "\n",
    "# def Log1mInvLogitDerivative_vjp(g, ans, vs, gvs, logit_u):\n",
    "#     return unbroadcast(vs, gvs, g * Log1mInvLogitSecondDerivative(logit_u))\n",
    "\n",
    "def Log1mInvLogitOneArg(u):\n",
    "    exp_u = np.exp(u)\n",
    "    return Log1mInvLogit(exp_u)\n",
    "\n",
    "Log1mInvLogit.defvjp(Log1mInvLogit_vjp, argnum=0)\n",
    "# Log1mInvLogit.defvjp(lambda g, ans, vs, gvs, exp_u, logit_u: unbroadcast(vs, gvs, 0.), argnum=1)\n",
    "# Log1mInvLogitDerivative.defvjp(Log1mInvLogitDerivative_vjp)\n",
    "u = 0.3\n",
    "\n",
    "\n",
    "Log1mInvLogitGrad = grad(Log1mInvLogitOneArg)\n",
    "Log1mInvLogitHess = hessian(Log1mInvLogitOneArg)\n",
    "\n",
    "Log1mInvLogitRawGrad = grad(Log1mInvLogitRaw)\n",
    "Log1mInvLogitRawHess = hessian(Log1mInvLogitRaw)\n",
    "\n",
    "print Log1mInvLogitGrad(u)\n",
    "print Log1mInvLogitHess(u)\n",
    "\n",
    "print Log1mInvLogitRawGrad(u)\n",
    "print Log1mInvLogitRawHess(u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
