{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "import autograd\n",
    "from VariationalBayes import Modeling as modeling\n",
    "\n",
    "import autograd.numpy as np\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.16028637  3.04733199  2.5133961   2.61091719  3.00178654]\n",
      " [ 2.4167367   2.88255359  2.21055327  2.6437328   2.19039271]\n",
      " [ 2.57782739  2.90933501  2.86792556  2.47122327  2.68260904]]\n"
     ]
    }
   ],
   "source": [
    "def fun(z):\n",
    "    return np.log1p(np.exp(z))\n",
    "\n",
    "def fun_grad(z):\n",
    "    return np.exp(z) / (1 + np.exp(z))\n",
    "\n",
    "def fun_hess(z):\n",
    "    p = np.exp(z) / (1 + np.exp(z))\n",
    "    return p * (1 - p)\n",
    "\n",
    "def log_fun(z):\n",
    "    return np.log(fun(z))\n",
    "\n",
    "def log_fun_grad(z):\n",
    "    return 0. * fun(z)\n",
    "    return fun_grad(z) / fun(z)\n",
    "\n",
    "def log_fun_hess(z):\n",
    "    f_z = fun(z)\n",
    "    return 0. * f_z\n",
    "    return fun_hess(z) / f_z - (fun_grad(z) / f_z) ** 2\n",
    "\n",
    "std_draws = modeling.get_standard_draws(20)\n",
    "\n",
    "z_mean = np.random.random((3, 5)) + 2.0\n",
    "z_sd = np.exp(np.random.random((3, 5)) - 1.0)\n",
    "\n",
    "z0 = z_mean\n",
    "imp_means = modeling.importance_sampling_integrate_univariate_normal(\n",
    "    z_mean, z_sd, z0, log_fun, log_fun_grad, log_fun_hess, std_draws, aggregate_all=False)\n",
    "\n",
    "print(imp_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.16391862,  3.04836873,  2.51815227,  2.61356615,  3.00455871],\n",
       "       [ 2.41882425,  2.88772953,  2.21332966,  2.652864  ,  2.1926762 ],\n",
       "       [ 2.58153411,  2.91111032,  2.86910251,  2.47452704,  2.68775121]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def gh_integrate(z_mean, z_sd, num_std_draws, aggregate_all=True):\n",
    "    gh_x, gh_w = onp.polynomial.hermite.hermgauss(num_std_draws)\n",
    "    draws_axis = z_sd.ndim\n",
    "\n",
    "    z_draws = \\\n",
    "        np.sqrt(2) * np.expand_dims(z_sd, axis=draws_axis) * gh_x + \\\n",
    "        np.expand_dims(z_mean, axis=draws_axis)\n",
    "\n",
    "    # By dividing by the number of standard draws after summing,\n",
    "    # we add the sample means for all the observations.\n",
    "    # Note that\n",
    "    # log(1 - p) = log(1 / (1 + exp(z))) = -log(1 + exp(z))\n",
    "    logit_term = gh_w * np.log1p(np.exp(z_draws)) / np.sqrt(np.pi)\n",
    "    if aggregate_all:\n",
    "        return np.sum(logit_term)\n",
    "    else:\n",
    "        return np.sum(logit_term, axis=draws_axis)\n",
    "\n",
    "    \n",
    "gh_integrate(z_mean, z_sd, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  5 10 30 50 70 80]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa2a059e7f0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwMUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa2a05b25f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "std_draws_vec = np.array([ 3, 5, 10, 30, 50, 70, 80, 100, 200, 500, 5000 ])\n",
    "\n",
    "imp_means = \\\n",
    "    [ modeling.importance_sampling_integrate_univariate_normal(\n",
    "      z_mean, z_sd, z0, log_fun, log_fun_grad, log_fun_hess,\n",
    "      modeling.get_standard_draws(num_std_draws), aggregate_all=True) \\\n",
    "      for num_std_draws in std_draws_vec ]\n",
    "    \n",
    "direct_means = \\\n",
    "    [ modeling.get_e_logistic_term_only(z_mean, z_sd, modeling.get_standard_draws(num_std_draws)) \\\n",
    "      for num_std_draws in std_draws_vec ]\n",
    "\n",
    "gh_std_draws_vec = std_draws_vec[std_draws_vec < 100]\n",
    "print(gh_std_draws_vec)\n",
    "gh_means = \\\n",
    "    [ gh_integrate(z_mean, z_sd, num_std_draws, True) \\\n",
    "      for num_std_draws in gh_std_draws_vec ]\n",
    "    \n",
    "    \n",
    "plt.plot(np.log10(std_draws_vec), direct_means, 'k.')\n",
    "plt.plot(np.log10(std_draws_vec), imp_means, 'r.')\n",
    "plt.plot(np.log10(gh_std_draws_vec), gh_means, 'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.log10(std_draws_vec), direct_means, 'k.')\n",
    "plt.plot(np.log10(std_draws_vec), imp_means, 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VariationalBayes.Modeling import univariate_normal_log_prob\n",
    "\n",
    "# q(u) will be a univariate normal importance sampling distribution.\n",
    "# Its natural parameters are given by a Taylor expansion of log_fun.\n",
    "z_info = 1 / z_sd ** 2\n",
    "\n",
    "z_nat_param = z_info * z_mean\n",
    "z2_nat_param = -0.5 * z_info\n",
    "\n",
    "u_nat_param = z_nat_param + log_fun_grad(z0)\n",
    "u2_nat_param = z2_nat_param + log_fun_hess(z0)\n",
    "\n",
    "u_info = -2 * u2_nat_param\n",
    "u_sd = 1. / np.sqrt(u_info)\n",
    "u_mean = u_nat_param / u_info\n",
    "\n",
    "# print('log fun --------')\n",
    "# print(log_fun_grad(z0))\n",
    "# print(log_fun_hess(z0))\n",
    "\n",
    "# print('z nat params --------')\n",
    "# print(z_nat_param)\n",
    "# print(z2_nat_param)\n",
    "\n",
    "# print('u nat parmas --------')\n",
    "# print(u_nat_param)\n",
    "# print(u2_nat_param)\n",
    "\n",
    "# print('u sd and mean --------')\n",
    "# print(u_sd)\n",
    "# print(u_mean)\n",
    "\n",
    "# print('z sd and mean --------')\n",
    "# print(z_sd)\n",
    "# print(z_mean)\n",
    "\n",
    "draws_axis = u_sd.ndim\n",
    "u_draws = \\\n",
    "    np.expand_dims(u_sd, axis=draws_axis) * std_draws + \\\n",
    "    np.expand_dims(u_mean, axis=draws_axis)\n",
    "\n",
    "u_log_prob = univariate_normal_log_prob(\n",
    "    u_draws,\n",
    "    np.expand_dims(u_mean, axis=draws_axis),\n",
    "    np.expand_dims(u_info, axis=draws_axis))\n",
    "z_log_prob = univariate_normal_log_prob(\n",
    "    u_draws,\n",
    "    np.expand_dims(z_mean, axis=draws_axis),\n",
    "    np.expand_dims(z_info, axis=draws_axis))\n",
    "log_f_z = log_fun(u_draws)\n",
    "\n",
    "# Importance sampling\n",
    "log_imp_weights = np.exp(z_log_prob + log_f_z - u_log_prob)\n",
    "\n",
    "result = np.sum(log_imp_weights, axis=draws_axis) / len(std_draws)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VariationalBayes as vb\n",
    "import Models.LogisticGLMM_lib as logit_glmm\n",
    "from VariationalBayes.SparseObjectives import Objective, SparseObjective\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from autograd import jacobian\n",
    "\n",
    "import copy\n",
    "from scipy import optimize\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "analysis_name = 'criteo_subsampled'\n",
    "\n",
    "data_dir = os.path.join(os.environ['GIT_REPO_LOC'], 'LRVBLogitGLMM/LogitGLMMLRVB/inst/data/')\n",
    "json_filename = os.path.join(data_dir, '%s_stan_dat.json' % analysis_name)\n",
    "json_output_filename = os.path.join(data_dir, '%s_python_vb_results.json' % analysis_name)\n",
    "\n",
    "json_file = open(json_filename, 'r')\n",
    "json_dat = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "stan_dat = json_dat['stan_dat']\n",
    "vp_base = json_dat['vp_base']\n",
    "\n",
    "print(stan_dat.keys())\n",
    "K = stan_dat['K'][0]\n",
    "NObs = stan_dat['N'][0]\n",
    "NG = stan_dat['NG'][0]\n",
    "#N = NObs / NG\n",
    "y_g_vec = np.array(stan_dat['y_group'])\n",
    "y_vec = np.array(stan_dat['y'])\n",
    "x_mat = np.array(stan_dat['x'])\n",
    "\n",
    "mu_info_min = vp_base['mu_info_min'][0]\n",
    "tau_alpha_min = vp_base['tau_alpha_min'][0]\n",
    "tau_beta_min = vp_base['tau_beta_min'][0]\n",
    "beta_diag_min = vp_base['beta_diag_min'][0]\n",
    "u_info_min = vp_base['u_info_min'][0]\n",
    "\n",
    "# Define a class to contain prior parameters.\n",
    "prior_par = vb.ModelParamsDict('Prior Parameters')\n",
    "\n",
    "prior_par.push_param(vb.VectorParam('beta_prior_mean', K, val=np.array(stan_dat['beta_prior_mean'])))\n",
    "beta_prior_info = np.linalg.inv(np.array(stan_dat['beta_prior_var']))\n",
    "prior_par.push_param(vb.PosDefMatrixParam('beta_prior_info', K, val=beta_prior_info))\n",
    "\n",
    "prior_par.push_param(vb.ScalarParam('mu_prior_mean', val=stan_dat['mu_prior_mean'][0]))\n",
    "prior_par.push_param(vb.ScalarParam('mu_prior_info', val=1 / stan_dat['mu_prior_var'][0]))\n",
    "\n",
    "prior_par.push_param(vb.ScalarParam('tau_prior_alpha', val=stan_dat['tau_prior_alpha'][0]))\n",
    "prior_par.push_param(vb.ScalarParam('tau_prior_beta', val=stan_dat['tau_prior_beta'][0]))\n",
    "\n",
    "# An index set to make sure jacobians match the order expected by R.\n",
    "prior_par_indices = copy.deepcopy(prior_par)\n",
    "prior_par_indices.set_name('Prior Indices')\n",
    "prior_par_indices.set_vector(np.array(range(prior_par_indices.vector_size())))\n",
    "\n",
    "glmm_fit = json_dat['glmm_fit']\n",
    "glmm_par['mu'].mean.set(glmm_fit['mu_mean'][0])\n",
    "glmm_par['mu'].info.set(1.0)\n",
    "\n",
    "tau_mean = 1.0 / glmm_fit['mu_sd'][0] ** 2\n",
    "tau_var = 1.0\n",
    "glmm_par['tau'].shape.set((tau_mean ** 2) / tau_var)\n",
    "glmm_par['tau'].rate.set(tau_var / tau_mean)\n",
    "\n",
    "glmm_par['beta'].mean.set(np.array(glmm_fit['beta_mean']))\n",
    "glmm_par['beta'].info.set(np.eye(K))\n",
    "\n",
    "glmm_par['u'].mean.set(np.array(glmm_fit['u_map']))\n",
    "glmm_par['u'].info.set(np.full(NG, 1.0))\n",
    "\n",
    "free_par_vec = glmm_par.get_free()\n",
    "init_par_vec = copy.deepcopy(free_par_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glmm_par = vb.ModelParamsDict('GLMM Parameters')\n",
    "\n",
    "# print(vp_base)\n",
    "\n",
    "glmm_par.push_param(\n",
    "    vb.UVNParam('mu', min_info=mu_info_min))\n",
    "glmm_par.push_param(\n",
    "    vb.GammaParam('tau', min_shape=tau_alpha_min, min_rate=tau_beta_min))\n",
    "glmm_par.push_param(vb.MVNParam('beta', K, min_info=beta_diag_min))\n",
    "glmm_par.push_param(vb.UVNParamVector('u', NG, min_info=u_info_min))\n",
    "\n",
    "model = logit_glmm.LogisticGLMM(glmm_par, prior_par, x_mat, y_vec, y_g_vec, 10)\n",
    "model.get_e_log_prior()\n",
    "model.get_log_lik()\n",
    "model.get_entropy()\n",
    "\n",
    "objective = Objective(model.glmm_par, model.get_kl)\n",
    "objective.fun_free(free_par_vec)\n",
    "\n",
    "glmm_par_opt = copy.deepcopy(glmm_par)\n",
    "def tr_optimize(trust_init, num_draws, maxiter=500):\n",
    "    model.set_draws(num_draws)\n",
    "    objective.logger.initialize()\n",
    "    objective.logger.print_every = 5\n",
    "    vb_opt = optimize.minimize(\n",
    "        lambda par: objective.fun_free(par, verbose=True),\n",
    "        x0=trust_init,\n",
    "        method='trust-ncg',\n",
    "        jac=objective.fun_free_grad,\n",
    "        hessp=objective.fun_free_hvp,\n",
    "        tol=1e-6, options={'maxiter': maxiter, 'disp': True, 'gtol': 1e-6 })\n",
    "    return vb_opt.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_beta = glmm_par_opt['beta'].e()\n",
    "e_u = glmm_par_opt['u'].e()[model.y_g_vec]\n",
    "cov_beta = glmm_par['beta'].cov()\n",
    "var_u = glmm_par['u'].var()\n",
    "\n",
    "\n",
    "z_mean = e_u[y_g_vec] + np.matmul(x_mat, e_beta)\n",
    "z_sd = np.sqrt(\n",
    "    var_u[y_g_vec] + np.einsum('nk,kj,nj->n',\n",
    "                      x_mat, cov_beta, x_mat))\n",
    "\n",
    "std_draws_vec = [ 3, 5, 10, 30, 50, 100, 200, 500, 5000 ]\n",
    "\n",
    "z0 = z_mean\n",
    "print('Importance sampling')\n",
    "imp_means = \\\n",
    "    [ modeling.importance_sampling_integrate_univariate_normal(\n",
    "      z_mean, z_sd, z0, log_fun, log_fun_grad, log_fun_hess,\n",
    "      modeling.get_standard_draws(num_std_draws), aggregate_all=True) \\\n",
    "      for num_std_draws in std_draws_vec ]\n",
    "    \n",
    "print('Direct sampling')\n",
    "direct_means = \\\n",
    "    [ modeling.get_e_logistic_term_only(z_mean, z_sd, modeling.get_standard_draws(num_std_draws)) \\\n",
    "      for num_std_draws in std_draws_vec ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_mean.shape)\n",
    "\n",
    "plt.plot(np.log10(std_draws_vec), direct_means, 'k.')\n",
    "plt.plot(np.log10(std_draws_vec), imp_means, 'r.')\n",
    "plt.plot(0, 0, 'r.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
