{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.core import primitive\n",
    "from autograd import grad, jacobian, hessian\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@primitive\n",
    "def MySum(x):\n",
    "    return 2 * np.sum(x)\n",
    "\n",
    "def MySameSum(x):\n",
    "    return 2 * np.sum(x)\n",
    "\n",
    "def MySum_vjp(g, ans, vs, gvs, x):\n",
    "    return np.full(x.shape, g) * np.full(x.shape, 2)\n",
    "\n",
    "MySum.defvjp(MySum_vjp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatEncode(mat, K):\n",
    "    return mat[np.triu_indices(K)]\n",
    "\n",
    "def EncodeInd(k1, k2):\n",
    "    def LDInd(k1, k2):\n",
    "        return k2 + k1 * (k1 + 1) / 2\n",
    "\n",
    "    if k2 <= k1:\n",
    "        return LDInd(k1, k2)\n",
    "    else:\n",
    "        return LDInd(k2, k1)\n",
    "    \n",
    "def MatDecode(vec, K):\n",
    "    nums = []\n",
    "    for k1 in range(K):\n",
    "        for k2 in range(K):\n",
    "            nums.append(vec[EncodeInd(k1, k2)])\n",
    "    return np.array(nums).reshape(K, K)\n",
    "\n",
    "K = 2\n",
    "mat = np.random.rand(K * K).reshape(K, K)\n",
    "mat = mat * mat.T\n",
    "vec = MatEncode(mat, K)\n",
    "print mat - MatDecode(vec, K)\n",
    "\n",
    "# works:\n",
    "MatEncodeJac = jacobian(MatEncode)\n",
    "MatEncodeJac(mat, K)\n",
    "\n",
    "# does not work:\n",
    "MatDecodeJac = jacobian(MatDecode)\n",
    "print MatDecodeJac(vec, K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@primitive\n",
    "def BinSum(x, y):\n",
    "    return y * x ** 2\n",
    "\n",
    "# It appears that the gradient is always with respect to the argument specified in argnum,\n",
    "# which defaults to zero (the first argument)\n",
    "\n",
    "def BinSum_vjp_x(g, ans, vs, gvs, x, y):\n",
    "    return unbroadcast(vs, gvs, g * 2 * x * y)\n",
    "\n",
    "global_vs = 0\n",
    "global_gvs = 0\n",
    "def BinSum_vjp_y(g, ans, vs, gvs, x, y):\n",
    "    global global_vs\n",
    "    global global_gvs\n",
    "    global_vs = vs\n",
    "    global_gvs = gvs\n",
    "    return unbroadcast(vs, gvs, g * 2 * x ** 2)\n",
    "\n",
    "BinSum.defvjp(BinSum_vjp_x, argnum=0)\n",
    "BinSum.defvjp(BinSum_vjp_y, argnum=1)\n",
    "\n",
    "BinSumGradX = grad(BinSum)\n",
    "BinSumGradY = grad(BinSum, argnum=1)\n",
    "print BinSum.vjps\n",
    "print BinSum(5., 0.1)\n",
    "print BinSumGradX(5., 0.1)\n",
    "print BinSumGradY(5., 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the derivative of diag, it appears tha the output is supposed\n",
    "# to have the same dimension as the input, but with g in the appropriate places?\n",
    "# anp.diag.defvjp(   lambda g, ans, vs, gvs, x, k=0          : anp.diag(g, k))\n",
    "\n",
    "# How does unbroadcast work?\n",
    "\n",
    "@primitive\n",
    "def ElementwiseProd(x, y):\n",
    "    return x * y\n",
    "\n",
    "def ElementwiseProd_vjp(g, ans, vs, gvs, x, y):\n",
    "#     return unbroadcast(vs, gvs, y) # Wrong.\n",
    "    return g * y\n",
    "\n",
    "ElementwiseProd.defvjp(ElementwiseProd_vjp)\n",
    "\n",
    "def AnotherProd(x, z, y):\n",
    "    return z * ElementwiseProd(x, y)\n",
    "\n",
    "AnotherProdJac = jacobian(AnotherProd)\n",
    "\n",
    "x = np.array([2., 3.])\n",
    "y = np.array([10., 200.])\n",
    "z = np.array([100., 1000.])\n",
    "print AnotherProd(x, y, z)\n",
    "print AnotherProdJac(x, z, y)\n",
    "print z * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5, 1])\n",
    "y = np.expand_dims(x, 5)\n",
    "print x.shape\n",
    "print y.shape\n",
    "print x ** 2\n",
    "print np.diag(np.array([5., 6., 7.]), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySumGrad = grad(MySum)\n",
    "MySameSumGrad = grad(MySameSum)\n",
    "\n",
    "x = np.array([2., 4.])\n",
    "print MySumGrad(x)\n",
    "print MySameSumGrad(x)\n",
    "\n",
    "x_full = np.full(13, x.shape)\n",
    "print x_full\n",
    "print x_full.shape\n",
    "\n",
    "print MySum.vjps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@primitive\n",
    "def SumSq(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def SumSq_vjp(g, ans, vs, gvs, x):\n",
    "    print 'g: '\n",
    "    print g\n",
    "    print 'ans: '\n",
    "    print ans\n",
    "    print 'vs: '\n",
    "    print vs\n",
    "    print 'gvs: '\n",
    "    print gvs\n",
    "    print 'x: '\n",
    "    print x\n",
    "    print 'Returning.'\n",
    "    return np.full(x.shape, g) * 2 * x\n",
    "\n",
    "SumSq.defvjp(SumSq_vjp)\n",
    "\n",
    "def MyFun(x):\n",
    "    return 3 * SumSq(x)\n",
    "\n",
    "SumSqGrad = grad(SumSq)\n",
    "MyFunGrad = grad(MyFun)\n",
    "MyFunHess = hessian(MyFun)\n",
    "\n",
    "x = np.array([2., 4.])\n",
    "print 'Stand alone:'\n",
    "print SumSqGrad(x)\n",
    "\n",
    "print '\\nIn function:'\n",
    "print MyFunGrad(x)\n",
    "\n",
    "print '\\nIn Hessian:'\n",
    "print MyFunHess(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import jacobian\n",
    "\n",
    "def MyVecFun(x):\n",
    "\n",
    "    return np.array([ SumSq(x), 5 * SumSq(x) ])\n",
    "\n",
    "MyVecJac = jacobian(MyVecFun)\n",
    "MyVecJac(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my own Hessian\n",
    "\n",
    "@primitive\n",
    "def Magnitude(x):\n",
    "    return np.sum(x ** 3)\n",
    "\n",
    "def MagnitudeRaw(x):\n",
    "    return np.sum(x ** 3)\n",
    "\n",
    "@primitive\n",
    "def MagnitudeGradPrimitive(x):\n",
    "    return 3 * (x ** 2)\n",
    "\n",
    "def MagnitudeGradJac(x):\n",
    "    return 6 * np.diag(x)\n",
    "\n",
    "def Magnitude_vjp(g, ans, vs, gvs, x):\n",
    "    return unbroadcast(vs, gvs, g * MagnitudeGradPrimitive(x))\n",
    "\n",
    "def MagnitudeGrad_vjp(g, ans, vs, gvs, x):\n",
    "    return np.matmul(MagnitudeGradJac(x), g)\n",
    "\n",
    "\n",
    "Magnitude.defvjp(Magnitude_vjp)\n",
    "MagnitudeGradPrimitive.defvjp(MagnitudeGrad_vjp)\n",
    "\n",
    "MagnitudeGrad = grad(Magnitude)\n",
    "MagnitudeHess = hessian(Magnitude)\n",
    "\n",
    "MagnitudeGradRaw = grad(MagnitudeRaw)\n",
    "MagnitudeHessRaw = hessian(MagnitudeRaw)\n",
    "\n",
    "x = np.array([2., 3.])\n",
    "print Magnitude(x)\n",
    "print MagnitudeGrad(x)\n",
    "print MagnitudeHess(x)\n",
    "\n",
    "print MagnitudeRaw(x)\n",
    "print MagnitudeGradRaw(x)\n",
    "print MagnitudeHessRaw(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Logistic operate on vectors may be a headache.\n",
    "\n",
    "# For testing\n",
    "def LogisticRaw(rho):\n",
    "    if rho <= 0:\n",
    "        exp_rho = np.exp(rho)\n",
    "        return exp_rho / (1 + exp_rho)\n",
    "    else:\n",
    "        mexp_rho = np.exp(-rho)\n",
    "        return 1 / (1 + mexp_rho)\n",
    "\n",
    "@primitive\n",
    "def Logistic(rho):\n",
    "    if rho <= 0:\n",
    "        exp_rho = np.exp(rho)\n",
    "        return exp_rho / (1 + exp_rho)\n",
    "    else:\n",
    "        mexp_rho = np.exp(-rho)\n",
    "        return 1 / (1 + mexp_rho)\n",
    "\n",
    "@primitive\n",
    "def LogisticGradient(logit_rho):\n",
    "    return logit_rho * (1 - logit_rho)\n",
    "\n",
    "def LogisticHessian(logit_rho):\n",
    "    return 1 - 2 * logit_rho\n",
    "\n",
    "def Logistic_vjp(g, ans, vs, gvs, x):\n",
    "    return unbroadcast(vs, gvs, g * LogisticGradient(ans))\n",
    "\n",
    "def LogisticGradient_vjp(g, ans, vs, gvs, x):\n",
    "    return unbroadcast(vs, gvs, g * LogisticHessian(x))\n",
    "\n",
    "Logistic.defvjp(Logistic_vjp)\n",
    "LogisticGradient.defvjp(LogisticGradient_vjp)\n",
    "\n",
    "LogisticADGrad = grad(Logistic)\n",
    "LogisticADHessian = hessian(Logistic)\n",
    "\n",
    "LogisticRawADGrad = grad(LogisticRaw)\n",
    "LogisticRawADHessian = hessian(LogisticRaw)\n",
    "\n",
    "x = 3.6\n",
    "print Logistic(x) - LogisticRaw(x)\n",
    "print LogisticADGrad(x) - LogisticRawADGrad(x)\n",
    "print LogisticADHessian(x) - LogisticRawADHessian(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def Log1mInvLogitRaw(u):\n",
    "    return -np.log1p(np.exp(u))\n",
    "\n",
    "@primitive\n",
    "def Log1mInvLogit(exp_u):\n",
    "    return -np.log1p(exp_u)\n",
    "  \n",
    "def Log1mInvLogitDerivative(exp_u):\n",
    "    return -1 / (1 + exp_u)\n",
    "\n",
    "def Log1mInvLogit_vjp(g, ans, vs, gvs, exp_u):\n",
    "    return unbroadcast(vs, gvs, g * Log1mInvLogitDerivative(exp_u))\n",
    "\n",
    "\n",
    "def Log1mInvLogitOneArg(u):\n",
    "    exp_u = np.exp(u)\n",
    "    return Log1mInvLogit(exp_u)\n",
    "\n",
    "Log1mInvLogit.defvjp(Log1mInvLogit_vjp)\n",
    "u = 0.3\n",
    "\n",
    "\n",
    "Log1mInvLogitGrad = grad(Log1mInvLogitOneArg)\n",
    "Log1mInvLogitHess = hessian(Log1mInvLogitOneArg)\n",
    "\n",
    "Log1mInvLogitRawGrad = grad(Log1mInvLogitRaw)\n",
    "Log1mInvLogitRawHess = hessian(Log1mInvLogitRaw)\n",
    "\n",
    "print Log1mInvLogitGrad(u)\n",
    "print Log1mInvLogitHess(u)\n",
    "\n",
    "print Log1mInvLogitRawGrad(u)\n",
    "print Log1mInvLogitRawHess(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache with a closure!\n",
    "\n",
    "class State:\n",
    "    pass\n",
    "\n",
    "def GetLogitVarCache():\n",
    "    st = State()\n",
    "    st.u = float('nan')\n",
    "    st.exp_u = float('nan')\n",
    "    st.logit_u = float('nan')\n",
    "\n",
    "    def Set(u):\n",
    "        st.u = u\n",
    "        st.exp_u = np.exp(u)\n",
    "        st.logit_u = st.exp_u / (1 + st.exp_u)\n",
    "\n",
    "    # Even though these take \"u\" as an argument, it is only to fool autograd.\n",
    "    # They are funcitons only of the cached variables.\n",
    "    @primitive\n",
    "    def Log1mInvLogit(u):\n",
    "        return -np.log1p(st.exp_u)\n",
    "\n",
    "    @primitive\n",
    "    def Log1mInvLogitDerivative(u):\n",
    "        return -st.logit_u\n",
    "\n",
    "    # Set as primitive because as-is this is not differentiable by autograd.\n",
    "    @primitive\n",
    "    def Log1mInvLogitSecondDerivative(u):\n",
    "        return -st.logit_u * (1 - st.logit_u)\n",
    "\n",
    "    def Log1mInvLogit_vjp(g, ans, vs, gvs, u):\n",
    "        return unbroadcast(vs, gvs, g * Log1mInvLogitDerivative(u))\n",
    "\n",
    "    def Log1mInvLogitDerivative_vjp(g, ans, vs, gvs, u):\n",
    "        return unbroadcast(vs, gvs, g * Log1mInvLogitSecondDerivative(u))\n",
    "\n",
    "    Log1mInvLogit.defvjp(Log1mInvLogit_vjp)\n",
    "    Log1mInvLogitDerivative.defvjp(Log1mInvLogitDerivative_vjp)\n",
    "\n",
    "    return Log1mInvLogit, Set\n",
    "\n",
    "\n",
    "GetLogitVarCache_Log1mInvLogit, GetLogitVarCache_Set = GetLogitVarCache()\n",
    "GetLogitVarCache_Set(0.3)\n",
    "\n",
    "print '------\\n'\n",
    "print Log1mInvLogitRaw(0.3)\n",
    "print GetLogitVarCache_Log1mInvLogit(0.3)\n",
    "\n",
    "# This won't change depending on the argument!!!\n",
    "print GetLogitVarCache_Log1mInvLogit(0.6)\n",
    "\n",
    "Log1mInvLogitRawGrad = grad(Log1mInvLogitRaw)\n",
    "Log1mInvLogitGrad = grad(GetLogitVarCache_Log1mInvLogit)\n",
    "\n",
    "Log1mInvLogitRawHess = hessian(Log1mInvLogitRaw)\n",
    "Log1mInvLogitHess = hessian(GetLogitVarCache_Log1mInvLogit)\n",
    "\n",
    "def Log1mInvLogitGradSet(u):\n",
    "    GetLogitVarCache_Set(u)\n",
    "    return Log1mInvLogitGrad(u)\n",
    "\n",
    "def Log1mInvLogitHessSet(u):\n",
    "    GetLogitVarCache_Set(u)\n",
    "    return Log1mInvLogitHess(u)\n",
    "\n",
    "print '------\\n'\n",
    "print Log1mInvLogitGradSet(0.5)\n",
    "print Log1mInvLogitRawGrad(0.5)\n",
    "\n",
    "print '------\\n'\n",
    "print Log1mInvLogitHessSet(0.4)\n",
    "print Log1mInvLogitRawHess(0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "time_num = 3000\n",
    "print 'Gradients:\\n'\n",
    "print timeit.timeit(lambda: Log1mInvLogitGrad(0.3), number=time_num) / time_num\n",
    "print timeit.timeit(lambda: Log1mInvLogitRawGrad(0.3), number=time_num) / time_num\n",
    "\n",
    "print 'Hessians:\\n'\n",
    "print timeit.timeit(lambda: Log1mInvLogitHess(0.3), number=time_num) / time_num\n",
    "print timeit.timeit(lambda: Log1mInvLogitRawHess(0.3), number=time_num) / time_num\n",
    "\n",
    "print 'Gradients with setting:\\n'\n",
    "print timeit.timeit(lambda: Log1mInvLogitGradSet(0.4), number=time_num) / time_num\n",
    "print timeit.timeit(lambda: Log1mInvLogitRawGrad(0.4), number=time_num) / time_num\n",
    "\n",
    "print 'Hessians with setting:\\n'\n",
    "print timeit.timeit(lambda: Log1mInvLogitHessSet(0.4), number=time_num) / time_num\n",
    "print timeit.timeit(lambda: Log1mInvLogitRawHess(0.4), number=time_num) / time_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
